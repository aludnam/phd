%!TEX root =  thesis.tex
\chapter{Non-negative matrix factorisation for localisation microscopy}
We propose non-negative matrix factorisation (NMF) as a natural model for LM data with QD labeled samples. NMF can separate the individual highly overlapping sources with individual different shapes. This situation that can arise in a real microscopic data when aberrations are present or when the sources are not in one focal plane (\autoref{fig:Simulted-PSF-different-focal-depths}). Moreover we use the NMF algorithm which takes the Poisson noise in the recorded images into account. This allows us to recover the individual sources from noisy QD data.

The chapter is composed in following parts: \Autoref{sec:LM} introduces the localisation microscopy (LM) technique and \autoref{sec:QD for LM} discusses the advantages and challenges of using the quantum dots as fluorescent labels and reviews some recent methods dealing with LM data containing overlapping sources. 

\Autoref{sec:NMF} introduces non-negative matrix factorisation (NMF). Alternative algorithms applying certain constraints on the estimated results are discussed in \autoref{sub:NMF constrains} and the generative probabilistic model for NMF is mentioned in \autoref{sub:GaP}.

\Autoref{sec: NMF} shows NMF as a natural model for intermittent overlapping QDs and \autoref{sec:NMF related} discusses the alternative methods used for treating QD data. A link to a standard deconvolution technique is also mentioned. The application of the NMF algorithm to real microscopic data is explored in \autoref{sec:NMF-for-real}. In this section we also present preliminary results to demonstrate specific problems of the topic.

We used synthetic data are used for analysing the performance of the algorithm. The main simulations used in this chapter are described in \autoref{sec:simulations} and \autoref{sec:evaluation} explains the evaluation techniques for the comparison of the results.

Comparison of the NMF applied to synthetic and real data is in \autoref{sec:results}. This section also contains a comparison with other techniques dealing with similar problematics. 

%==========================================
%==========================================

\section{Localisation microscopy\label{sec:LM}}

Localisation microscopy (LM) is a conceptually simple and accessible technique for super-resolution imaging of fluorescent samples. LM takes as input a stack of images containing a number of fluorescent sources with time-varying intensity and identify the locations and point-spread functions (PSFs) of these sources. If the sources are attached to structures of interest (e.g.\ in biological samples), then this provides useful information about the target structures. By exploiting multiple images of time-varying sources, LM can achieve a resolution beyond the classical diffraction limit of $\sim \lambda/2$ \cite{Abbe1873}, where $\lambda$ is the wavelength of the light source. Provided enough photons are collected, the localisation of an individual source can be an order of magnitude below $\lambda/2$ \cite{Ober2004}, meaning that sources whose point spread functions overlap heavily can be resolved. 
%See \autoref{fig:Iterative restarts}, \autoref{fig:Real-data-QDrandom} and \autoref{fig:Real-data-patch-B24} for examples.

LM techniques are based on sources with transition between bright (ON) and dark (OFF) states. Fluorescent proteins or organic dyes are used as fluorophores in the standard techniques (fPALM \cite{Hess2006}, STORM \cite{Rust2006}). In this case the density of the ON sources in each captured frame can be controlled by photo-switching and must be optimised experimentally. High density of ON fluorophores results in overlapping sources and complicates localisation (overlapping sources are usually discarded), whereas low density leads to a excessive total acquisition time. Several thousands frames are typically required for an image reconstruction.

%==========================================

\subsection{Quantum dots for localisation microscopy\label{sec:QD for LM}}

In recent years there has been interest in using quantum dots (QDs) as sources for localisation microscopy. QDs are an order of magnitude brighter compared to the organic dyes or fluorescent proteins used in conventional LM \cite{Resch-Genger2008}. Under continuous excitation QDs exhibit a stochastic blinking between ON and OFF states. Excellent photo-stability, low cyto-toxicity and distinctive spectral properties make QDs very attractive for biological research. However, the stochastic blinking of QDs is impractical for standard LM techniques because the rate of switching, and hence the density of ON sources, is difficult to control. Thus QD-labeled data typically consist of highly overlapping sources which cannot be localised with standard techniques.

%==========================================

\subsection{Overlapping sources\label{sec:Overlapping sources}}

Several techniques dealing with overlapping sources have been proposed recently. Most of these methods model the LM data using a known image of a single source, the so called point spread function (PSF). Most often a single PSF is assumed to be shared by all sources in the dataset.

There are two main groups of the algorithms addressing the overlapping sources in the LM data. 

The first group operates separately on each frame of the LM dataset: a method proposed in \cite{Huang2011} tries to fit multiple PSFs into each frame of the dataset while the CSSTORM proposed in \cite{Zhu2012} make use of compressed sampling to recover the sparse vector representing the fluorophores distribution. The DAOSTORM algorithm \cite{Holden2011} applies iterative fitting and subtracting procedure in each frame. 

Because each frame of the dataset is treated independently, these methods ignore the fact that some sources can stay ON for several subsequent frames or can even reappear in different frames due to blinking and can generally deal with moderately overlapping sources with densities $<10\unit{sources/\mu m^{2}}$ \cite{Huang2011,Holden2011,Zhu2012}.

The second group of the algorithms models LM dataset as a collection of blinking sources. They can improve the localisation for higher densities of the overlapping sources by taking the the reappearance of fluorophores into account. However, these algorithms are in general computationally more expensive. 

Modelling the whole dataset from known PSF with maximum aposterioiry (MAP) fitting has been proposed in \cite{Harrington2008}. Bayesian analysis of the intermittent sources (Bayesian Blinking and Bleaching (3B) analysis) has been suggested in \cite{Cox2011}. The blinking behaviour of the fluorophores is modelled as a hidden markov model with three distinct states: emitting, not-emitting and bleached. Each source is described by its position, size of the PSF, and intensity. MAP estimates of the positions obtained from different sampling of state sequences are used as estimated locations of the fluorophores. While the 3B analysis adjusts the width of the PSF (gaussian approximation of the PSF \cite{Zhang2007}), it cannot deal with PSFs of different shapes. This situation can arise, for example, in three dimensional samples, where the overlapping sources can be located in different focal planes (see \autoref{fig:Simulted-PSF-different-focal-depths}). Moreover, the 3B analysis assumes a mono-exponential decay of the fluorescence for the individual sources. QDs have a complex blinking behaviour with power-law distribution in the histogram of on and off times \cite{Shimizu2001}. This can complicate the 3B analysis of the QD data. 

The independent component analysis (ICA) have been proposed for analysis of overlapping intermittent sources in \cite{Lidke2005} and s further discussed in \autoref{sub:ICA}.

Yet another approach to the LM data with overlapping sources problem has been proposed in a method called SOFI (Superresolution Optical Fluctuation Imaging) \cite{Dertinger2010b}. Instead of separating the individual sources, SOFI analyses higher order statistics of the intensity fluctuation. The intensity values in the SOFI image, however, reflect the fluctuation behaviour, rather than the strength of the emitters. Sources which do not blink will not appear in the SOFI image. This issue has been addressed recently by bSOFI (balanced SOFI) \cite{Geissbuehler2012}.
% The balanced SOFI paper to read!!

\begin{figure}[!htb]
	\newcommand{\widthfig}{.95\textwidth}
	\newcommand{\barspace}{-.5cm}
	\centering
	\condcomment{\boolean{includefigs}}{ 
	\begin{tabular}{l}
		\includegraphics[width=\widthfig]{\qd S393/images/psf_outOfFocus}\vspace{\barspace}\tabularnewline		
		\includegraphics[width=\widthfig]{\qd S393/images/psf_outOfFocus_intBars}
	\end{tabular}
	}	
	\caption{Simulated PSF in different depths of focus. The number in each figure indicates the distance in $\unit{\mu m}$ from the in-focus plane. The maximum intensity relative to the in-focus PSF is indicated in the bars below and corresponds to about 10\% at $1\unit{\mu m}$ and 3\% at $1.5\unit{\mu m}$.}
	\label{fig:Simulted-PSF-different-focal-depths}
\end{figure}

%==========================================
%==========================================

\clearpage
\section{Non-negative matrix factorisation\label{sec:NMF}}
Non-negative matrix factorisation (NMF) solves the approximative factorisation of an $N\times T$ data matrix $\bm{D}$ with non-negative entries:
%
\begin{equation}
	\bm{D}\approx\bm{WH},
	\label{eq:NMF approx}
\end{equation}
%
where $\bm{W}$ and $\bm{H}$ are $N\times K$ and $K\times T$  matrices ($K<N,T$), respectively. The factorisation is constraint to $\bm{W}$ and $\bm{H}$ with non-negative entries. 

Initial factorisation algorithms (so called positive matrix factorisation) \cite{Paatero1994} were published in 1994. However, it was in 1999 when NMF attracted attention of researchers after publication of the \emph{Nature} article by Daniel Lee and Sebastian Seung \cite{Lee1999}. NMF was presented as an efficient and powerful method for approximation of non-negative data (in their case a database of facial images) by linear combination of non-negative localised basis vectors (images of the nose, mouth, ears, eyes, etc.) An individual face from the data-set can be recovered as a non-subtractive composition of individual basis vectors. 

Lee and Seung also proposed simple multiplicative updates \cite{Lee2001} for the elements of $\bm{W}$ and $\bm{H}$
%
\begin{alignat}{1}
	w_{xk} & =\frac{w_{xk}}{\sum_{t=1}^{T}h_{kt}}\left[(\bm{V}./\bm{WH})\bm{H^{\top}}\right]_{xk}\nonumber \\
	h_{kt} & =\frac{h_{kt}}{\sum_{x=1}^{N}w_{xk}}\left[\bm{W^{\top}}(\bm{V}./\bm{WH})\right]_{kt},
	\label{eq:NMF classic updates}
\end{alignat}
%
minimising KL-divergence between data matrix $\bm{D}$ and its factorised approximation $\bm{WH}$ (for details see \autoref{sec: NMF} and \autoref{app:NMF-algorithm}). The symbol ``$./$'' denotes the element-wise division of matrices.

Note that updates \autoref{eq:NMF classic updates} automatically ensure that $\bm{W}$ and $\bm{H}$ remains non-negative if initialised so. Also once they become zero they remain zero for the rest of iterations. Sufficient conditions for uniqueness of solutions to the NMF problem has been studied in \cite{Donoho2004}. 

%==========================================

\subsection{Additional constraints to the NMF model \label{sub:NMF constrains}}
Various alternative minimisation strategies have been explored in an effort to speed up the convergence properties of the Lee \& Seung updates. A comprehensive discussion on the variety of these algorithms can be found in \cite{Berry2007}. 

Additional constraints can be imposed on $\bm{W}$ and $\bm{H}$ matrices. Imposing a defined ``sparsity'' on either columns of $\bm{W}$ or rows of $\bm{H}$ has been proposed in \cite{Hoyer2004} and is discussed in \autoref{sub:Hoyer}. Enforcing the temporal smoothness of $\bm{H}$ in the analysis of EEG recordings has been published in \cite{Chen2005}. Multiplicative updates for various constraints have been suggested in \cite{Chen2005,Pauca2006}  (see \autoref{app:NMF-algorithm}).

%==========================================

\subsection{Gamma - Poisson model \label{sub:GaP}}
The gamma-Poisson (GaP) model has been proposed \cite{Canny2004} as a probabilistic model for documents. It represents a generative model for NMF \autoref{eq:NMF model}. The entries $h_{kt}$ of the intensity matrix $\bm{H}$ in \autoref{eq:NMF model element-wise} are regarded as latent variables generated from a Gamma distribution with parameters $\alpha_{k}, \beta_{k}$ and the data are modelled as a Poisson variable with mean $\bm{WH}$. Variables $\theta = \{\bm{w_{k}},\alpha_{k}, \beta_{k}\}; k = 1..K$ are then parameters of the GaP model.

%==========================================
%==========================================
\clearpage
\section{NMF as a natural model for QD data\label{sec: NMF}}
Non-negative matrix factorisation (NMF) \cite{Lee1999,Lee2001} is a natural model for QD data. NMF decomposes a movie of the blinking QDs into spatial and temporal parts, i.e.,\ time independent emission profiles of the individual sources and fluctuating intensities of each source, respectively. NMF imposes non-negativity constraints on both the spatial and the temporal components which are natural constraints for the source profiles and intensities of blinking QDs.

Consider a $N\times T$ data matrix $\bm{D}$, where $N$ is the number of pixels in each frame, and $T$ is the number of time frames. The columns $\bm{D}$ are the individual frames of the movie reshaped into $N\times 1$ vector by concatenating the columns the image. All entries in $\bm{D}$ are non-negative, i.e.,\ $d_{xt}\geq 0$. Under the NMF model, the expectation value of the $\bm{D}$ is assumed to be decomposed into a $N\times K$ spatial component matrix $\bm{W}$ (images of the $K$ individual sources) and the $K\times T$ temporal component matrix $\bm{H}$ (the intensities of the sources):
%
\begin{equation}
	\mathbb{E}\left[\bm{D}\right]=\bm{WH};\;w_{xk},\, h_{kt}\geq0
	\label{eq:NMF model}
\end{equation}
%
or in element-wise form
%
\begin{equation}
	\mathbb{E}\left[d_{xk}\right]=\sum_{k=1}^{K}w_{xk}h_{kt};\;w_{xk},\, h_{kt}\geq0
	\label{eq:NMF model element-wise}
\end{equation}

The predominant noise model in microscopy imaging is Poisson noise \cite{PawleyHandbook2006}. Therefore the log-likelihood function can be expressed as
%
\begin{equation}
	\log p(\bm{D}|\bm{W},\bm{H})=\sum_{xt}\left(d_{xt}\log\sum_{k=1}^{K}w_{xk}h_{kt}-\sum_{k=1}^{K}w_{xk}h_{kt}\right)+C_{1},
	\label{eq:NMF model likelihood}
\end{equation}
%
where $C_{1}$ is independent of $\bm{W}$and $\bm{H}$. 

The Lee and Seung NMF updates \autoref{eq:NMF classic updates} minimise the divergence between the data and the NMF model
%
\begin{equation}
	\mbox{KL}(\bm{D}\parallel\bm{WH})=-\sum_{xt}\left(d_{xt}\log\sum_{k=1}^{K}w_{xk}h_{kt}-\sum_{k=1}^{K}w_{xk}h_{kt}\right)+C_{2}
	\label{eq:KL divergence}
\end{equation}
%
where $C_{2}$ is independent of $\bm{W}$and $\bm{H}$. Comparison with the log-likelihood \autoref{eq:NMF model likelihood} shows that the minimum of the divergence with positivity constrains on $\bm{W}$ and $\bm{H}$ is equivalent to the maximum of the log-likelihood. 

There is a scaling indeterminacy between $\bm{W}$ and $\bm{H}$ in the NMF model. We fix this by setting the $L_1$ norm of each column of $\bm{W}$ to 1. The background fluorescence in the images is modelled as a `flat' component $\bm{w}_{K} = \bfone/N$ with corresponding intensity $\bm{h}_{K}$. The spatial part $\bm{w}_{K}$ is not updated during the algorithm. 

The NMF model is fitted to data iteratively using multiplicative updates \autoref{eq:NMF classic updates} sequentially \cite{Lee2001}. Note that the divergence (\autoref{eq:KL divergence}) is convex wrt $\bm{H}$ and $\bm{W}$ individually, but not in both variables together \cite{Lee2001}, leading to local optima.

%==========================================
%==========================================

\clearpage
\section{Related work \label{sec:NMF related}}
This section points to published work relevant to NMF application to the QD data. A NMF algorithm with sparsity constraints is reviewed and demonstrated on simulated data in \autoref{sub:Hoyer}. \Autoref{sub:ICA} discusses the proposed independent component analysis (ICA) as a model for QD data, while \autoref{sub:RL deconvolution} shows a link between NMF and a standard Richardson-Lucy deconvolution.
%==========================================

\subsection{Hoyer's sparse NMF \label{sub:Hoyer}}
The in-focus PSF (see \autoref{fig:Simulted-PSF-different-focal-depths}, left) is a fairly compact structure with only few pixels with significant values. A constraint on a sparsity of the estimated $\bm{w_{k}}$ would likely to facilitate the estimation of the credible sources and might lead to a faster convergence to a better local minimum. NMF with explicit sparsity constraints has been developed by Hoyer  \cite{Hoyer2004}. 

The ``sparsity'' of a vector $\bm{x}$ is defined as 
%
\begin{equation}
	s(\bm{x})=\frac{\sqrt{n}-L_{1}/L_{2}}{\sqrt{n}-1},
	\label{eq:Hoyers sparsity}
\end{equation}
%
where $L_{1}=\sum|x_{i}|$, $L_{2}=\sum \sqrt{x^{2}_{i}}$ and $n$ is the dimensionality of the vector $\bm{x}$.

Specific fixed constraints on the ``sparsity'' on the columns of $\bm{W}$ can be imposed during the optimisation. After each iteration, the columns $\bm{w}_{k}$s of the estimated matrix $\bm{W}$ are projected to be non-negative, have unchanged $L_{2}$ norm, but $L_{1}$ norm set to achieve the desired sparseness \autoref{eq:Hoyers sparsity}.

Note that the assumption that all columns have identical ``sparseness'' might be restrictive when out-of-focus PSFs are present. For example, the in focus PSF in \autoref{fig:Simulted-PSF-different-focal-depths} has Hoyer's sparsity $s=0.83$ while the PSF from $1 \unit{\mu m}$ out-of-focus plane has $s=0.4$ and the PSF from $1.8 \unit{\mu m}$ out-of-focus plane  has $s=0.1$.

The Hoyers's sparse NMF algorithm also minimises $\|\bm{D} - \bm{WH}\|^{2}$ rather then the KL-divergence \autoref{eq:KL divergence}. This corresponds to the Gaussian rather then Poisson noise assumption. 

\begin{figure}[!htb] %copied from S433_report.tex
	\newcommand{\sizefig}{.9}
	\centering
	\subfloat[$10 \unit{sources/\mu m^{2}}$]{	
	\includegraphics[scale=\sizefig]{\qd S429/figures/resN_simiter1_1to14}}\\
%	\subfloat[$20 \unit{sources/\mu m^{2}}$]{	
%	\includegraphics[scale=\sizefig]{\qd S430/figures/resN_simiter1_1to14}}\\
	\subfloat[$30 \unit{sources/\mu m^{2}}$]{	
	\includegraphics[scale=\sizefig]{\qd S431/figures/resN_simiter1_1to14}}\\
%	\subfloat[$40 \unit{sources/\mu m^{2}}$]{	
%	\includegraphics[scale=\sizefig]{\qd S432/figures/resN_simiter1_1to14}}\\
	\subfloat[$50 \unit{sources/\mu m^{2}}$]{	
	\includegraphics[scale=\sizefig]{\qd S433/figures/resN_simiter1_1to14}}\\	
	\caption{$\bm{W}$ estimated with Hoyer's algorithm with no sparsity constraints. This corresponds to a conventional NMF. Evaluation of the simulated data of randomly scattered sources with different density. Shown first 14 estimated components.}
	\label{fig: Hoyer no sparsity constraint}
\end{figure}

\begin{figure}[htbp!] %copied from S433_report.tex
	\newcommand{\sizefig}{.9}
	\centering
	\subfloat[$10 \unit{sources/\mu m^{2}}$]{	
	\includegraphics[scale=\sizefig]{\qd S429/figures/res07_simiter1_1to14}}\\
%	\subfloat[$20 \unit{sources/\mu m^{2}}$]{	
%	\includegraphics[scale=\sizefig]{\qd S430/figures/res07_simiter1_1to14}}\\
	\subfloat[$30 \unit{sources/\mu m^{2}}$]{	
	\includegraphics[scale=\sizefig]{\qd S431/figures/res07_simiter1_1to14}}\\
%	\subfloat[$40 \unit{sources/\mu m^{2}}$]{	
%	\includegraphics[scale=\sizefig]{\qd S432/figures/res07_simiter1_1to14}}\\
	\subfloat[$50 \unit{sources/\mu m^{2}}$]{	
	\includegraphics[scale=\sizefig]{\qd S433/figures/res07_simiter1_1to14}}\\	
	\caption{Sparsity constraints $s=0.7$ on $\bm{W}$ estimated with Hoyer's algorithm from simulated data of randomly scattered sources with different density. Shown first 14 estimated components.}
	\label{fig: Hoyer sparsity 0.7}
\end{figure}

We used simulated data to explored the ability of the Hoyer's algorithm to recover credible sources. Data were simulated randomly scattered sources with densities $10-50\,\unit{\mu m^{-2}}$. The blinking intensity  was assumed to be uniformly distributed on the interval $[0, 5000]$ photons. The background was $100$ photons/pixel and data were corrupted with Poisson noise. Prior to the evaluation with the Hoyer's algorithm, the true background value was subtracted from the data (clipping negative pixels to zero). The number of components $K$ was set to correct (true) value. The algorithm was run for 1000 iterations. Running the algorithm for longer (2000, 5000) iterations did not improve the estimated results. 

\Autoref{fig: Hoyer no sparsity constraint} shows estimated $\bm{W}$ with Hoyer's algorithm without sparsity constraints, which corresponds to a conventional NMF. Note that most of the $\bm{w_{k}}$ for higher densities contain multiple sources \autoref{fig: Hoyer no sparsity constraint}b,c. Imposing the sparsity constraints $s=0.7$ on the columns of $\bm{W}$, estimated from the true PSF, gives better estimated sources for densities$<30\unit{\mu m^{-2}}$  \autoref{fig: Hoyer sparsity 0.7}a, however, for dense data the method fails to recover the individual sources and gives unsatisfactory results, see \autoref{fig: Hoyer sparsity 0.7}c.

%==========================================

\subsection{Independent component analysis\label{sub:ICA}}
\begin{figure}[!htb] % this figure is copied from ~/DTC/paper/NMFLM.tex
	\condcomment{\boolean{includefigs}}{
	\newcommand{\sizefig}{.4}
	\centering
	\subfloat[NMF (noise)]{
	\begin{tabular}{c}
		\includegraphics[scale=\sizefig]{\qd S310/images/nmf_noise_loc_c1} \tabularnewline
		\includegraphics[scale=\sizefig]{\qd S310/images/nmf_noise_loc_c2}\tabularnewline
	\end{tabular}}
	\subfloat[ICA (noise)]{
	\begin{tabular}{c}
		\includegraphics[scale=\sizefig]{\qd S310/images/ica_noise_loc_c1} \tabularnewline
		\includegraphics[scale=\sizefig]{\qd S310/images/ica_noise_loc_c2}\tabularnewline
	\end{tabular}}	
	\subfloat[ICA (noise free)]{
	\begin{tabular}{c}
		\includegraphics[scale=\sizefig]{\qd S310/images/ica_noNoise_loc_c1} \tabularnewline
		\includegraphics[scale=\sizefig]{\qd S310/images/ica_noNoise_loc_c2}\tabularnewline
	\end{tabular}}}
	\caption{Comparison of the components separated with NMF (a) and ICA (b) for	simulated noisy data of two blinking QDs separated by $0.5 \unit{pixel}$ (which corresponds to $50\unit{nm}$ or $\lambda/12$. ICA for noise free data (c).  Blue pixels contain negative values. The true and the estimated positions are shown as red circles and green crosses, respectively. The airy disk is shown as a green circle (radius $333\unit{nm}$).}
	\label{fig:Comparison of NMF and ICA}
\end{figure}

The independent component analysis (ICA) algorithm \cite{Hyvarinen2000} has been used for separating the overlapping QDs \cite{Lidke2005}. ICA allows each sources to have a different individual PSF.  However, the ICA model allows negative entries in the individual PSFs and does not account for noise in the measured data, which can make recovery of the individual sources difficult in realistic noise levels \autoref{fig:Comparison of NMF and ICA}. 

\Autoref{fig:Comparison of NMF and ICA} shows results from $10^{3}$ simulated frames containing two sources with blinking intensity uniformly distributed on the interval $[0, 1500]$ photons and with background $100$ photons/pixel. The true background level was subtracted (clipping the negative values) prior to the ICA evaluation.


%==========================================

\subsection{Richardson-Lucy deconvolution \label{sub:RL deconvolution}}
%This is from DecovolutionNotes.lyx file.
There is a link between NMF and classical Richadson-Lucy deconvolution. An observed ``blurred'' (diffraction limited) image $\bm{i}$ ($N\times1$ vector) can expressed as a (discretised) convolution 
%
\begin{equation} 
	i_{x}=\sum_{j=1}^{N}o_{j}w_{x-j}, 
\end{equation} 
%
where $\bm{o}$ ($N\times1$) is the original (unblurred) object which represents locations and intensities of fluorescent sources. $\bm{w}$ ($N\times1$) is an image of point spread function (PSF) centred in the middle of the image. Richardson \cite{Richardson1972} and Lucy \cite{Lucy1974} published an iterative deconvolution technique for astronomical images with known PSF. They used Bayes theorem as a `hint' for an iterative update of $\bm{o}$. This update is usually referred to as Richardson-Lucy (RL) deconvolution algorithm and is identical to the Lee-Seung NMF update with generalised KL-divergence objective function \cite{Lee2001}. 

Holmes \cite{Holmes:92} derived the RL updates based on maximum likelihood estimation of the model with Poisson noise using the expectation-maximisation algorithm. He also proposed an update for $\bm{w}$ so that the method can be used as a blind deconvolution algorithm (PSF is not known). This is sometimes referred to as a `blind RL algorithm'. The updates for $\bm{o}$ and $\bm{w}$ are technically identical to the Lee and Seung NMF updates (KL divergence as an objective function). 

However, (blind) RL deconvolution solves a different problem then NMF. RL deconvolution estimates one PSF ($\bm{w}$) which is shared by all sources. The deconvolution is performed for each frame separately, independent on the rest of the dataset. NMF models the whole dataset as a collection of individual (and in general different) PSFs (columns of $\bm{W}$) each changing intensity over time (rows of $\bm{H}$). While one source which appears in $n$ different frames is treated as $n$ different individual sources by RL, NMF can identify it as a single source. 

Modified updates imposing radial symmetry constraints on the PSF were also proposed. There exist several modified updates derived using EM algorithm which impose some constraints on $\bm{o}$ or $\bm{w}$. \cite{Joshi:93} gives updates where Good's roughness measure ($\int\frac{\left|\nabla f(x)\right|^{2}}{f(x)}dx$) on the original image $\bm{o}$ is used as a regularisation term. This biases the solution towards the `smooth' images and avoids speckle artefacts in the reconstructions that are sometimes experienced in deconvolution methods. 

\cite{Fish:95} use RL `blind' algorithm (updates on both $\bm{o}$ and $\bm{w}$) but after some number of iterations they fit some approximation of the PSF to the estimated $\bm{w}$ and uses this fit as a new $\bm{w}$. They claim that in noisy images this `semi-blind' deconvolution can perform better than the one with known PSF. The comparison of the regularised RL versions and some other deconvolution techniques has been shown in \cite{Kempen1997BA,Verveer1999}. RL usually performs well for noisy images.

%==========================================
%==========================================

\clearpage
\section{Simulations \label{sec:simulations}}
In this section we describe the generation of the simulated datasets used to test the performance of the algorithm in different experimental settings. These datasets were also used for comparing the NMF with CSSTORM \cite{Zhu2012} and the 3B \cite{Cox2011} methods. 

%Main competitors 3B \cite{Cox2011}, CSSTORM \cite{Zhu2012} and SOFI \cite{Dertinger2009}.	

The parameters of the simulations were chosen to correspond to real experimental data with QDs.  \Autoref{tab:Simulations parameters} summarises the main simulation parameters. 
%
\begin{table}[!h]
	\centering
	\begin{tabular}{|c|c|l|}
		\hline 
		Parameter 		& Note  							& Value\tabularnewline
		\hline \hline 
		$\lambda_{em}$ 	& wavelength of the emission light 	& 625 nm\tabularnewline
		\hline 
		NA 				& numerical aperture 				& 1.3\tabularnewline
		\hline 
		RI 				& refraction index 					& 1.5\tabularnewline
		\hline 
		pixel-size 		& size of a pixel in image plane	& 80 nm\tabularnewline
		\hline 
		$n_{t}$ 			& number of frames  					& $50-1000$\tabularnewline
		\hline 
		$\unit{mean}(n_{phot})$ & mean number of photons / source / frame 	& 2500\tabularnewline
		\hline 
		$\max(n_{phot})$ & max number of photons / source / frame 	& 5000\tabularnewline
		\hline 
		$b$ 				& \# of photons/pixel/frame 			& 100\tabularnewline
		\hline
		noise			& 		-						& Poisson\tabularnewline
		\hline
	\end{tabular}
	\caption{Main parameters for data simulations.}
	\label{tab:Simulations parameters}
\end{table}

The blinking behaviour of the QDs was simulated as either:
%
\begin{enumerate}
	\item
	Uniform random number between $0$ and $\max(n_{phot})$.
	\item
	Telegraph process with switching rate $\gamma$. Difference between the sampling rate of the experiment and the blinking of the fluorophores is considered by simulating the blinking behaviour on the oversampled time axes followed with averaging over several bins. The simulation of the different blinking behaviour is discussed in \autoref{sub:results - blinking behaviour}. 
\end{enumerate}
 
%==========================================
\clearpage
\subsection{Randomly scattered sources\label{sub:Simul random}}
The ability of the algorithm to separate individual overlapping sources was tested on simulated data of randomly scattered fluorophores.     The density of the sources was in a range $\rho=10-50 \unit{sources/\mu m^{2}}$. Several frames of the dataset for four different densities is shown in  \autoref{fig:simulated data random}. The mean projection of the frames, which corresponds to a wide-field images, is shown in \autoref{fig:simulated data random}.

\begin{figure}[!htb]	
	\newcommand{\widthfig}{1\textwidth}
	\centering	
	\subfloat[density $10\unit{\mu m^{-2}}$ ($14$ sources)]{
	\includegraphics[width=\widthfig]{\qd S455/images/dpixc_1to8_dens10}}
	
	\subfloat[density $30\unit{\mu m^{-2}}$ ($43$ sources)]{
	\includegraphics[width=\widthfig]{\qd S455/images/dpixc_1to8_dens30}}
	
	\subfloat[density $50\unit{\mu m^{-2}}$ ($72$ sources)]{
	\includegraphics[width=\widthfig]{\qd S455/images/dpixc_1to8_dens50}}		
	\caption{First eight frames of simulated randomly scattered sources with density $10-50\unit{\mu m^{-2}}$. The area of the frame is $1.2\times1.2\unit{\mu m}$ ($15\times15$ pixels)}
	\label{fig:simulated data random}
\end{figure} 
%
\begin{figure}[!htb]	
	\newcommand{\widthfig}{.25\textwidth}
	\centering	
	\subfloat[density $10\unit{\mu m^{-2}}$]{
	\includegraphics[width=\widthfig]{\qd S455/images/dpixc_mean_dens10}}\hspace{.3cm}	
	\subfloat[density $30\unit{\mu m^{-2}}$]{
	\includegraphics[width=\widthfig]{\qd S455/images/dpixc_mean_dens30}}\hspace{.3cm}	
	\subfloat[density $50\unit{\mu m^{-2}}$]{
	\includegraphics[width=\widthfig]{\qd S455/images/dpixc_mean_dens50}}		
	\caption{Mean projection of the simulated frames \autoref{fig:simulated data random}. The sources' positions are marked with red dots.}
	\label{fig:simulated data random - mean}
\end{figure} 

%==========================================
\clearpage
\subsection{Artificial structure\label{sub:Simul hash}}
Dataset with sources arranged in a shape of a hash symbol (\#) was used for testing the algorithm to recover a structural details in the sample. The vertical parallel lines were aligned with the pixels grid, the horizontal lines were slightly tilted to investigate the possible effect caused by the geometrical configuration of the sources with respect to the pixel grid. 

The distance $d$ between the parallel lines and the linear density of the sources $\mu$ were two main parameters of the structure. \Autoref{fig:simulated data hash} shows several frames of the simulated dataset for two different linear densities and $d=100\unit{nm}$, which corresponds to half of the radius of the Airy disk. The mean projection of the frames is shown in \autoref{fig:simulated data hash - mean}.
%
\begin{figure}[!htb]	
	\newcommand{\widthfig}{1\textwidth}
	\centering	
%	\subfloat[linear density $0.3\unit{\mu m^{-1}}$ ]{
%	\includegraphics[width=\widthfig]{\qd S569/figures/dpixc_1to8_dens3}}
	
	\subfloat[linear density $0.6\unit{\mu m^{-1}}$]{
	\includegraphics[width=\widthfig]{\qd S569/figures/dpixc_1to8_dens6}}
	
	\subfloat[linear density $1.2\unit{\mu m^{-1}}$]{
	\includegraphics[width=\widthfig]{\qd S569/figures/dpixc_1to8_dens12}}		
	\caption{First eight frames of the simulated dataset. The area of the frame is $1.7\times1.7\unit{\mu m}$ ($21\times21$ pixels).}
	\label{fig:simulated data hash}
\end{figure} 
%
\begin{figure}[!htb]	
	\newcommand{\widthfig}{.3\textwidth}
	\centering	
%	\subfloat[$\mu=0.3\unit{\mu m^{-1}}$]{
%	\includegraphics[width=\widthfig]{\qd S569/figures/dpixc_mean_dens3}}\hspace{.3cm}	
	\subfloat[$\mu=0.6\unit{\mu m^{-1}}$]{
	\includegraphics[width=\widthfig]{\qd S569/figures/dpixc_mean_dens6}}\hspace{.3cm}	
	\subfloat[$\mu=1.2\unit{\mu m^{-1}}$]{
	\includegraphics[width=\widthfig]{\qd S569/figures/dpixc_mean_dens12}}		
	\caption{Mean projection of the simulated frames \autoref{fig:simulated data hash}. The sources' positions are marked with red dots.}
	\label{fig:simulated data hash - mean}
\end{figure} 

%==========================================
%==========================================

\clearpage
\section{Evaluation of the results\label{sec:evaluation}}

The performance of the algorithm applied on a simulated dataset can be quantitatively measured, because the true locations of the sources are known. We used several measures to compare the algorithm performance on simulated datasets consisting of randomly scattered in-focus PSFs. 

\begin{figure}[!h]
	\centering
	\includegraphics[width=1.0\textwidth]{figures/TPFNFPillustration/TPFNFPs}
	\caption{True positives TP, false positives FP and false negatives FN illustration. A red dot represents the true location with a circle of radius $r$, a green cross denotes an estimated position.}
	\label{fig:TPFPFN}
\end{figure}

The individual estimated sources $\bm{w_{k}}$ were localised by ML fitting of a gaussian approximation of a PSF \cite{Zhang2007}. We used a greedy algorithm to assign the estimated locations ($E$) to their nearest true positions ($T$). Only one estimated position was assigned to each true position. If the distance between and estimated and the true position was smaller than a threshold $r$, then the source was consider as a true positive (TP). A true position with no estimated source within a disk of radius $r$ was false negative (FN), whereas an estimated position further then $r$ from any true position was considered as false positive (FP), see \autoref{fig:TPFPFN}. $M$ estimated sources in the proximity of one true source are counted as $1$TP and $(M-1)$FP (\autoref{fig:TPFPFNcombi}, left). One estimated source in proximity of $M$ true sources gives $1$TP and $(M-1)$FN (\autoref{fig:TPFPFNcombi}, right).

We set the threshold $r=\sigma/2$, where $\sigma=\frac{\sqrt{2}}{2\pi}\frac{\lambda_{em}}{NA}$ is the standard deviation of the in-focus PSF gaussian approximation \cite{Zhang2007}. For the parameters used in our simulations (see \autoref{tab:Simulations parameters}) the threshold corresponds to $r=0.7\unit{pixels}$ ($56\unit{nm}$). 

\begin{figure}[!h]
	\centering
	\includegraphics[width=1.0\textwidth]{figures/TPFNFPillustration/TPFNFPs_comb}
	\caption{There is only one estimated position assigned to each true position. Two estimated sources in the proximity of one true source are counted as $1$TP and $1$FP (left). One estimated source in proximity of two true sources gives $1$TP and $1$FN (right)}
	\label{fig:TPFPFNcombi}
\end{figure}

Localisation error was estimated as an average distance between true locations and all estimated locations classified as TP. 

The estimated density of the sources was counted as number of all TP divided by the area of the image.

The average precision (AP) \cite{Salton1986,Everingham2009} was used to summarise both localisation precision and ability to recover the individual sources. The estimated positions $e_{k}$ are ranked according to the square of the source's mean brightness
%
\begin{equation}
	b_{k}=\sqrt{\bar{N_{k}}}
\end{equation}
%
because the localisation precision should scale as $1/\sqrt{N}$, where $N$ is a number of photons emitted by a source. This is retrieved from the matrix $\bm{H}$ as a mean along the rows
%
\begin{equation}
	\bar{N_{k}}=\underset{t}{\unit{mean}}(h_{kt}).
\end{equation}

The interval $[l_{min},l_{max}]$ between the dimmest $l_{min}$ and the brightest $l_{max}$ source intensity is divided into number of intervals (confidence levels) $l_{i}$ defined by the steps in the sorted intensities of all sources. For each confidence level $l_i$ only the sources with $b_{k}$ above $l_i$ are considered. True positives ($\unit{TP}_{i}$), false negatives ($\unit{FN}_{i}$) and false positives ($\unit{FP}_{i}$) are computed for each confidence level $l_{i}$.

Precision $P$ and recall $R$ are computed from $\unit{TP}(l_{i})$, $\unit{FP}(l_{i})$ and $\unit{FN}(l_{i})$ for each confidence level $l_i$:
%
\begin{align} \label{eq:TP,FN} 
	P(l_i)& = \frac{\unit{TP}(l_i)}{\unit{TP}(l_i)+\unit{FP}(l_i)},\\
	R(l_i)& = \frac{\unit{TP}(l_i)}{\unit{TP}(l_i)+\unit{FN}(l_i)}.
\end{align}
%
An example of precision $P(l_{i})$ and recall $R(l_{i})$ curves for different confidence levels is shown in \autoref{fig:PRcurve}a. 
%
\begin{figure}[!h]
	\newcommand{\widthfig}{.5\textwidth}
	\newcommand{\sizefig}{.4}
	\centering
	\subfloat[]{
	\includegraphics[scale=\sizefig]{\qd S407/images/PRconfidence}}
	\subfloat[]{
	\includegraphics[scale=\sizefig]{\qd S407/images/PR_Pinterpol}}
	\caption{(a) Example of the precision $P(l_{i})$ (blue) and recall $R(l_{i})$ (green) curve. (b) The precision/recall curve $P(R)$ (blue) with interpolated precision $P_{interp}(\tilde{R}$) (red).}
	\label{fig:PRcurve}
\end{figure}

Following \cite{Everingham2009}, the precision/recall (PR) curve $P(R)$ (\autoref{fig:PRcurve}a) is interpolated for $11$ equally spaced recall levels $\tilde{R}_{i}\in[0:.1:1]$ by taking the maximum precision for which the corresponding recall exceeds $\tilde{R}_{i}$ (\autoref{fig:PRcurve}b):
%
\begin{equation}
	P_{interp}(\tilde{R})=\max_{R;R\geq \tilde{R}}P(R).
\end{equation}
%
The precision/recall (PR) curve is interpolated in order to reduce the impact of ``wiggles'' in the PR curve (see \autoref{fig:PRcurve}b). Note that to obtain a high AP, the method must have precision at all levels of recall, penalising methods that can accurately estimate only few very bright sources. 
% ``wiggles'' in the precision/recall curve, caused by small variations in the ranking of examples. 

Average precision (AP) is a mean of interpolated precision:
%
\begin{equation}
	AP=\frac{1}{11}\sum_{\tilde{R}}{P_{interp}(\tilde{R})}.
	\label{eq:AP}
\end{equation}

%==========================================
%==========================================

\clearpage
\section{NMF for realistic microscopy datasets \label{sec:NMF-for-real}}

NMF model fitting for blinking quantum dots is challenging when applied to a dataset of around $1,000$ large images (each containing $\sim 10^{5}$ pixels and $\sim10^{3}-10^{4}$ QDs). Beside an excessively large computational time, the local minima in NMF fitting complicate the optimisation \cite{Kim2008}. We address this partly by dividing the data into overlapping patches, so that NMF is applied to each of patch individually, and then these results are ``stitched'' back together. We also develop further methods to reduce local optima problems in the fitting procedure. 

A pipe-line for NMF evaluation of a realistic microscopy datasets is described in this section.


%==========================================

\subsection{Pre-processing \label{sec:preproc}}

Raw data are calibrated such that the image intensity corresponds to photon counts. Each image is divided into patches of size $m\times m$ pixels with $o$ pixels overlap. We usually use $m = 25$ and $o = 5$. The overlap has been chosen as the estimated extent of a single in-focus point spread function. Each patch $p$ is reshaped into a vector by concatenation of columns, so that a patch in all $T$ frames creates a $m^{2}\times T$ data matrix $\bm{D}_{p}$.

To detect patches with low signal, the maximum intensity pixel in the time average of each patch $m_{p}=\max_{i}\left\langle \bm{D}_{p}(i,t)\right\rangle _{t}$ is compared to the maximum intensity pixel of the average of the whole data $m=\max_{i}\left\langle \bm{D}(i,t)\right\rangle _{t}$. The patches with $m_{p}/m< \theta_{m}$ are considered to contain only weak signal and are not considered for further evaluation. In our dataset se set $\theta_{m}=0.25.$

%==========================================
\clearpage
\subsection{Estimation of number of sources $K$\label{sub:Estimation-of-number-of-sources}}

The NMF model \autoref{eq:NMF model element-wise} requires prior knowledge about the number of sources $K$ to be separated. This is a difficult task for noisy datasets. In preliminary work we explored this on simulated data with models fitted for a range of $K$ values. 
%
\begin{figure}[!ht]
	\centering
	\newcommand{\sizee}{.25}		
	\newcommand{\sizebb}{.6}
	\newcommand{\ima}{$2\delta$} 
	\newcommand{\imb}{$1.5\delta$}
	\newcommand{\imc}{$\delta$}
	\newcommand{\pca}{, PCA}
	\newcommand{\data}{, wide-field}
	\newcommand{\lbd}{, lower bound}
	\newcommand{\mxc}{, max correlation}
	
	\subfloat[\ima \data]{
	\includegraphics[scale=\sizebb]{\qd S300/images/reslocalized_nc11_sc20}}
	\subfloat[\imb \data]{
	\includegraphics[scale=\sizebb]{\qd S300/images/reslocalized_nc11_sc15}}
	\subfloat[\imc \data]{
	\includegraphics[scale=\sizebb]{\qd S300/images/reslocalized_nc11_sc10}}\\
	
	\caption{Sum of the simulated frames. Red marks indicate the locations of the sources. Green circle shows the Airy disk (with radius $\delta$).  Ten sources contained within a disk with radius \ima \ (left column), \imb \ (middle column) and \imc \ (right column). The border of the disk is marded with red dashed circle.}	
	 \label{fig:K estimation data}
\end{figure}

\Autoref{fig:K estimation data} illustrates three different simulated datasets with 10 sources randomly scattered within an area of radius $2\delta$, $1.5\delta$ and $\delta$, respectively. $\delta$ was equal to diameter of an Airy disk, shown as a green circle in \autoref{fig:K estimation data}(c) ($\delta=0.61\frac{\lambda_{em}}{NA}$). This corresponds to the sources densities of $29$, $13$ and $7$ sources per $\unit{\mu m^{2}}$, respectively. The mean of the simulated frames, which corresponds to a wide-field image is shown as a grey-value image. Red marks indicate the true positions of the sources. Ten datasets with different geometrical configurations of the randomly scattered sources were simulated for each source density. 

A simple model comparison can obtained by using the Bayesian Information Criterion \cite{Bishop2006} where we find $\hat{\theta}$ that maximises $p(\bm{D}|K, \theta)$, and then add a penalty term for the $NK$ parameters contained in $\bm{W}$, so that models with larger $K$ would be more heavily penalised. BIC, however, did not provide a satisfactory results. We therefore estimated the number of sources $K$ using:

\begin{enumerate}
	\item 
	\emph{Principal Component Analysis (PCA)}	
	A crude estimation of $K$ can be obtained from a position of a ``kink'' in the plot of sorted principal values \autoref{fig:K estimation}(a-c). However, the ``kink'' is not obvious in the noisy data or data with high density of blinking sources, see \autoref{fig:K estimation}c.
	\item
	\emph{A variational lower bound (LB)} 	
	A variational approximation of the GaP model \autoref{sub:GaP} provide lower bound $\mathcal{L}$ on the likelihood $p(\bm{D}|K, \theta)$ by approximately integrating out the latent variables $\bm{h}_{k}$ \cite{Buntine2006}. To obtain the marginal likelihood $p(\bm{D}|K)$ it would be necessary to also integrate out $\theta$, but this is computationally challenging. We show in \autoref{fig:K estimation}(d-f) that in fact the lower bound already underestimates the value of $K$, so that $p(\bm{D}|K)$ would likely peak at even lower values of $K$ and thus systematically underestimate the number of sources.
	
	\item 
	\emph{Analysis of correlations in residuals (ACR)}	
	An alternative approach for estimating $K$ is to analyse the residuals of the ``data-model''. The entries of the $N\times T$ residual matrix $\bm{S}$:
	%
	\begin{equation}
		s_{nt}=\frac{d_{nt}-\sum_{k=1}^{K}w_{nk}h_{kt}}{\sqrt{\sum_{k=1}^{K}w_{nk}h_{kt}}}.
	\end{equation}
	%
	The factor $1/\sqrt{\sum_{k=1}^{K}w_{nk}h_{kt}}$ is applied in order to standardise the residuals (zero mean and unit variance) of Poisson distributed data. We can then compute the $N\times N$ correlation matrix 
	%
	\begin{equation}
		\bm{C}_{S}=\bm{SS}^{T},
	\end{equation}
	%
	and the $N\times N$ matrix of the correlation coefficients $\bm{R}_{S}$ with entries 
	%
	\begin{equation}
		r_{ij}=\frac{c_{ij}}{\sqrt{c_{ii}c_{jj}}}.
		\label{eq:Correlation in residuals}
	\end{equation}
	
	Underestimation of the number of sources ($K<K_{\mbox{true}}$) will lead to correlations between some pixels as the model will try to explain multiple sources with one component. For $K\geq K_{\mbox{true}}$ the correlations are expected to drop to a base level and the residuals become uncorrelated. We can pick the value of $K$ for which the residual correlations decrease to a certain level and where further increase of $K$ does not give any further improvement \autoref{fig:K estimation}(g-i).
\end{enumerate}
%
\begin{figure}[!ht]
	\centering
	\newcommand{\sizee}{.24}		
	\newcommand{\sizebb}{.6}
	\newcommand{\ima}{$2\delta$} 
	\newcommand{\imb}{$1.5\delta$}
	\newcommand{\imc}{$\delta$}
	\newcommand{\pca}{, PCA}
	\newcommand{\data}{, data}
	\newcommand{\lbd}{, lower bound}
	\newcommand{\mxc}{, max correlation}
	
%	\subfloat[\ima \data]{
%	\includegraphics[scale=\sizebb]{\qd S300/images/reslocalized_nc11_sc20}}
%	\subfloat[\imb \data]{
%	\includegraphics[scale=\sizebb]{\qd S300/images/reslocalized_nc11_sc15}}
%	\subfloat[\imc \data]{
%	\includegraphics[scale=\sizebb]{\qd S300/images/reslocalized_nc11_sc10}}\\
	\begin{tabular}{ccc}
	\subfloat[\ima \pca]{
	\includegraphics[scale=\sizee]{\qd S301/images/pca_nc20}}&
	\subfloat[\imb \pca]{
	\includegraphics[scale=\sizee]{\qd S301/images/pca_nc15}}&
	\subfloat[\imc \pca]{
	\includegraphics[scale=\sizee]{\qd S301/images/pca_nc10}}\tabularnewline
%	
%	\subfloat[\ima \llk]{
%	\includegraphics[scale=\sizee]{\qd S303/images/LogLikPoisson_nc20}}
%	\subfloat[\imb \llk]{
%	\includegraphics[scale=\sizee]{\qd S303/images/LogLikPoisson_nc15}}
%	\subfloat[\imc \llk]{
%	\includegraphics[scale=\sizee]{\qd S303/images/LogLikPoisson_nc10}}\\
	
	\subfloat[\ima \lbd]{
	\includegraphics[scale=\sizee]{\qd S303/images/LowerBound_nc20}}&
	\subfloat[\imb \lbd]{
	\includegraphics[scale=\sizee]{\qd S303/images/LowerBound_nc15}}&
	\subfloat[\imc \lbd]{
	\includegraphics[scale=\sizee]{\qd S303/images/LowerBound_nc10}}\tabularnewline
	
	\subfloat[\ima \mxc]{
	\includegraphics[scale=\sizee]{\qd S303/images/MaxCorrInResid_nc20_NMF}}&
	\subfloat[\imb \mxc]{
	\includegraphics[scale=\sizee]{\qd S303/images/MaxCorrInResid_nc15_NMF}}&
	\subfloat[\imc \mxc]{
	\includegraphics[scale=\sizee]{\qd S303/images/MaxCorrInResid_nc10_NMF}}
	\end{tabular}
	%
	\caption{$K$ estimation for $10$ sources contained within a disk with radius \ima \ (left column), \imb \ (middle column) and \imc \ (right column). Lines for three datasets with different configuration of the sources are shown. $K_{true}$ is marked with red vertical line.}	
	 \label{fig:K estimation}
\end{figure}

\begin{figure}[!hbt]
	\newcommand{\sizef}{.4}		
	\centering
	\subfloat[Maximum correlations in residuals]{
	\includegraphics[scale=\sizef]{\qd S301/images/KestHist_maxC_Kink}}
	\subfloat[Variational lower bound]{
	\includegraphics[scale=\sizef]{\qd S300/images/KestHist_lb}}\\
	\subfloat[PCA]{
	\includegraphics[scale=\sizef]{\qd S300/images/KestHist_PCA_Kink}}
	\subfloat[NMF iterative restarts]{
	\includegraphics[scale=\sizef]{\qd S560/figures/KestHist_NMFiter}}
	\caption{Histograms of the $K$ estimations with (a) analysis of correlations in residuals, (b) variational lower bound, (c) PCA and (d) iterative NMF. Simulated data with three different sources density: tens sources within a disk of $\delta$ (blue), $1.5\delta$ (green) and $2\delta$ (red).}
	\label{fig:K estimation hist}
\end{figure}
%
A reliable estimation of $K$ is a difficult task for higher source densities. \Autoref{fig:K estimation hist} shows the histograms of the estimated $K$s for ten different geometrical configurations of the sources with a given density. From the methods presented in this section  (\autoref{fig:K estimation hist}(a-c)), the analysis of the correlations in residuals (ACR) shows the best performance.

Both LB and ACR, require evaluation of the model for a reasonable range of possible $K$s. The range can be estimated from the PCA  (\autoref{fig:K estimation hist}(c)), because the prinicpal coefficients can be computed directly from the data matrix $\bm{D}$.

In the following section we will be discussing an iterative procedure of the NMF algorithm, which can deal with moderate overestimation of $K$  (estimated from PCA). The correct number of sources can be estimated additionally by analysing the optimised matrix $\bm{W}$ and selecting the ``credible'' sources $\bm{w_{k}}$. Therefore evaluation for only one overestimated value of $K$, rather then a range of $K$s, is required. The histogram of the $K$s estimated with the iterative algorithm is shown in  \autoref{fig:K estimation hist}(d) for comparison. This method gives the most acurate estimates of $K$. 

%==========================================
\clearpage
\subsection{Tackling local optima in NMF fitting with iterative restarts \label{sub: Iterative restarts}}

Although the Lee and Seung algorithm is convex with respect to $\bm{W}$ and $\bm{H}$ separately, it is non-convex in both simultaneously \cite{Lee2001}. Multiple restarts can be used to address the problem of local optima, but we have not found good solutions with this approach on the QD data. Instead, we exploit some prior knowledge about the problem, namely that PSFs are likely to have a fairly compact structure, see e.g. \autoref{fig:Simulted-PSF-different-focal-depths}. As the estimated sources $\bm{w_{k}}$ are normalised to have an $L_1$-norm of 1 (i.e.,\ $\sum_{j}w_{jk}=1$), we use the inverse $L_2$-norm to rank the columns $\bm{w_{k}}$'s of the matrix $\bm{W}$. Note that Hoyer's sparsity measure \autoref{eq:Hoyers sparsity} is a $L_{1}/L_{2}$ measure normalised to the $[0..1]$ interval \cite{Kim2008}. 

This leads to an iterative algorithm (see \autoref{alg:restarts}) where on iteration $j+1$ the first $j$ sorted sources $\{ \bm{w} \}_{1}^{j}$ (and corresponding $\{ \bm{h} \}_{1}^{j}$) are used as initial values for the first $j$ columns of $\bm{W}$ and the corresponding rows of $\bm{H}$. The remaining components are re-initialised from a uniform random distribution. Initial values of $\bm{W}$ and $\bm{H}$ for the $j+1$th iteration are therefore composed of the $j$ `sparsest' components of the previous iteration and $K-j$ randomly initialised components. The procedure runs until $j=K$. 

We used a crude over-estimation of $K$ with PCA which can be computed directly from data $\bm{D}$ prior to the evaluation: 
%
\begin{enumerate}
	\item
	We compute the sorted principal coefficients $\lambda_{j}$ of $\bm{D}$ ($\lambda_{1}>\lambda_{2}>...$). 
	\item
	$K$ is (over)estimated as the number of components which satisfy $\lambda_{j}/\lambda_{1}>t_{\unit{PCA}}$, where $t_{\unit{PCA}}$ is a threshold. 
\end{enumerate}
%
User should be able to test the source estimation procedure on a patch where the number of sources can be guessed (e.g.\ an area with sparse sources) to get a notion about the threshold. The threshold should be set such that is overestimates the true number of sources.

\begin{algorithm}
	\caption{Iterative restarts of the NMF}	
	\label{alg:restarts}
	\begin{enumerate}
		\item Set $\bm{W}_{init}$ and $\bm{H}_{init}$ as random positive matrices.
		\item Iterate for $j=1:K$ ($K$ is the (over) estimated number of sources.)
		\begin{enumerate}
			\item Run NMF with $\bm{W}_{init}$ and $\bm{H}_{init}$ as initial values.
			\item Sort columns of $\bm{W}$ according to $L_2$ norm and permute rows of $\bm{H}$ correspondingly.
			\item Replace first $j$ columns of $\bm{W}_{init}$ with first $j$ columns of sorted $\bm{W}$.
			\item Replace last $j+1:K$ columns of $\bm{W}_{init}$ with positive random vectors.
			\item Replace first $j$ rows of $\bm{H}_{init}$ with first $j$ rows of sorted $\bm{H}$.
			\item Replace last $j+1:K$ rows of $\bm{H}_{init}$ with positive random vectors.
		\end{enumerate}
	\end{enumerate}    
\end{algorithm}

The motivation for the iterative procedure \autoref{alg:restarts} is to progressively exploit the credible (and therefore sparse) components from the data.  

It should be noted that in contrast to the Hoyer's sparse NMF (\autoref{sub:Hoyer}) where the ``sparsity'' on the $\bm{w_{k}}$ is imposed as a ``hard'' constraint, the iterative procedure \autoref{alg:restarts} leads to ``soft'' constraints on the sparsity of $\bm{w_{k}}$. The sparse components are preferably reused in the following iterative restarts but are still allowed to change during the iterations. 

The iterative procedure is illustrated on simulated data of slanted line with eight attached PSFs in \autoref{fig:Iterative restarts}. The parameters of the simulations are discussed in  \autoref{sec:results} with illustration of a typical frame of the dataset \autoref{fig:Data-true-estimations}a and true sources \autoref{fig:Data-true-estimations}b.

The algorithm was run for $K=15$, with the last component reserved for background. The results of the first run (random initialisation) are shown in \autoref{fig:Iterative restarts}a. The individual PSFs (see \autoref{fig:Data-true-estimations}b) are spread across all $\bm{w}$s, and many of them contain a mixture of multiple PSFs. This is a typical solution corresponding to a local minimum of the objective function \autoref{eq:KL divergence}. As the iterative procedure progresses, realistic sources are gradually recovered, see \autoref{fig:Iterative restarts}b. After nine iterations, the first eight $\bm{w}$'s show credible PSFs, while the rest represent only noise, see \autoref{fig:Iterative restarts}c. Further iterations do not have a significant effect on the already estimated PSFs, see \autoref{fig:Iterative restarts}d. 

\begin{figure}[!htb]
	\newcommand{\widthfig}{.95\textwidth}
	\newcommand{\barspace}{-.7cm}
	\condcomment{\boolean{includefigs}}{ 
	\centering
		
		\subfloat[run 1 ]{
		\begin{tabular}{l}
			\includegraphics[width=\widthfig]{\qd S382/images/w_restart0_1toEnd}\vspace{\barspace}\tabularnewline
			\includegraphics[width=\widthfig]{\qd S382/images/w_restart0_1toEnd_intBars}
			\tabularnewline
		\end{tabular}}
		
		\subfloat[run 4 ]{
		\begin{tabular}{l}
			\includegraphics[width=\widthfig]{\qd S382/images/w_restart3_1toEnd}\vspace{\barspace}\tabularnewline
			\includegraphics[width=\widthfig]{\qd S382/images/w_restart3_1toEnd_intBars}
			\tabularnewline
		\end{tabular}}
		
		\subfloat[run 8 ]{
		\begin{tabular}{l}
			\includegraphics[width=\widthfig]{\qd S382/images/w_restart7_1toEnd} \vspace{\barspace}\tabularnewline
			\includegraphics[width=\widthfig]{\qd S382/images/w_restart7_1toEnd_intBars}
			\tabularnewline
		\end{tabular}}
		
		\subfloat[run 14 ]{
		\begin{tabular}{l}
			\includegraphics[width=\widthfig]{\qd S382/images/w_restart13_1toEnd} \vspace{\barspace}\tabularnewline
			\includegraphics[width=\widthfig]{\qd S382/images/w_restart13_1toEnd_intBars}\tabularnewline
		\end{tabular}}		
	}
	\caption{Illustration of the iterative restart procedure. Estimated sources after (a) 1, (b) 4, (c) 9 and (c) 14 runs of the algorithm. Bars below the figures show the maximum of the $\bm{w}$.}
	\label{fig:Iterative restarts}
\end{figure}

The iterative restart procedure leads to better local minima of the NMF optimisation problem. Moreover the $L_{2}$ norm sorting of the recovered $\bm{w}$s after each iteration (step 2a in \autoref{alg:restarts}) indirectly enhances sparsity of the $\bm{W}$'s columns. The sparse $\bm{w}$ with small $L_{2}$ norm are likely to be reused in subsequent restart (step 2c in \autoref{alg:restarts}), whereas the $\bm{w}$ with large $L_{2}$ norm are likely to be replaced by a random vector. This ``soft'' sparsity enhancement allows for higher flexibility of the recovered $\bm{w}$ and allows recovery of the sources with different individual sparsities such as the sources from different focal depths as shown in \autoref{fig:Iterative restarts}d. This is and advantage compared to a ``hard'' sparsity enhancement used in the Hoyer's algorithm (\autoref{sub:Hoyer}).

%==========================================

\subsection{Classification of the estimated sources\label{sub:Classification-of-sources}} \fix

The estimated sources can greatly vary in quality. While some $\bm{w}$'s are credible representation of a PSF, there are often $\bm{w}$'s which contain multiple PSFs. Because of the overestimation of the number of sources $K$ (\autoref{sub:Estimation-of-number-of-sources}), there are also $\bm{w}$'s present which correspond to background noise. Sources located close to the patch border and therefore partially missing should also be identified; these sources will likely appear in the adjacent patch entirely, because the overlap of the patches is set to approximately the extent of the (in focus) PSF (\autoref{sec:preproc}).
%
\begin{figure}[!htb]
	\newcommand{\fw}{.98\textwidth}
	\newcommand{\barspace}{-.55cm}
	\centering
	\begin{tabular}{l}			
		\includegraphics[width=\fw]{\qd S455/images/resw_1to16_col}\vspace{\barspace}\tabularnewline
		\includegraphics[width=\fw]{\qd S455/images/resw_1to16_col_barInt}\tabularnewline
		\includegraphics[width=\fw]{\qd S455/images/resw_17to32_col}\vspace{\barspace}\tabularnewline
		\includegraphics[width=\fw]{\qd S455/images/resw_17to32_col_barInt}\tabularnewline
		\includegraphics[width=\fw]{\qd S455/images/resw_33to48_col}\vspace{\barspace}\tabularnewline
		\includegraphics[width=\fw]{\qd S455/images/resw_33to48_col_barInt}\tabularnewline
		\includegraphics[width=\fw]{\qd S455/images/resw_49to64_col}\vspace{\barspace}\tabularnewline
		\includegraphics[width=\fw]{\qd S455/images/resw_49to64_col_barInt}\tabularnewline
		\includegraphics[width=\fw]{\qd S455/images/resw_65to80_col}\vspace{\barspace}\tabularnewline
		\includegraphics[width=\fw]{\qd S455/images/resw_65to80_col_barInt}\tabularnewline
	\end{tabular}	
	\caption{Selection of the credible $\bm{w}$s. The green and blue boxes indicates the estimated ``credible'' sources (with only one major global maximum). The sources with blue frame have the maximum closer then 2 pixels to the border and can be therefore considered as partly missing sources. The sources with red frame have two local maxima of comparable strength. Bars under the figures show the normalised maximum value of the estimated $\bm{w}_{k}$.}. 
	\label{fig:good w}	
\end{figure}
%
If all the sources are expected to be in-focus and therefore have a fairly compact PSF with one global maximum (\autoref{fig:Simulted-PSF-different-focal-depths} left), we can use a simple procedure for identifying reasonable $\bm{w}$s:
%
\begin{enumerate}
	\item
	Each estimated source $\bm{w}$ is convolved with in-focus point spread function (PSF) (generated from the parameters of the experimental setup). This is to smooth the noise in the results and to enhance the structures at the scale of PSF. 
	\item
	Number of local maxima bigger then $50\%$ of the global maximum is counted. 
\end{enumerate}

Only the sources with one local maximum of the convolution of PSF and $\bm{w}$ are considered for further evaluation. The distance of the maximum from the edge can indicate a partially missing source. 

The process is illustrated in \autoref{fig:good w} on $\bm{w}$ estimated from the simulated dataset of randomly scattered sources with density $50\unit{\mu m^{-2}}$ (\autoref{sub:Simul random}). The $\bm{w}$s considered as a ``credible'' are indicated with blue or green frame. The blue frame shows the sources with maximum closer then two pixels from the border. The red frame shows the $\bm{w}$ with two local minima of similar strength (at least $50\%$ of the strength of the stronger maximum).

This approach would, however, fail when used on data with out-of-focus PSF, because the images of the out-of-focus PSF do not have one compact global maximum (\autoref{fig:Simulted-PSF-different-focal-depths} right).

\emph{Discuss here the linear classifier?} \fix

%We use a linear classifier to automate the quality assessment of the estimated $\bm{w}_{k}$. We compute a set of features based on intensity values of $\bm{w}_{k}$, thresholded background image $\bm{b}_{k}=\bm{w}_{k}<t_b$, thresholded foreground image $\bm{f}_{k}=\bm{w}_{k}>t_f$ and an image of $\bm{w}_{k}$ smoothed with estimated in-focus PSF. The features are listed in \autoref{tab:Features}a and five classes for $\bm{w}_{k}$ are listed in \autoref{tab:Classes}b.
%
%
%\begin{table}[!h]
%	\begin{centering}		
%		\footnotesize{\subfloat[Features]{
%		\begin{tabular}{|c|c|}
%			\hline 
%			\textbf{\#} & \textbf{note}\tabularnewline
%			\hline
%			\hline 
%			\textbf{1} & $l^{2}$ norm ($\sqrt{\sum_{j}w_{jk}^{2}}$)\tabularnewline
%			\hline 
%			\textbf{2} & Smoothness of $\bm{w}_{k}$ (discrete version of $\int\left|\frac{\partial}{\partial x}w_{k}(x)\right|dx$).\tabularnewline
%			\hline 
%			\textbf{3} & Smoothness of $\bm{b}_{k}$ (discrete version of $\int\left|\frac{\partial}{\partial x}b_{k}(x)\right|dx$).\tabularnewline
%			\hline 
%			\textbf{4} & Maximum of the cross-correlation between $\bm{w}_{k}$ and PSF.\tabularnewline
%			\hline 
%			\textbf{5} & Difference between PSF smoothed image and original $\bm{w}_{k}$.\tabularnewline
%			\hline 
%			\textbf{6} & Sum of the foreground $\bm{f}_{k}$. $\sum_{j}a_{jk}$ $ $\tabularnewline
%			\hline 
%			\textbf{7} & Distance of the global maximum from the nearest edge.\tabularnewline
%			\hline
%		\end{tabular}}				
%		\textbf{\hspace{.2cm}}\subfloat[Classes]{
%		\begin{tabular}{|c|c|}
%			\hline 
%			\textbf{\#} & \textbf{note}\tabularnewline
%			\hline
%			\hline 
%			\textbf{1} & One credible PSF\tabularnewline
%			\hline 
%			\textbf{5} & One PSF partly missing \tabularnewline
%			\hline 
%			\textbf{2} & Two credible PSFs\tabularnewline
%			\hline 
%			\textbf{3} & Three credible PSFs\tabularnewline
%			\hline 
%			\textbf{4} & Multiple credible PSFs\tabularnewline
%			\hline 
%			\textbf{0} & Noise \tabularnewline
%			\hline
%		\end{tabular}}}
%	\end{centering}	
%	\caption{(a) Features and (b) classes for classification of $\bm{w}_{k}$.}\label{tab:Features}\label{tab:Classes}
%\end{table}

%==========================================

\subsection{Localisation and stitching\label{sub:Localisation-and-stitching}}
The individual estimated sources classified as a credible representation of the PSF can be now localised. Conventional LM techniques applies the maximum likelihood fitting of an in-focus PSF gaussian approximation to the estimated images \cite{Hess2006}. The localisation precision is estimated from the number of photons emitted by the sources in the frame from which the source was localised. The NMF estimated intensity matrix $\bm{H}$ gives us much greater flexibility as we have an access to the entire intensity profile of the source. We can therefore estimate number of all photons emitted by the source during the measurement. The sources close to the edge can be problematic to localise. If the source represents an in-focus PSF, then it should appear entirely in the adjacent patch and can be localised there. When dealing with images with mostly in focus PSFs we can simply discard the sources classified as ``partly missing'' (\autoref{sub:Classification-of-sources}). More problematic are the out-of-focus PSFs with extent larger then the overlap area. These sources have to be first stitched together before further processing.

%==========================================

\subsection{Visualisation of the results\label{sub:visualisation}}

\begin{figure}[!hbt]
	\newcommand{\sizef}{.8}		
	\newcommand{\textgaussdiff}{Gauss, $\sigma^{2} \propto$ 1/intenisty}
	\newcommand{\textgauss}{Gauss, $\sigma=3$ pixels}
	\newcommand{\textpowers}{Powers, $p=30$}
	\centering
	\subfloat[\textgaussdiff]{
	\includegraphics[scale=\sizef]{\qd S575/figures/oneeval_gaussFiltered_coordsEstTrue_diffSigma_bar04um}}
	\subfloat[\textgaussdiff]{
	\includegraphics[scale=\sizef]{\qd S575/figures/gaussFiltered_coordsEstTrue_diffSigma_bar04um}}\\	

%	\subfloat[\textgauss]{
%	\includegraphics[scale=\sizef]{\qd S575/figures/oneeval_gaussFiltered_coordsEstTrue_bar04um}}
%	\subfloat[\textgauss]{
%	\includegraphics[scale=\sizef]{\qd S575/figures/gaussFiltered_coordsEstTrue_bar04um}}\\	

	\subfloat[\textpowers]{
	\includegraphics[scale=\sizef]{\qd S575/figures/res_truepos_bar04um}}
	\subfloat[\textpowers]{
	\includegraphics[scale=\sizef]{\qd S575/figures/res_meaniter_coordTrue_bar04um}}
	\caption{Visualisatoin of the results. Left column shows the results of one NMF evluation. Right column shows the sum of ten NMF evaluations of the same dataset. (a,b) The conventional visualisation by placing gaussians located at the positions of the estimated sources. (c,d) Powers of $\bm{w_{k}}$s.  Scale bar $400 \unit{nm}$.}
	\label{fig:visualisation gaussf}
\end{figure}

The conventional way for visualisation of the LM results (STORM, PALM) is to sum gaussian functions placed in the estimated locations. The variance $\sigma^{2}$ of each gaussian reflects the ``uncertainty'' of the estimated position. This is usually set to be proportional to the inverese of number of photons $N$ emitted by the source. The motivation behind this is the Cramer-Rao (CR) lower bound on the localisatoin accuracy 
%
\begin{equation}
	\sigma_{CR}^{2} \approx \sigma_{Airy}^{2}/N,
\end{equation}
%
where the  $\sigma_{Airy}^{2}$ is the variance of the PSF Gaussian approximation \fixme{reference to the chapter about CR}. As the $\sigma$ is typically considerably smaller then the resolution limit, the rendered image can provide super-resolution information about the specimen's structure. 
	
In terms of the NMF procedure, this method replaces the credible estimated $\bm{w}$s with an ideal, sub-resolution PSFs centered at the estimated source's location. The intensity values for each source can be estimated from the intensity time profiles of each source (rows of $\bm{H}$). 

The conventional visualisation of the NMF evaluation of the synthetic dataset \autoref{sub:Simul hash} representing an artifical structure ($\mu = 1\unit{\mu m^{-1}}$, $d=100 \unit{nm}$) is shown in \autoref{fig:visualisation gaussf}a-b. The standard deviation of each gaussian was set to $\sigma=20\sigma_{CR}$. The left column displays the result of one evaluation, the right column shows the sum of six \fixme{make it ten} evaluaitons of the same dataset.

\begin{figure}[!hbt]
	\newcommand{\sizef}{.8}			
	\newcommand{\widthfig}{1\textwidth}	
	\centering
	\subfloat[Estimated $\bm{w}$s.]{
	\includegraphics[width=\widthfig]{\qd S575/figures/demowpow_rf1_pow1}}\\
	\subfloat[$\bm{w}$s resampled by a factor $r=4$ and taken to the power $p=10$]{
	\includegraphics[width=\widthfig]{\qd S575/figures/demowpow_rf4_pow5}}	
	\caption{Illustration of the $\bm{w_{k}}$s ``squeezeing''. Eight (out of 60) selected $\bm{w_{k}}$s  shown. The number in the top left corner is the index in the $L_{2}$ norm soreted $\bm{w}$s.}
	\label{fig:demo pow w}	
\end{figure}

We can use the estimated sources $\bm{w}$s directly without replacing them with ``ideal'' PSFs. By taking the power $p>1$ of the estimated sources $\bm{w}^{p}$ we achieve a ``shrinking'' of the individual $\bm{w}$ while keeping some characteristics of each source's shape (elongation along a certain direction, for example). 

Certain oversampling of $\bm{w_{k}}$s (for example zero-padding of the Fourier transform of the $\bm{w_{k}}'s image$) is needed before taking the higher powers $p$. \Autoref{fig:demo pow w} shows the original estimated $\bm{w}$s and the corresponding upsampled (by a factor of $r=4$) $\bm{w}$s taken to the power $p=5$. 

This approach allows taking into account even the $\bm{w_{k}}$s containing multiple sources ($\bm{w_{32}}$ in \autoref{fig:demo pow w}, for example).

If we normalise the $L_{1}$ norm of $\bm{w}^{p}$ to 1 ($\sum_{x} \bm{w}^{p}(x)=1$), we can reconstruct a ``super-resolution'' image by summing all $\bm{w_{k}}^{p}$, weighted by the corresponding mean intensity $\unit{mean}(\bm{h_{k}})$. The results for different values of $p$ are shown in \autoref{fig:demo pow w result} and compared with the conventional visualisation in \autoref{fig:visualisation gaussf}c,d.

\begin{figure}[!hbt]
	\newcommand{\sizef}{.45}			
	\newcommand{\widthfig}{1\textwidth}	
	\centering
	\subfloat[Wide-field]{
	\includegraphics[scale=\sizef]{\qd S575/figures/wf_bar04um}}
	\subfloat[$p=5$]{
	\includegraphics[scale=\sizef]{\qd S575/figures/res_meaniter_rf4_pow5_coordTrue_bar04um}}	
	\subfloat[$p=10$]{
	\includegraphics[scale=\sizef]{\qd S575/figures/res_meaniter_rf4_pow10_coordTrue_bar04um}}	
	\subfloat[$p=30$]{
	\includegraphics[scale=\sizef]{\qd S575/figures/res_meaniter_rf4_pow30_coordTrue_bar04um}}	
	\caption{(a) Sum projecton of the frames. (b-d) Reconstructed results from $\bm{w}^{p}$ for different values of $p$.}
	\label{fig:demo pow w result}	
\end{figure}


%==========================================
%==========================================
\clearpage
\section{Results \label{sec:results}}
We used simulated data for exploring the behaviour of the NMF in different experimental regimes. Average precision and estimated density were used as a quantitative quality assessments of the algorithm performance in  \autoref{sub:results - blinking behaviour} - \ref{sub:results - number of frames}. We also used the simulated dataset for comparing NMF with other techniques dealing with overlapping sources \autoref{sub:results - comparison}. The simulated data are described and illustrated in \autoref{sec:simulations}. 

The unique ability of the NMF to recover different individual overlapping PSFs is illustrated on simulated data in \autoref{sub:results - out of focus PSF} and on real data of randomly scattered out-of-focus PSFs in \autoref{sub:results - out of focus PSF real data}.

The evaluation of the real biological samples consisting of tubulin structures labeled with QDs is shown in \autoref{sub:results - tubulin}

%==========================================

\subsection{Effect of the source density and blinking behaviour \label{sub:results - blinking behaviour}}
The performance of the algorithm for different source densities types of the blinking behaviour was tested on simulated data. The parameters of the simulations are in \autoref{tab:Simulations parameters}. We used randomly scattered sources with different densities illustrated in \autoref{sub:Simul random}.

\begin{figure}[!h]
	\centering
	\newcommand{\sizef}{.35}
	\subfloat[uniform]{
	\includegraphics[scale=\sizef]{\qd S455/images/blinkmatS422}}
	\subfloat[telegraph ($\gamma=0.5$)]{
	\includegraphics[scale=\sizef]{\qd S455/images/blinkmatS445}}\\
	\subfloat[telegraph oversampled ($\gamma=5$)]{
	%\includegraphics[scale=\sizef]{\qd S455/images/blinkmatS450}}
	%{\includegraphics[scale=\sizef]{\qd S455/images/blinkmatS557}} % same varaince as random
	\includegraphics[scale=\sizef]{\qd S455/images/blinkmatS565}} % half variance compared to random
	\subfloat[telegraph oversampled ($\gamma=0.5$)]{
	\includegraphics[scale=\sizef]{\qd S455/images/blinkmatS455}}	
	\caption{Examples of blinking behaviour of one source (50 points out of 1000).}
	\label{fig:blinking}
\end{figure}

Four different blinking behaviours with fixed number of emitted photons were considered: 
%
\begin{enumerate}[(a)]
	\item
	Uniform random distribution between $0$ and $\max(n_{phot})$ illustrated in  \autoref{fig:blinking}a. 
	\item
	A telegraph process with rate $\gamma=0.5$, where the intensity is switching between $0$ and $\max(n_{phot})$ shown in \autoref{fig:blinking}b.
	\item
	The time axes was oversampled $q$ times and a telegraph process with a rate $\gamma/q$ was generated. Finally, the blinking was under-sampled $q$ times. \Autoref{fig:blinking}c illustrates the result for $\gamma=5$ which corresponds to $10\times$ faster blinking then the sampling frequency of the measurement. This leads to the averaging (smoothing) of the intermittent behaviour.
	\item
	The blinking simulated according to the description in (c) for $\gamma=0.5$ represents more realistic intermittent behaviour then the ``binary'' telegraph process described in (b), keeping the same blinking rate. The switching between two states is no longer synchronised with the sampling and which gives rise to intermediate intensity values, \autoref{fig:blinking}d. 
\end{enumerate}
	
\begin{figure}[!h]
	\centering
	\newcommand{\sizefig}{.40}
	\subfloat[Variance of blinking]{
	%\includegraphics[scale=\sizefig]{\qd S455/images/blinkmat_varS422S445S450S455}
	%\includegraphics[scale=\sizefig]{\qd S455/images/blinkmat_varS422S445S557S455} %the faster telegraph the same variance as random
	\includegraphics[scale=\sizefig]{\qd S455/images/blinkmat_varS422S445S565S455} %the faster telegraph the half variance as 
	\label{fig:blinking var}}
	\subfloat[average precision]{
	%\includegraphics[scale=\sizefig]{\qd S455/images/AP_letters_S422S445S450S455}
	%\includegraphics[scale=\sizefig]{\qd S455/images/APerrorbar_lettersS422S445S557S455} %the faster telegraph the same variance as random
	\includegraphics[scale=\sizefig]{\qd S455/images/APerrorbar_lettersS422S445S565S455} %the faster telegraph the half variance as random
	\label{fig:AP blinking}}
	\caption{(a) Variance of the intensity time profiles for four different blinking behaviours shown in \autoref{fig:blinking} and (b) corresponding average precision of the estimated results.}		
\end{figure}

The variance of the four different intensity profiles is shown in \autoref{fig:blinking var}. Note that NMF algorithm does not take the time sequence into account. Random permutation of the individual frames in the sequence does not have an effect on the evaluation.

We used \autoref{alg:restarts} to evaluate simulated data of randomly scattered sources for a range of densities $5-50\unit{\mu m^{-2}}$. The parameters of the simulations are in \autoref{tab:Simulations parameters} and an illustration of the datasets for is shown  in \autoref{fig:simulated data random}. 

The number of sources $K$ was set to $K=K_{true}+10$, where $K_{true}$ is the true number of sources used for simulation. 

%	\begin{figure}[!h]
%	\centering
%	\includegraphics[scale=.6]{\qd S455/images/AP_letters_S422S445S450S455}
%	\caption{Average Precision. Comparison of the different blinking behaviours from \autoref{fig:blinking}.}
%	\end{figure}

The average precision (\autoref{eq:AP}) for the four blinking behaviours from \autoref{fig:blinking} is shown in \autoref{fig:AP blinking}. The mean and the standard deviation from evaluation of five different configuration of randomly scattered sources is shown. 
%The severely under-sampled blinking \autoref{fig:blinking}c represents a difficult dataset with little variation of the sources intensity. The poor performance of the NMF on these data is therefore not surprising. 
\fix
\emph{comment on results}

%==========================================

\subsection{Effect of the number of frames\label{sub:results - number of frames}}
%Simulated data of randomly scattered sources (\autoref{sec:simulations}) were used to evaluate effect of the number of frames in dataset. We used a dataset with $3000$ frames and used first $[50,\,100,\,200,\,500,\, 1000]$ frames for evaluation. The achieved average precision (AP) for three different densities of the sources are shown in \autoref{fig:AP on frames}. 
%%
%\begin{figure}[!h]
%	\newcommand{\sizef}{.7}
%	\centering
%	\includegraphics[scale=\sizef]{\qd S486/figures/APS486S476S481}
%	\caption{Average Precision}
%	\label{fig:AP on frames}
%\end{figure}
%
%For low densities ($10\unit{\mu m^{-2}}$) the results reach the plateau AP about $90\%$ for 100 frames (blue curve in \autoref{fig:AP on frames}). Adding more frames does not increase AP.

What is the optimal way of acquiring data when we have a limited total acquisition time? Is it better to use longer acquisition time per frame to acquire smaller number of frames with better signal-to-noise ratio in each frame? Or rather use as fast acquisition as possible acquiring many frames? 

\begin{figure}[!htb]	
	\newcommand{\widthfig}{1\textwidth}
	\centering	
	\subfloat[original data]{
	\includegraphics[width=\widthfig]{\qd S537/figures/dpixc_1to8_subf1}}
	
	\subfloat[subsampled $2\times$]{
	\includegraphics[width=\widthfig]{\qd S537/figures/dpixc_1to8_subf2}}
	
	\subfloat[subsampled $10\times$]{
	\includegraphics[width=\widthfig]{\qd S537/figures/dpixc_1to8_subf10}}
	
%	\subfloat[subsampled $20\times$]{
%	\includegraphics[width=\widthfig]{\qd S537/figures/dpixc_1to8_subf20}}		
	\caption{First eight frames of subsampled simulated data. }
	\label{fig:subsampled data}
\end{figure} 

To address these question we tested the algorithm on simulated data of randomly scattered sources (\autoref{sub:Simul random}). The parameters of the simulation were taken from \autoref{tab:Simulations parameters} but the maximum of the sources' intensity $\max(n_{phot})$ was set to 300. This represents weak sources with a fast acquisition time. The telegraph process \autoref{fig:blinking}c was used for simulated intensity profiles. 

Several frames of the dataset are shown in \autoref{fig:subsampled data}a. From the dataset of $1000$ frames we generated four more datasets by subsampling the data $q=2,\,5,\,10$ and $20$ times by summing the $q$ subsequent frames. If  we neglect the read-out noise of the camera, this corresponds to data taken with $q$ times longer acquisition time per frame. The subsampled data consist of $1000/q$ frames. Several frames of the subsampled data for $q=2$ and $10$ are shown in \autoref{fig:subsampled data}b,c, respectively. 

\fix emph{Add estimated density graph + error-bars}.
\begin{figure}[!h]	
	\newcommand{\wf}{.9\textwidth}
	\centering
	\subfloat[Average precision]{
	\includegraphics[width=\wf]{\qd S537/figures/APS537S527S532}}\\
	\caption{Average precision as a function of number of subsampled frames.}
	\label{fig:AP subsampled}
\end{figure}

AP computed for NMF evaluated data for three different source densities is shown in \autoref{fig:AP subsampled}. 

The subsampling of the data gives higher signal-to-noise rate in each frame, however the sources appear to be more in the ``ON'' state because of their ``OFF'' periods get averaged out. 

\emph{Maybe rather try this experiment with uniform random blinking. Because here the blinking rate is set to 0.5 (every other frame switch) and therefore any subsampling makes it worse.}

%==========================================
\clearpage
\subsection{Comparison with other methods\label{sub:results - comparison}} %Main competitors 3B \cite{Cox2011}, CSSTORM \cite{Zhu2012} and SOFI \cite{Dertinger2009}.	
We used simulated data of randomly scattered overlapping sources (\autoref{sub:Simul random}) to compare the performance of the NMF with CSSTORM \cite{Zhu2012}, and 3B analysis \cite{Cox2011}, where the code is freely available. 

Data-stets with densities $[10 - 40]\ \unit{sources/\mu m^{2}}$ were used. An empty margin of three pixels was left along every edge of the frames to ensure all there are no partially missing PSFs. The sum projection of the frames is shown in \autoref{fig:density 10 wf}-\ref{fig:density 40 wf}. Several individual data frames are shown in \autoref{fig:simulated data random}, illustrating highly overlapping sources in each frame (dataset displayed in \autoref{fig:simulated data random} does not contain the 3 pixel empty margin). The blinking behaviour was simulated as an oversampled telegraph process with switching rate $\gamma=0.5$ as described in \autoref{sub:results - blinking behaviour} (d) and illustrated in \autoref{fig:blinking}d.

For CSSTORM and 3B evaluation the true background value of $100$ photons per pixel per frame was subtracted before evaluation, clipping the negative values. The true PSF was provided to both CSSTORM and 3B algorithms. 

\begin{figure}[!h]
	\centering
	\newcommand{\sizef}{.38}
	\subfloat[density]{
	\includegraphics[scale=\sizef]{\qd S455/images/densitytrueestNMFCSSTORMCSSTORMind3B}}
	\subfloat[AP]{
	\includegraphics[scale=\sizef]{\qd S455/images/AP_NMFCSSTORMCSSTORMind3B}}
	
	\caption{Comparison of the NMF and CSSTORM evaluation of the randomly scattered PSFs. The blue curve corresponds to NMF (the mean and the standard deviation from five different configuration of the randomly scattered sources), green curve to projected CSSTORM image and green curve to values estimated from individual frames of the CSSTORM results. (a) Density (b) Average precision}
	\label{fig:comparison AP,dens}
\end{figure}
%
The estimated density and the AP values obtained from the results of the NMF algorithm are denoted as \textsf{NMF} and are shown as blue lines in \autoref{fig:comparison AP,dens}. The mean and the standard deviation from five different configuration of the randomly scattered sources are shown. The visualisation of the results (\autoref{sub:visualisation}, $p=4,\,q=4$ used) for datasets with four different densities is shown as a grey-scale image in \autoref{fig:density 10 NMF}-\ref{fig:density 40 NMF}. The green crosses show the ML fit of a gaussian function to the NMF estimated $\bm{w_{k}}$ selected from by the procedure described in \autoref{sub:Classification-of-sources}.

CSSTORM processes each input frame individually, independent of the rest of the dataset. It tries to recover a sparse distribution of the fluorophores in each frame considering a known PSF.The output for each frame is a ``high resolution'' frame showing the possible positions of the sources on a sub-pixel grid (8 times oversampled). Following \cite{Zhu2012}, we estimated the position of each source as a centre of mass of the small clusters formed on a sub-pixel grid. The AP (the mean of AP from individual frames) and the estimated density (the maximum of estimated densities from the individual frames) are denoted as \textsf{CSSTORMind} and are shown as red curves in \autoref{fig:comparison AP,dens}.

We also processed the sum of all CSSTORM output frames which summarises all the estimated sources. The summed image was filtered with gaussian kernel ($\sigma=1\unit{pixel}$). The result for different densities of the sources is shown in \autoref{fig:density 10 CSSTORM}-\ref{fig:density 40 CSSTORM}. The local maxima stronger then $5\%$ of the global maximum were identified. The positions of these local maxima were used as estimated sources' positions and used for computation of the AP. The true positives were used for density estimation (\autoref{sec:evaluation}). The results are denoted as \textsf{CSSTORM} and are shown as green curves in \autoref{fig:comparison AP,dens}.

The results in \autoref{fig:comparison AP,dens} suggest that CSSTORM cannot recover enough sources in individual frames (red curves). The density is severely underestimated which leads to many false negatives (FN) and therefore low recall values \autoref{eq:TP,FN} which penalises AP. However, CSSTORM recovers some subset of the sources in each frame and the sum projection show dramatically improved AP and density estimation. It should be noted, however, that the estimated density and AP from the sum projection is highly dependent on the threshold for considering local maxima (see above). We chose the threshold of $5\%$, because for this value the number of local maxima roughly correspond to the true number of sources $K_{true}$.

%parameters adjusted for the 3B evaluatiom: \texttt{blur.mu=0.37261, blur.sigma=0.1, intensity.rel\_sigma=}$K_{true}$
We also used 3B analysis for the simulated datasets. The prior parameters for the size of the PSF were adjusted to the true values. Also the true number of sources $K_{true}$ was used as an initial number of spots in the model.The 3B algorithm was run for at least 50 iterations. Following \cite{Cox2011}, the output coordinates of the 3B analysis were placed on $100\times$ oversampled grid ($0.8\unit{nm}$ pixel-size) and convolved with a gaussian ($\sigma=10\unit{pixels}$, which corresponds to $\sigma=8\unit{nm}$). The resulting image is shown in as a grey-scale image in \autoref{fig:density 10 3B}-\ref{fig:density 40 3B}. Similar to analysis of the projected CSSTORM, we identified a local minima in the images. Only the minima above a certain threshold were considered for evaluation. The threshold was set individually for each images, such that the number of local maxima roughly corresponds to the number of sources considered by the 3B analysis after 50 iterations. The estimated density and the AP are denoted as \textsf{3B} and are shown in \autoref{fig:comparison AP,dens} as cyan lines. 

Experiments with simulated data shown above are useful for a quantitative comparison of the different methods. Randomly scattered sources are, however, of little practical interest. The main motivation of the super-resolution microscopy is to recover sub-diffraction details of the sample structure. Therefore we used simulated data with sources attached to an artificial structure to compare the performance of the three methods. Simulated data are illustrated in \autoref{sub:Simul hash} with parameters shown in  \autoref{tab:Simulations parameters} .
\fix \emph{comment on results}


\begin{figure}[!h]
	\centering
	\newcommand{\sizef}{.95}
	\subfloat[wide-field]
	{
	\includegraphics[scale=\sizef]{\qd /S500/figures/wf_dens10_simiter1_bar4um}
	\label{fig:density 10 wf}
	}
	\subfloat[NMF]
	{
	\includegraphics[scale=\sizef]{\qd /S500/figures/demo_dens10_simiter1_bar4um}
	\label{fig:density 10 NMF}
	}\\
	\subfloat[CSSTORM]
	{
	\includegraphics[scale=\sizef]{\qd S492/figures/demo_dens10_simiter1_bar4um}
	\label{fig:density 10 CSSTORM}
	}
	\subfloat[3B]
	{
	\includegraphics[scale=\sizef]{\qd S494/figures/demo_dens10_simiter1_bar4um}
	\label{fig:density 10 3B}
	}	
	\caption{Comparioins of the results for simulated of randomly scattered sources with density $10\unit{\mu m^{-2}}$ ($14$ sources in total). Sum projection of the dataset is shown in (a). Red circles show the true locations of the sources. The radius of the circles $r=0.7\unit{pixels}$ ($56\unit{nm}$) indicates the true-positive threshold distance. For further information see \autoref{sec:evaluation}. Scale bar 400\unit{nm}.}
	\label{fig:comparison density 10}
\end{figure}

\begin{figure}[!h]
	\centering
	\newcommand{\sizef}{.95}
	\subfloat[wide-field]
	{
	\includegraphics[scale=\sizef]{\qd /S500/figures/wf_dens20_simiter1_bar4um}
	\label{fig:density 20 wf}
	}
	\subfloat[NMF]
	{
	\includegraphics[scale=\sizef]{\qd /S500/figures/demo_dens20_simiter1_bar4um}
	\label{fig:density 20 NMF}
	}\\
	\subfloat[CSSTORM]
	{
	\includegraphics[scale=\sizef]{\qd S492/figures/demo_dens20_simiter1_bar4um}
	\label{fig:density 20 CSSTORM}
	}
	\subfloat[3B]
	{
	\includegraphics[scale=\sizef]{\qd S501/figures/demo_dens20_simiter1_bar4um}
	\label{fig:density 20 3B}
	}	
	\caption{Comparison of the results for simulated of randomly scattered sources with density $20\unit{\mu m^{-2}}$ ($14$ sources in total). Sum projection of the dataset is shown in (a). Red circles show the true locations of the sources. The radius of the circles $r=0.7\unit{pixels}$ ($56\unit{nm}$) indicates the true-positive threshold distance. For further information see \autoref{sec:evaluation}. Scale bar 400\unit{nm}.}
	\label{fig:comparison density 20}
\end{figure}

\begin{figure}[!h]
	\centering
	\newcommand{\sizef}{.95}
	\subfloat[wide-field]
	{
	\includegraphics[scale=\sizef]{\qd /S500/figures/wf_dens30_simiter1_bar4um}
	\label{fig:density 30 wf}
	}
	\subfloat[NMF]
	{
	\includegraphics[scale=\sizef]{\qd /S500/figures/demo_dens30_simiter1_bar4um}
	\label{fig:density 30 NMF}
	}\\
	\subfloat[CSSTORM]
	{
	\includegraphics[scale=\sizef]{\qd S492/figures/demo_dens30_simiter1_bar4um}
	\label{fig:density 30 CSSTORM}
	}
	\subfloat[3B]
	{
	\includegraphics[scale=\sizef]{\qd S495/figures/demo_dens30_simiter1_bar4um}
	\label{fig:density 30 3B}
	}	
	\caption{Comparison of the results for simulated of randomly scattered sources with density $30\unit{\mu m^{-2}}$ ($14$ sources in total). Sum projection of the dataset is shown in (a). Red circles show the true locations of the sources. The radius of the circles $r=0.7\unit{pixels}$ ($56\unit{nm}$) indicates the true-positive threshold distance. For further information see \autoref{sec:evaluation}. Scale bar 400\unit{nm}.}
	\label{fig:comparison density 30}
\end{figure}

\begin{figure}[!h]
	\centering
	\newcommand{\sizef}{.95}
	\subfloat[wide-field]
	{
	\includegraphics[scale=\sizef]{\qd /S500/figures/wf_dens40_simiter1_bar4um}
	\label{fig:density 40 wf}
	}
	\subfloat[NMF]
	{
	\includegraphics[scale=\sizef]{\qd /S500/figures/demo_dens40_simiter1_bar4um}
	\label{fig:density 40 NMF}
	}\\
	\subfloat[CSSTORM]
	{
	\includegraphics[scale=\sizef]{\qd S492/figures/demo_dens40_simiter1_bar4um}
	\label{fig:density 40 CSSTORM}
	}
	\subfloat[3B]
	{
	\includegraphics[scale=\sizef]{\qd S516/figures/demo_dens40_simiter1_bar4um}
	\label{fig:density 40 3B}
	}	
	\caption{Comparison of the results for simulated of randomly scattered sources with density $40\unit{\mu m^{-2}}$ ($14$ sources in total). Sum projection of the dataset is shown in (a). Red circles show the true locations of the sources. The radius of the circles $r=0.7\unit{pixels}$ ($56\unit{nm}$) indicates the true-positive threshold distance. For further information see \autoref{sec:evaluation}. Scale bar 400\unit{nm}.}
	\label{fig:comparison density 40}
\end{figure}


%%%% original figures grouped together according to the method
%\begin{figure}[!h]
%	\centering
%	\newcommand{\sizef}{1}
%	\subfloat[density $10\unit{\mu m^{-2}}$, $14$ sources]{
%	\includegraphics[scale=\sizef]{\qd /S500/figures/wf_dens10_simiter1_bar4um}}
%	\subfloat[density $20\unit{\mu m^{-2}}$, $29$ sources]{
%	\includegraphics[scale=\sizef]{\qd S500/figures/wf_dens20_simiter1_bar4um}}\\
%	\subfloat[density $30\unit{\mu m^{-2}}$, $43$ sources]{
%	\includegraphics[scale=\sizef]{\qd S500/figures/wf_dens30_simiter1_bar4um}}
%	\subfloat[density $40\unit{\mu m^{-2}}$, $58$ sources]{
%	\includegraphics[scale=\sizef]{\qd S500/figures/wf_dens40_simiter1_bar4um}}
%	
%	\caption{Sum projection of four datasets with different densities of the randomly scattered sources. Red circles show the true sources. The radius of the circles $r=0.7\unit{pixels}$ ($56\unit{nm}$) indicates the true-positive threshold distance. For further information see \autoref{sec:evaluation}. Scale bar 400\unit{nm}.}
%	\label{fig:wf demo}
%\end{figure}
%
%\begin{figure}[!h]
%	\centering
%	\newcommand{\sizef}{1}
%	\subfloat[density $10\unit{\mu m^{-2}}$]{
%	\includegraphics[scale=\sizef]{\qd /S500/figures/demo_dens10_simiter1_bar4um}}
%	\subfloat[density $20\unit{\mu m^{-2}}$]{
%	\includegraphics[scale=\sizef]{\qd S500/figures/demo_dens20_simiter1_bar4um}}\\
%	\subfloat[density $30\unit{\mu m^{-2}}$]{
%	\includegraphics[scale=\sizef]{\qd S500/figures/demo_dens30_simiter1_bar4um}}
%	\subfloat[density $40\unit{\mu m^{-2}}$]{
%	\includegraphics[scale=\sizef]{\qd S500/figures/demo_dens40_simiter1_bar4um}}
%	
%	\caption{Demonstration of the NMF results. The results are visualised as a weighted sum of $\bm{w}^{p}$ (with$p=5$) as described in \autoref{sub:visualisation}. Red circles show the true sources. The radius of the circles indicates the true-positive threshold distance $r=56\unit{nm}$. For further information see \autoref{sec:evaluation}. Green crosses show the local maxima in the image. ($4\times$ oversampled original). Scale bar 400\unit{nm}.}
%	\label{fig:NMF demo}
%\end{figure}
%
%\begin{figure}[!h]
%	\centering
%	\newcommand{\sizef}{1}
%	\subfloat[density $10\unit{\mu m^{-2}}$]{
%	\includegraphics[scale=\sizef]{\qd S492/figures/demo_dens10_simiter1_bar4um}}
%	\subfloat[density $20\unit{\mu m^{-2}}$]{
%	\includegraphics[scale=\sizef]{\qd S492/figures/demo_dens20_simiter1_bar4um}}\\
%	\subfloat[density $30\unit{\mu m^{-2}}$]{
%	\includegraphics[scale=\sizef]{\qd S492/figures/demo_dens30_simiter1_bar4um}}
%	\subfloat[density $40\unit{\mu m^{-2}}$]{
%	\includegraphics[scale=\sizef]{\qd S492/figures/demo_dens40_simiter1_bar4um}}
%	
%	\caption{Demonstration of the CSSTORM results. Gaussian filtered sum projection  of all output frames is shown. Red circles show the true sources. The radius of the circles indicates the true-positive threshold distance $r=56\unit{nm}$. For further information see \autoref{sec:evaluation}. Green crosses show the local maxima in the image. ($8\times$ oversampled original). Scale bar 400\unit{nm}.}
%	\label{fig:CSSTORM demo}
%\end{figure}
%
%\begin{figure}[!h]
%	\centering
%	\newcommand{\sizef}{1}
%	\subfloat[density $10\unit{\mu m^{-2}}$]{
%	\includegraphics[scale=\sizef]{\qd S494/figures/demo_dens10_simiter1_bar4um}} %70 iterations>20 points (initial 10 points); 50 iterations>18 points
%	\subfloat[density $20\unit{\mu m^{-2}}$]{
%	\includegraphics[scale=\sizef]{\qd S501/figures/demo_dens20_simiter1_bar4um}}\\ %38 iterations>39 points (initial 29 points)
%	\subfloat[density $30\unit{\mu m^{-2}}$]{
%	\includegraphics[scale=\sizef]{\qd S495/figures/demo_dens30_simiter1_bar4um}} %217 iterations>58 points (initial 40 points); 50 iterations>43 points.	
%	\subfloat[density $40\unit{\mu m^{-2}}$]{
%	\includegraphics[scale=\sizef]{\qd S516/figures/demo_dens40_simiter1_bar4um}} %50 iterations>62 points (initial 58 points)
%	
%	\caption{Demonstration of the 3B results. Red circles show the true sources. The radius of the circles indicates the true-positive threshold distance $r=56\unit{nm}$. For further information see \autoref{sec:evaluation}. Green crosses show the local maxima in the 3B image. ($100\times$ oversampled original). Scale bar 400\unit{nm}.}
%	\label{fig:3B demo}
%\end{figure}

\begin{figure}[!h]
	\centering
	\newcommand{\sizef}{.95}
	\subfloat[wide-field]
	{
	\includegraphics[scale=\sizef]{\qd /S568/figures/wf_bar4um}
	\label{fig:hash wf}
	}
	\subfloat[NMF]
	{
	\includegraphics[scale=\sizef]{\qd /S568/figures/res_bar4um}
	\label{fig:hash NMF}
	}\\
	\subfloat[CSSTORM]
	{
	\includegraphics[scale=\sizef]{\qd S566/figures/res_bar4um}
	\label{fig:hash CSSTORM}
	}
	\subfloat[3B]
	{
	\includegraphics[scale=\sizef]{\qd S567/figures/res_bar4um}
	\label{fig:hash 3B}
	}	
	\caption{Scale bar 400\unit{nm}.}
	\label{fig:comparison hash}
\end{figure}

\begin{figure}[!h]
	\centering
	\newcommand{\sizef}{.95}
	\subfloat[wide-field]
	{
	\includegraphics[scale=\sizef]{\qd /S570/figures/wf_bar4um}
	\label{fig:hash wf  dens 12}
	}
	\subfloat[NMF]
	{
	\includegraphics[scale=\sizef]{\qd /S570/figures/res_bar4um}
	\label{fig:hash NMF dens 12}
	}\\
	\subfloat[CSSTORM]
	{
	\includegraphics[scale=\sizef]{\qd S571/figures/res_bar4um}
	\label{fig:hash CSSTORM  dens 12}
	}
	\subfloat[3B]
	{
	\includegraphics[scale=\sizef]{\qd S572/figures/res_bar4um}
	\label{fig:hash 3B  dens 12}
	}	
	\caption{Scale bar 400\unit{nm}.}
	\label{fig:comparison hash dens 12}
\end{figure}

%==========================================

\clearpage
\subsection{Out of focus PSFs\label{sub:results - out of focus PSF}\label{sub:results - out of focus PSF real data}}
%
\begin{figure}[!htb]	
	\newcommand{\widthfig}{0.95\textwidth}
	\newcommand{\barspace}{-.5cm}
	\condcomment{\boolean{includefigs}}{ 
	\centering	
	\subfloat[Data]{	
	\begin{tabular}{l}
		\includegraphics[width=\widthfig]{\qd S382/images/dpixc_1to8}		
	\end{tabular}}\\	
	\subfloat[True sources]{
	\begin{tabular}{l}
		\noindent		
		\includegraphics[width=\widthfig]{\qd S382/images/wtrue_l2sort}\vspace{\barspace}\tabularnewline
		\includegraphics[width=\widthfig]{\qd S382/images/wtrue_l2sort_intBars}\tabularnewline
	\end{tabular}}\\
	\subfloat[True sources corrupted with noise]{
	\begin{tabular}{l}
		\includegraphics[width=\widthfig]{\qd S382/images/wtrue_l2sort_noise}\vspace{\barspace}\tabularnewline
		\includegraphics[width=\widthfig]{\qd S382/images/wtrue_l2sort_noise_intBars}\tabularnewline
	\end{tabular}}\\	
	\subfloat[Estimated sources]{
	\begin{tabular}{l}
		\includegraphics[width=\widthfig]{\qd S382/images/resw_1to8}\vspace{\barspace}\tabularnewline
		\includegraphics[width=\widthfig]{\qd S382/images/resw_1to8_intBars}\tabularnewline
	\end{tabular}}}
	\caption{Simulated data of eight sources. (a) Eight frames (out of 500) of the simulated data set. (b) The true sources. (c) Noisy version of the true sources with their maximum intensity. (d) The first 8 estimated sources (see \autoref{fig:Iterative restarts}d for all $\bm{w}$'s.) Bars under the figures show the maximum of the estimated $\bm{w}_{k}$. }
	\label{fig:Data-true-estimations}
\end{figure} 
%
There is no assumption about the shape of the estimated components $\bm{w_{k}}$ in the NMF updates \autoref{eq:NMF classic updates}. It is therefore possible to recover individual sources each having a different shape. We demonstrate this on simulated data of eight blinking QDs attached to a bar slanting in depth, see \autoref{fig:Data-true-estimations}a. The individual simulated sources were separated by 200 nm and the axial difference between the tips of the bar was $1600$ nm. Other parameters of the simulation are: emission wavelength 625 nm, numerical aperture 1.3, refractive index 1.5, edge size of a pixel in the image plane 100 nm, $T=500$, mean number of photons per source 1500, background photons/pixel 70, uniform distribution of blinking. 

The true sources (individual PSFs) are shown in \autoref{fig:Data-true-estimations}b and their noisy version (obtained from the frame with the maximum intensity of each source) is shown in \autoref{fig:Data-true-estimations}c. The results of the iterative procedure \autoref{alg:restarts} (several steps of the procedure are illustrated in \autoref{fig:Iterative restarts}) is shown in \autoref{fig:Data-true-estimations}d. The correspondence of the estimated sources $\bm{w}$ (first eight out of 16 sources form \autoref{fig:Iterative restarts}) to the true sources shown in \autoref{fig:Data-true-estimations} demonstrated the ability of the NMF to recover sources with individual different shapes from noisy data with highly overlapping emitters (see \autoref{fig:Data-true-estimations}). 

It should be noted that this is beyond the ability of either 3B analysis \cite{Cox2011} or CSSTORM \cite{Zhu2012} discussed in \autoref{sub:results - comparison}. ICA analysis also allows different shapes of individual emitters, however, as shown in \autoref{sub:ICA} ICA performance is poor for when applied to noisy data.   

We applied the iterative procedure \autoref{alg:restarts} on a movie of real out-of-focus QDs. We analysed $1000$ frames acquired with $50\unit{ms/frame}$ acquisition time. The results are shown in \autoref{fig:Real-data-QDrandom}a. 
 
Credible out-of-focus PSFs from different focal depths (cf. \autoref{fig:Simulted-PSF-different-focal-depths}) have been recovered (the first two rows in \autoref{fig:Real-data-QDrandom}b). The $\bm{W}_{k}$s in the last row of \autoref{fig:Real-data-QDrandom}b are mostly noise contribution. The mean brightness of these sources (estimated form $\bm{H}$) is about 10\% of the brightest $\bm{W}_{k}$.
%
\begin{figure}[!htb]
	\newcommand{\sizeresw}{.85}
	\newcommand{\barspace}{-.6cm}
	\condcomment{\boolean{includefigs}}{ 
	\centering
	\begin{tabular}{l}
		\subfloat[Data]{
		\includegraphics[scale=\sizeresw]{\qd S392/images/dpixc_randind}}
	\end{tabular}		
	\subfloat[Estimated sources]{
	\begin{tabular}{l}			
		\includegraphics[scale=\sizeresw]{\qd S392/images/resw_1to11}\vspace{\barspace}\tabularnewline
		\includegraphics[scale=\sizeresw]{\qd S392/images/resw_1to11_intBars}\tabularnewline
		\includegraphics[scale=\sizeresw]{\qd S392/images/resw_12to22}\vspace{\barspace}\tabularnewline
		\includegraphics[scale=\sizeresw]{\qd S392/images/resw_12to22_intBars}\tabularnewline
		\includegraphics[scale=\sizeresw]{\qd S392/images/resw_23to33}\vspace{\barspace}\tabularnewline
		\includegraphics[scale=\sizeresw]{\qd S392/images/resw_23to33_intBars}\tabularnewline			
	\end{tabular}}	
	}
	\caption{Real data of randomly scattered QDs. (a) Eleven randomly selected frames (out of $1,000$) of the overlapping PSFs produced by blinking QDs. (b) Estimated sources $\bm{w}_{k}$ sorted according to their estimated mean brightness. Bars below each figure show the maximum of the $\bm{w}_{k}$.}
	\label{fig:Real-data-QDrandom}	
\end{figure}

%==========================================

\subsection{Real data: QD stained tubulin fibres\label{sub:results - tubulin}}
We have applied the pipeline to a stack of $T=1000$, $128\times128$ images of tubulin fibres labelled with QDs (\autoref{fig:Patches}). The parameters of the experiment were: excitation wavelength $405 \unit{nm}$, emission wavelength $625 \unit{nm}$, exposure time $50 \unit{ms/frame}$, numerical aperture $1.4$, refractive index $1.52$, edge size of a pixel in image plane $79 \unit{nm}$, quantum dots (the emission wavelength $\lambda=625 \unit{nm}$).The dataset was divided into $25 \times 25$ patches (see \autoref{sec:preproc}), and only patches with sufficiently strong signal (thick boxes in \autoref{fig:Patches}) are considered for further evaluation (see \autoref{sec:preproc}). The number of sources within each patch is over-estimated via principal components analysis (\autoref{sub:Estimation-of-number-of-sources}). Results of the procedure applied to the patch \texttt{24} from \autoref{fig:Patches}, where all the sources are approximately in the same focal plane, are shown in \autoref{fig:Real-data-patch-B24}b. \emph{say more} \fix

\begin{figure}[!htb]
	\centering
	\condcomment{\boolean{includefigs}}{ 
	\includegraphics[scale=.9]{\qd S364/results/dataChunks}}
	
	\caption{Division of the dataset into smaller patches. The time average of the data is shown as a grey-valued image. Boxes with thick lines will be considered for NMF evaluation. Boxes with thin lines are considered to be empty. The identifier of the patch and the (over)estimated number of sources ($K$) are shown in each box.}
	\label{fig:Patches}
\end{figure}

\begin{figure}[!htb]
	\newcommand{\widthf}{.95\textwidth}
	\newcommand{\barspace}{-.6cm}
	\condcomment{\boolean{includefigs}}{ 
	\centering
	\subfloat[Data]{			
	\begin{tabular}{l}
		\includegraphics[width=\widthf]{\qd S364/results/dpixc_randind}\tabularnewline
	\end{tabular}}	
					
	\subfloat[Estimated sources]{				
	\begin{tabular}{l}
		\includegraphics[width=\widthf]{\qd S364/results/resw_B24_1to14}\vspace{\barspace}\tabularnewline
		\includegraphics[width=\widthf]{\qd S364/results/resw_B24_1to14_intBars}\tabularnewline
		\includegraphics[width=\widthf]{\qd S364/results/resw_B24_15to28}\vspace{\barspace}\tabularnewline
		\includegraphics[width=\widthf]{\qd S364/results/resw_B24_15to28_intBars}\tabularnewline
		\includegraphics[width=\widthf]{\qd S364/results/resw_B24_29to42}\vspace{\barspace}\tabularnewline
		\includegraphics[width=\widthf]{\qd S364/results/resw_B24_29to42_intBars}\tabularnewline
		\includegraphics[width=\widthf]{\qd S364/results/resw_B24_43to56}\vspace{\barspace}\tabularnewline
		\includegraphics[width=\widthf]{\qd S364/results/resw_B24_43to56_intBars}\tabularnewline			
	\end{tabular}}
	}
	\caption{Real data - patch \texttt{B24} from \autoref{fig:Patches}. (a) 14 randomly selected frames (out of $10^{3}$) of the tubulin structure stained with QDs. (b) Estimated sources $\bm{w}_{k}$ sorted according to their $l^{2}$ norm (shown all $K=56$ sources). Bars below each figure show the maximum of the $\bm{w}_{k}$.}
	\label{fig:Real-data-patch-B24}
	
\end{figure}

\begin{figure}[!htb]
	\newcommand{\wf}{.9\textwidth}
	\centering
	\includegraphics[width=\wf]{\qd S364/results/all_mean_zoh}
	\caption{Wide - filed data.}
\end{figure}

\begin{figure}[!htb]
	\newcommand{\wf}{.9\textwidth}
	\centering
	\includegraphics[width=\wf]{\qd S364/results/all_gauss3_int}
	\caption{QD - labelled tubulin. Gaussian filtered.}
\end{figure}

\begin{figure}[!htb]
	\newcommand{\wf}{.9\textwidth}
	\centering
	\includegraphics[width=\wf]{\qd S364/results/all_powFromLabel}
	\caption{QD - labelled tubulin. PSF squeezed.}
\end{figure}

\begin{figure}[!htb]
	\newcommand{\wf}{.9\textwidth}
	\centering
	\includegraphics[width=\wf]{\qd S364/results/all_powFromLabel}
	\caption{QD - labelled tubulin. PSF squeezed.}
\end{figure}

\begin{figure}[!htb]
	\newcommand{\wf}{.9\textwidth}
	\centering
	\includegraphics[width=\wf]{\qd S364/results/meanResIm_power05_scalebar500nm}
	\caption{QD - labelled tubulin. PSF squeezed. Mean of 10 evaluations. Scale bar $500 \unit{nm}$.}
\end{figure}



%We trained the classifier on $10^{3}$ labelled $\bm{w}_{k}$, computed by NMF from a real dataset (\autoref{fig:Patches}). Confusion matrix of the ten-fold cross validation is shown in \autoref{tab:Confusion-matrix}. From all $\bm{w}_{k}$s classified as good sources (class 1) 89\% were correct, while the rest 11\% being spread into classes for two sources (6\%), half missing source (3\%), noise (2\%) and multiple sources (1\%).
%
%\begin{table}[!h]
%	\subfloat[Counts]{
%	\begin{tabular}{|c||c|c|c|c|c|c|c|}
%		\hline 
%		\textbf{Class} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \tabularnewline
%		\hline
%		\hline 
%		\textbf{0} & \textcolor{red}{130} & 4 & 6 & 1 & 18 & 26 & \tabularnewline
%		\textbf{1} & 6 & \textcolor{red}{335} & 21 & 0 & 3 & 13 & \tabularnewline
%		\textbf{2} & 15 & 43 & \textcolor{red}{89} & 0 & 9 & 16 & \tabularnewline
%		\textbf{3} & 3 & 0 & 6 & \textcolor{red}{3} & 9 & 2 & \tabularnewline
%		\textbf{4} & 29 & 7 & 23 & 1 & \textcolor{red}{32} & 1 & \tabularnewline
%		\textbf{5} & 12 & 12 & 8 & 0 & 0 & \textcolor{red}{187} & \tabularnewline
%		\hline
%	\end{tabular}}
%	\hspace{.2cm}
%	\subfloat[Percentage (sum over rows gives 100\%).]{
%	\begin{tabular}{|c||c|c|c|c|c|c|c|}
%		\hline 
%		\textbf{Class} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \tabularnewline
%		\hline
%		\hline 
%		\textbf{0} & \textcolor{red}{70} & 2 & 3 & 1 & 10 & 14 & \tabularnewline
%		\textbf{1} & 2 & \textcolor{red}{89} & 6 & 0 & 1 & 3 & \tabularnewline
%		\textbf{2} & 9 & 25 & \textcolor{red}{52} & 0 & 5 & 9 & \tabularnewline
%		\textbf{3} & 13 & 0 & 26 & \textcolor{red}{13} & 39 & 9 & \tabularnewline
%		\textbf{4} & 31 & 8 & 25 & 1 & \textcolor{red}{34} & 1 & \tabularnewline
%		\textbf{5} & 5 & 5 & 4 & 0 & 0 & \textcolor{red}{85} & \tabularnewline
%		\hline 
%	\end{tabular}}
%	\caption{Confusion matrix for 10-fold cross validation. Correctly classified $\bm{w}_{k}$ are on diagonal highlighted in red.} \label{tab:Confusion-matrix}
%\end{table}

\begin{table}[!h]	
	\subfloat[Real data]{
	\begin{tabular}{|c|c|c|}
		\hline 
		Parameter & Note  & Value\tabularnewline
		\hline
		\hline 
		$\lambda_{ex}$ & excitation light & 405 nm\tabularnewline
		\hline 
		$\lambda_{em}$ & emission light & 625 nm\tabularnewline
		\hline 
		$t_{exp}$ & exposure time  & 50 ms\tabularnewline
		\hline 
		NA & numerical aperture & 1.4\tabularnewline
		\hline 
		RI & refraction index & 1.52\tabularnewline
		\hline 
		pixel-size & size of a pixel in image plane & 79 nm\tabularnewline
		\hline 
		QD & quantum dots  & QD625\tabularnewline
		\hline 
		$n_{t}$ & number of frames  & $10^{3}$\tabularnewline
		\hline
	\end{tabular}}
	\caption{Parameters of the experiment.}\label{tab:Parameters of the (a) simulations (b) real data}
\end{table}

%==========================================
%==========================================

\clearpage
\section{Discussion\label{sec:Discussion}}
Discussion of the results. Comparison with other methods. 
%==========================================
%==========================================

\clearpage
\section{Conclusion\label{sec:Conclusion}}
Conclusion of the Chapter 1 + future work?.