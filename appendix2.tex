%!TEX root = thesis.tex
\chapter{Resolution limit for blinking QDs\label{app:Resolution limit for the blinking QDs}}

%==========================================
%==========================================

This is derivation of the fisher information for Poisson distributed
variable $X$ with mean $\lambda$.

\begin{equation}
X\sim\Po(n,\lambda)=p(n|\theta)=\frac{\lambda^{n}e^{-\lambda}}{n!}
\end{equation}



\section{Likelihood}

Likelihood of the Poisson distributed variable with detection $n_k$ in K pixels: 
%
\begin{equation}
	l(\theta)=\prod_{k=1}^Kl_k=\prod_{k=1}^K\frac{\lambda_k^{n_k}e^{-\lambda_k}}{n_k!}\label{eq:app-Likelihood of Poisson}
\end{equation}
%
where $l_k(\theta)=p(n_k|\theta)$ to emphasise the dependency on the parameter $\theta$.

Log-Likelihood:
\begin{equation}
	\mathcal{L}=\sum_k\left(n_k\log\lambda_k-\lambda_k-\log n_k!\right)
\end{equation}

%==========================================
%==========================================

\section{Fisher Information}

Fisher information:
\begin{equation}
	I(\theta)=-\E\left[\frac{\partial^2\mathcal{L}}{\partial\theta^2}\right]=\E\left[\left(\frac{\partial\mathcal{L}}{\partial\theta}\right)^2\right]=\E\left[\left(\sum_k\frac{\partial\log(l_k)}{\partial\theta}\right)^2\right]=\E\left[\left(\sum_k\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}\right)^2\right]\label{eq:app-Fisher Info Definition}
\end{equation}


\begin{alignat*}{1}
	I(\theta) 
	& =\E\left[\left(\sum_k\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}\right)\left(\sum_{m}\frac{1}{l_{m}}\frac{\partial l_{m}}{\partial\theta}\right)\right]\\
	& =\E\left[\sum_k\frac{1}{l_k^2}\left(\frac{\partial l_k}{\partial\theta}\right)^2\right]+\E\left[\sum_k\sum_{m\neq k}\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}\frac{1}{l_{m}}\frac{\partial l_{m}}{\partial\theta}\right]
\end{alignat*}
%
as $n_k$ are iid then the second term can be expressed as 
%
\begin{align*}
	\E\left[\sum_k\sum_{m\neq k}\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}\frac{1}{l_{m}}\frac{\partial l_{m}}{\partial\theta}\right] 
	& =\sum_k\sum_{m\neq k}\E_k\left[\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}\right]\E_{m}\left[\frac{1}{l_{m}}\frac{\partial l_{m}}{\partial\theta}\right]
\end{align*}
%
where
%
\begin{equation}
	\E_k\left[f(n_k)\right]=\sum_{n_k\geq0}p(n_k|\theta)f(n_k)
\end{equation}
%
But 
\begin{equation}
	\E_k\left[\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}\right]=\sum_{n_k}l_k\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}=\sum_{n_k}\frac{\partial l_k}{\partial\theta}=\frac{\partial\sum_{n_k}l_k}{\partial\theta}=0
\end{equation}

as $\sum_{n_k}l_k=\sum_{n_k}p(n_k|\theta)=1$. The Fisher Information can then be expressed 
%
\begin{align*}
	I(\theta) & =\E\left[\sum_k\frac{1}{l_k^2}\left(\frac{\partial l_k}{\partial\theta}\right)^2\right]\\
	& =\sum_{k=1}^k\sum_{n_k\geq0}l_k\frac{1}{l_k^2}\left(\frac{\partial l_k}{\partial\theta}\right)^2\\
	& =\sum_{k=1}^k\sum_{n_k\geq0}\frac{1}{l_k}\left(\frac{\partial l_k}{\partial\theta}\right)^2
\end{align*}

Derivatives of likelihood \autoref{eq:app-Likelihood of Poisson}: 
%
\begin{equation}
	\frac{\partial l_k}{\partial\theta}=\frac{l_k(n_k-\lambda_k)}{\lambda_k}\frac{\partial\lambda_k}{\partial\theta}
\end{equation}

And we get:
%
\begin{align*}
	I(\theta) 
	& =\sum_{k=1}^k\sum_{n_k\geq0}\frac{l_k(n_k-\lambda_k)^2}{\lambda_k^2}\left(\frac{\partial\lambda_k}{\partial\theta}\right)^2\\
	& =\sum_{k=1}^k\frac{1}{\lambda_k^2}\left(\frac{\partial\lambda_k}{\partial\theta}\right)^2\E_k\left[(n_k-\lambda_k)^2\right]
\end{align*}
%
for Poisson $\var(n)=\mathrm{mean}(n)=\lambda$ gives 
%
\begin{equation}
	\E_k\left[(n_k-\lambda_k)^2\right]=\var(n_k)=\lambda_k
\end{equation}
%
and 
%
\begin{equation}
	I(\theta)=\sum_{k=1}^K\frac{1}{\lambda_k}\left(\frac{\partial\lambda_k}{\partial\theta}\right)^2
	\label{eq:app-Fisher Info for Poisson}   	  
\end{equation}

This is the pixelised version (detection of the photons in K detectors - CCD camera and $\lambda_k=\int_{\Gamma_k}\lambda(x)dx$ where $\Gamma_k$ is an area of the pixels of the detector). 

Non pixelised version \cite{Ram2006}
%
\begin{equation}
	I(\theta)=\int\frac{1}{\lambda(x)}\left(\frac{\partial\lambda(x)}{\partial\theta}\right)^2dx
\end{equation}

%==========================================
%==========================================
\clearpage{}
\section{Two sources separated by a distance $d$\label{sub:Two-sources-separated}}

These are comments on Fisher Information estimation as described in \cite{Ram2006}.

For two sources separated by a distance $d$ we have a mean value of the intensity:
%
\begin{equation}
	\lambda=\Lambda_1f_1+\Lambda_2f_2
\end{equation}
%
where $f_i$ and $\Lambda_i$ is the response function and intensity, respectively, of the source $i$. For translationally invariant PSF and in-focus sources: $f_1=q(x-\frac{d}{2})$ and $f_2=q(x+\frac{d}{2})$
%
\begin{equation}
	\lambda(d)=\Lambda_1q(x-\frac{d}{2})+\Lambda_2q(x+\frac{d}{2})
\end{equation}
%
where $q$ is the PSF of the sources. For pixelised version (integral over pixel area $\Gamma_k$) with homogeneous bacground $b$ in each pixel the intensity can be expressed as:
%
\begin{equation}
	\lambda_k(d)=\Lambda_1\int_{\Gamma_k}q(x-\frac{d}{2})dx+\Lambda_2\int_{\Gamma_k}q(x+\frac{d}{2})dx+b
\end{equation}
%
so we get (as described in\cite{Ram2006})
%
\begin{alignat}{1}
	I(d)
	&=\frac{1}{4}\sum_{k=1}^K\frac{\left(\Lambda_1\int_{\Gamma_k}\partial_{x}q(x-\frac{d}{2})dx-\Lambda_2\int_{\Gamma_k}\partial_{x}q(x+\frac{d}{2})dx\right)^2}{\Lambda_1\int_{\Gamma_k}q(x-\frac{d}{2})dx+\Lambda_2\int_{\Gamma_k}q(x+\frac{d}{2})dx+b}\nonumber\\
	&=\frac{1}{4}\sum_{k=1}^N\frac{\left[\Lambda_1q_k'(-\frac{d}{2})-\Lambda_2q_k'(\frac{d}{2})\right]^2}{\Lambda_1q_k(-\frac{d}{2})+\Lambda_2q_k(\frac{d}{2})+b},
	\label{eq:app-Fisher Info Pixelised - Ram}	
\end{alignat}
%
where we set $q_k(z)=\int_{\Gamma_k}q(x-z)dx$ as the pixelised version of a point spread function translated by $z$ with $\Gamma_k$ being an area of the $k$th pixel, and $q'_k(z)=\int_{\Gamma_k}\frac{\partial q(x-z)}{\partial x}dx$ as the corresponding pixelised derivative.


\subparagraph*{Limit $d=0$:}
If $\Lambda_1=\Lambda_2$ then $I(d=0)=0$ which means $\var(d=0)\rightarrow\infty$. However, this does not hold for $\Lambda_1\neq\Lambda_2$ and the variance remains finite.

\subparagraph*{Limit $d\rightarrow\infty$:}
When sources are far apart then the mixing term in nominator in \autoref{eq:app-Fisher Info Pixelised - Ram} $\Lambda_1\Lambda_2\partial_{x}q(x-\frac{d}{2})\partial q(d+\frac{d}{2})=0$ as the $\partial_{x}q(x-\frac{d}{2})$ and $\partial_{x}q(x+\frac{d}{2})$ do not have any overlap. \Autoref{eq:app-Fisher Info Pixelised - Ram} then decomposes into two individual terms (sum of Fisher Information for localisation of individual sources.) 

\begin{alignat*}{1}
	I(d) 
	& =\frac{1}{4}\sum_{k=1}^K\left[\frac{\left(\Lambda_1q'_k(x-\frac{d}{2})\right)^2}{\Lambda_1q_k(x-\frac{d}{2})+b}+\frac{\left(\Lambda_2 q'_k(x+\frac{d}{2})\right)^2}{\Lambda_2q_k(x+\frac{d}{2})+b}\right]\\
 	& =\frac{1}{4}\sum_{k=1}^K\frac{\left(q'_k(x)\right)^2}{q_k(x)+b}\left[\Lambda_1+\Lambda_2\right]
\end{alignat*}

\subparagraph*{Limit $\Lambda_i=0$:}
If $\Lambda_1=0$ or $\Lambda_2=0$ $I(d)\neq0$. So the variance is finite even if one of the sources is not present.

%==========================================
%==========================================

\section{An alternative way to derive Fisher information for two sources separated by $d$:}
\label{sec:Appendix FI alternative}
This is a suggestion how to fix the problems with limits for Fisher Information derived above. This gives infinite variance when one of the sources is no present. Also fix weird behaviour of the $I(d)$ for $d=0$. 

We consider two sources located at $c_1$ and $c_2$, respectively. The distance between the sources is therefore $d=c_1-c_2$. This is a linear combination $\bm{a}^T\cdot\bm{c}$ of the variable $\bm{c}=(c_1,c_2)^T$ where $\bm{a}=(1,-1)^T$. The variance of $d$ is given by 
%
\begin{equation}
	\var(d)=\var(\bm{a}^T\cdot\bm{c})=\bm{a}^T\cdot\bm{Q}\cdot\bm{a}=Q_{11}+Q_{22}-2Q_{12}
\end{equation}
%
where $\bm{Q}$ is a covariance matrix $\bm{Q}=\bm{I}^{-1}(\theta)$ and $\bm{I}(\theta)$ is the Fisher information matrix (symmetric
$I_{12}=I_{21}$)
%
\begin{equation}
	\bm{I}(\theta)=\left(
	\begin{array}{cc}
		I_{11} & I_{12}\\
		I_{12} & I_{22}
	\end{array}\right)
\end{equation}
%
given by generalisation of \autoref{eq:app-Fisher Info for Poisson}
%
\begin{equation}
	I_{ij}(\theta)=\sum_{k=1}^K\frac{1}{\lambda_k}\frac{\partial\lambda_k}{\partial\theta_i}\frac{\partial\lambda_k}{\partial\theta_j}
	\label{eq:app-Fisher Info gerneral lambda}
\end{equation}
%
The covariance matrix $\bm{Q}$ is then 
%
\begin{equation}
	\bm{Q}=\bm{I}^{-1}(\theta)=\frac{1}{I_{11}I_{22}-I_{12}^2}\left(
	\begin{array}{cc}
		I_{22} & -I_{12}\\
		-I_{12} & I_{11}
	\end{array}\right)
\end{equation}
%
and the variance of $d=c_1-c_2$ 
%
\begin{equation}
	\var(d)=(1,-1)^T\cdot\bm{Q}\cdot(1,-1)=\frac{I_{11}+I_{22}+2I_{12}}{I_{11}I_{22}-I_{12}^2}=\frac{p}{r}
	\label{eq:app-variance alternative}
\end{equation}

The individual terms of the Fisher Information matrix 
%
\begin{equation}
	I_{11}=\sum_{k=1}^K\frac{1}{\lambda_k}\left(\frac{\partial\lambda_k}{\partial c_1}\right)^2=\sum_{k=1}^K\frac{\left(\Lambda_1q'_k(c_1)\right)^2}{\Lambda_1q_k(c_1)+\Lambda_2q_k(c_2)+b}
\end{equation}
%
where $q_k(c_i)$ is the pixelised version (pixel area $\Gamma_k$) of the PSF
%
\begin{alignat*}{1}
	q_k(c_i) & =\int_{\Gamma_k}q(x-c_i)dx\\
	q'_k(c_i) & =\int_{\Gamma_k}\frac{\partial q(x-c_i)}{\partial x}dx
\end{alignat*}

If we use 
%
\begin{equation}
	f_k(c_1,c_2)=\Lambda_1q_k(c_1)+\Lambda_2q_k(c_2)+b,
\end{equation}
%
\begin{equation}
	\begin{aligned}I_{ii} & =\Lambda_i^2\sum_{k=1}^K\frac{\left(q'_k(c_i)\right)^2}{f_k(c_1,c_2)}\\
		I_{ij} & =\Lambda_i\Lambda_j\sum_{k=1}^K\frac{q'_k(c_i)q'_k(c_j)}{f_k(c_1,c_2)}
	\end{aligned}
	\label{eq:app-Fisher Information alternative - Individual}
\end{equation}
 
Numerator $p=I_{11}+I_{22}+2I_{12}$ in \autoref{eq:app-variance alternative}
\begin{equation}
	p=\sum_{k=1}^K\frac{1}{f_k(c_1,c_2)}\left[\Lambda_1^2q'^2{}_k(c_1)+\Lambda_2^2q'{}_k^2(c_2)+2\Lambda_1\Lambda_2q'_k(c_1)q'_k(c_2)\right]
\end{equation}

The terms in the denominator $r=I_{11}I_{22}-I_{12}^2$ in \autoref{eq:app-variance alternative}
%
\begin{equation}
	\begin{alignedat}{1}
		I_{11}I_{22} & =\Lambda_1^2\Lambda_2^2\sum_{k,l}^K\frac{\left(q'_k(c_1)q'_l(c_2)\right)^2}{f_k(c_1,c_2)f_l(c_1,c_2)}\\
		I_{12}^2 & =\Lambda_1^2\Lambda_2^2\sum_{k,l}^K\frac{q'_k(c_1)q'_k(c_2)q'_l(c_1)q'_l(c_2)}{f_k(c_1,c_2)f_l(c_1,c_2)}
	\end{alignedat}
\end{equation}

\subparagraph*{Limit $c_1\rightarrow c_2,\,(d\rightarrow0)\Rightarrow q_k(c_1)\rightarrow q_k(c_2)$:}
%
\begin{equation}
	p=(\Lambda_1^2+\Lambda_2^2+2\Lambda_1\Lambda_2)\sum_{k=1}^K\frac{q'{}_k^2(c)}{f_k(c,c)}
\end{equation}
%
which can be further simplified by explicitly writing $f_k(c_1,c_2)$
%
\begin{equation}
	p=(\Lambda_1+\Lambda_2)\sum_{k=1}^K\frac{q'{}_k^2(c)}{q_k(c)+b/(\Lambda_1+\Lambda_2)}
\end{equation}
%
$q_k$ is strictly positive function, therefore the sum is not zero and $p$ is non-zero for any $\Lambda_1,\,\Lambda_2$. 

The two terms in the denominator in \autoref{eq:app-variance alternative} are identical for $c_1=c_2$
\begin{equation}
	I_{11}I_{22}=I_{12}^2
	%\frac{\Lambda_1^2\Lambda_2^2}{\left(\Lambda_1+\Lambda_2\right)^2}\sum_{k,l=1}^K\frac{\left(q'_k(c)\right)^2}{\left(q_k(c)+d/(\Lambda_1+\Lambda_2)\right)}\frac{\left(q'_l(c)\right)^2}{\left(q_l(c)+d/(\Lambda_1+\Lambda_2)\right)}
\end{equation}
%
and therefore 
%
\begin{eqnarray*}
	r=I_{11}I_{22}-I_{12}^2=\det\left[\bm{I}(\theta)\right] & \equiv & 0
\end{eqnarray*}
%
for any $\Lambda_i$. $\bm{I}(\theta)$ is therefore a singular matrix for $d=0$ and inversion $\bm{I}^{-1}(\theta)$ does not exist for $c_1=c_2$, but the limit $c_1\rightarrow c_2,\,(d\rightarrow0)$ gives $p\neq0,\, r\rightarrow0$ and $\var(d\rightarrow0)=\frac{p}{r}\rightarrow\infty$. 

\subparagraph*{Limit $d\rightarrow\infty$:}
The cross term $I_{ij}$ in \autoref{eq:app-Fisher Information alternative - Individual} vanishes ($I_{ij}=0,\: i\neq j$) because of the multiplication $q'_k(c_1)q'_k(c_2)$ which is zero for largely separated PSF with finite support (if the support of $q_k(c_1)$ and $q_k(c_2)$ do not have mutual overlap). Then from \autoref{eq:app-variance alternative} 
%
\begin{equation}
	\var(d)=\frac{1}{I_{11}}+\frac{1}{I_{22}}
\end{equation}
%
which is the sum of variances for estimation two separated sources.

\begin{equation}
	I_{ii}=\Lambda_i^2\sum_{k=1}^K\frac{q_k'^2(c_i)}{\Lambda_1q_k(c_1)+\Lambda_2q_k(c_2)+b}=\Lambda_i\sum_{k=1}^K\frac{q_k'^2(c_i)}{q_k(c_i)+b/\Lambda_i}
\end{equation}
%
if $q_k(c_i)$ (and $q_k'(c_i)$) have a finite support.

For non-pixelised version, negligible background $b/\Lambda\ll1$ and for Gaussian approximation of the PSF ($q(x-a)\propto\exp\left(-\frac{(x-a)^2}{2\sigma^2}\right)$) with
$\sigma=\frac{\sqrt{2}}{2\pi}\frac{\lambda}{NA}$ (\cite{Zhang2007}) we have $q'(x)=\frac{1}{\sigma^2}xq(x)$ and $q'^2/q=\frac{1}{\sigma^4}x^2q$ which gives $\int q'^2/qdx=\frac{1}{\sigma^4}\int qx^2=\frac{1}{\sigma^4}\sigma^2=\frac{1}{\sigma^2}$ and therefore $I_{ii}=\frac{\Lambda_i}{\sigma^2}$ 

\begin{alignat*}{1}
	\var(d\rightarrow\infty) & =\sigma^2\left(\frac{1}{\Lambda_1}+\frac{1}{\Lambda_2}\right)
\end{alignat*}

\subparagraph*{Limit $\Lambda_i=0,\ \Lambda_j\neq0$:}
$I_{ii}\equiv0$ and $I_{ij}\equiv0$ and so $\det(\bm{I}(\theta))\equiv0$, and matrix is singular. In the limit $\Lambda_i\rightarrow0$ the
variance \autoref{eq:app-variance alternative} $\var(d)\rightarrow\infty$. 


For equally strong sources ($\Lambda_1= \Lambda_2=\Lambda$) the original FREM formula gives identical results as our proposed one. Due to symmetry we get equality of the diagonal terms $I_{11}=I_{22}$. From \autoref{eq:app-variance alternative}
\begin{alignat}{1}
	\var^{-1}(d)
	&=\frac{I_{11}^2-I_{12}^2}{2(I_{11}+I_{12})}\nonumber\\
	&=\frac{1}{2}\left(I_{11}-I_{12}\right)
\end{alignat}
%
From  \autoref{eq:app-Fisher Information alternative - Individual} we get
\begin{alignat}{2}
	\var^{-1}(d)
	&=\frac{\Lambda}{2}\sum_k\left[\frac{\left(q'_k(c_1)\right)^2-q'_k(c_1)q'_k(c_2)}{q_k(c_1)+q_k(c_2)+b/\Lambda} \right]\nonumber\\
	&=\frac{\Lambda}{4}\sum_k\left[\frac{\left(q'_k(c_1)-q'_k(c_2)\right)^2}{q_k(c_1)+q_k(c_2)+b/\Lambda} \right],
\end{alignat}
% 
where we used $\sum_k\left(q'_k(c_1)\right)^2=\sum_k\left(q'_k(c_2)\right)^2$. Comparison with \autoref{eq:app-Fisher Info Pixelised - Ram} shows the equality fo the boht fomula for the situation of equally strong sources $\Lambda_1=\Lambda_2$.

For sources with different intensity $\Lambda_1\neq\Lambda_2$ can be shown that the ratio between the original FREM and our proposed formula for a given distance $d$ is a function of $\Lambda_1/\Lambda_2$. 

%==========================================
%==========================================

\section{Time distribution of the intensities - known intensity states in the individual frames\label{sec:Appendix - blinking not integrated}}
We assume a likelihood dependent on parameter $\bm{\Lambda_t}$ (for example $\bm{\Lambda_t}=(\Lambda_1,\Lambda_2)_t$ - intensty of two sources at frame $t$, $T$ different time slices). If we know the configuration of $\bm{\Lambda^\alpha_t}$ in each frame, then we can write: 
$
\begin{equation}
	l_T(\theta,\Lambda)=\prod_{k=1}^K\prod_{t=1}^Tp(n_k|\theta,\Lambda_t)p(\Lambda_t)
\end{equation}

\begin{equation}
	\mathcal{L}_T(\theta,\Lambda)=\sum_{k=1}^K\sum_{t=1}^T\left[\log\left(l_k(\theta,\Lambda_t)\right)+\log\left(p(\Lambda_t)\right)\right]
\end{equation}
%
as $p(\Lambda)$ is not dependent on $\theta$ then
%
\begin{equation}
	\frac{\partial\mathcal{L}_T(\theta,\Lambda)}{\partial \theta}=\sum_{k=1}^K\sum_{t=1}^T\frac{\partial\log\left(l_k(\theta,\Lambda_t)\right)}{\partial \theta}=\sum_{t=1}^T\frac{\partial\mathcal{L}(\theta,\Lambda_t)}{\partial \theta}
\end{equation}
%
but in the expectation equation \autoref{eq:app-Fisher Info Definition} the time dependence appears as
%
\begin{alignat*}{1}
	I_T(\theta) & =-\E_T\left[\sum_{t=1}^T\frac{\partial^2\mathcal{L}(\theta,\Lambda_t)}{\partial \theta^2}\right]=\sum_{t=1}^T-\E_T\left[\frac{\partial^2\mathcal{L}(\theta,\Lambda_t)}{\partial \theta^2}\right]=\sum_{t=1}^T\E_T\left[\left(\frac{\partial\mathcal{L}(\theta,\Lambda_t)}{\partial \theta}\right)^2\right]\\
	& =\sum_{t=1}^T\int_{\Lambda_t}p(\Lambda_t)I(\theta)d\Lambda_t=\sum_t\int_{\Lambda_t}p(\Lambda_t)\sum_k\frac{1}{\lambda_k(\Lambda_t)}\left(\frac{\partial\lambda_k(\Lambda_t)}{\partial \theta}\right)^2d\Lambda_t.
\end{alignat*}
%
For discrete states of $\bm{\Lambda_t}$, for example  
%
\begin{alignat*}{1}
	\bm{\Lambda_t}
	&=\left\{\bm{\Lambda^{\alpha=1}},\,\bm{\Lambda^{\alpha=2}},\,\bm{\Lambda^{\alpha=3}},\,\bm{\Lambda^{\alpha=4}}\right\}\\
	&=\left\{[\Lambda_1,0],\,[\Lambda_2,0],\,[\Lambda_1,\Lambda_2],\,[0,0]\right\}
\end{alignat*}	
% 
we get
\begin{equation}
	I_T(\theta)=\sum_t\sum_{\alpha}p(\bm{\Lambda^{\alpha}_t})I^{stat}_t(\theta,\bm{\Lambda^{\alpha}_{t}})
\end{equation}
%
where the static Fisher Information for each frame $t$ (see \autoref{eq:app-Fisher Info for Poisson})
%
\begin{equation}
	I^{stat}_t=\sum_k\frac{1}{\lambda_k(\Lambda_t)}\left(\frac{\partial\lambda_k(\Lambda_t)}{\partial \theta}\right)^2
\end{equation}
%
 is computed for each configuration of $\bm{\Lambda_t^\alpha}$ and then summed with weights $p(\bm{\Lambda^\alpha})$. 

%==========================================
%==========================================

\section{Time distribution of the intensities - integrating out $\Lambda$}
\label{sub:Appendix Time-distribution-Integrating out}
If we do not know the configuration of the $\bm{\Lambda^\alpha_t}$ in each frame, then we have to rely only on the distribution $p(\Lambda)$ and integrate over it within the likelihood funcition:
%
\begin{equation}
	l_k(\theta)=\int_{\Lambda}l_k(\theta,\Lambda)d\Lambda=\int_{\Lambda}p(n_k|\theta,\Lambda)p(\Lambda)d\Lambda
	\label{eq:app-log lik - int out}
\end{equation}
%
for four state model of two sources: $\left\{ (\Lambda_1,0),(0,\Lambda_2),(\Lambda_1,\Lambda_2),(0,0)\right\}$ with uniform background intensity $b$ in each pixel: 
\begin{alignat}{3}
	\lambda_k^{\alpha=1}&=\Lambda_1q_k(x-c_1) & &+b,\nonumber\\ 
	\lambda_k^{\alpha=2}&=&\Lambda_2q_k(x-c_2) &+b,\nonumber\\ 
	\lambda_k^{\alpha=3}&=\Lambda_1q_k(x-c_1)&+\Lambda_2q_k(x-c_2)&+b,\nonumber\\ 
	\lambda_k^{\alpha=4}&=& &+b,
\end{alignat}
%
with uniform distribution over these states. From \autoref{eq:app-log lik - int out}:
%
\begin{equation}
	l_k(\theta)=\frac{1}{4}\sum_{\alpha=1}^4\Po(\lambda_k^\alpha)
\end{equation}
%
derivatives 
%
\begin{equation}
	\frac{\partial l_k}{\partial c_p}=\frac{1}{4}\sum_\alpha\frac{\partial\Po(\lambda_k^\alpha)}{\partial c_p}=\frac{1}{4}\sum_\alpha\left(\Po(\lambda_k^\alpha)\frac{(n_k-\lambda_k^\alpha)}{\lambda_k^\alpha}\frac{\partial\lambda_k^\alpha}{\partial c_p}\right)
\end{equation}
%
The Fisher information matrix diagonal entries:
%
\begin{alignat}{1}
	I_{pp}(\theta) & =\E\left[\left(\sum_{k=1}^N\frac{1}{l_k}\frac{\partial l_k}{\partial c_p}\right)^2\right]\nonumber \\
 	& =\E\left[\left\{ \sum_{k=1}^N\left(\frac{1}{\sum_{\alpha=1}^4\Po(\lambda_k^\alpha)}\frac{\partial\sum_{\alpha=1}^4\Po(\lambda_k^\alpha)}{\partial c_p}\right)\right\} \left\{ \sum_{l=1}^N\left(\frac{1}{\sum_{\alpha=1}^4\Po(\lambda_l^\alpha)}\frac{\partial\sum_{\alpha=1}^4\Po(\lambda_l^\alpha)}{\partial c_p}\right)\right\} \right]\nonumber \\
	& =\sum_{k=1}^N\E_k\left[\frac{\left(\sum_{\alpha=1}^4\frac{\partial\Po(\lambda_k^\alpha)}{\partial c_p}\right)^2}{\left(\sum_{\alpha=1}^4\Po(\lambda_k^\alpha)\right)^2}\right],
	\label{eq:app-Fisher Info Integrated Out - diagonal entries}
\end{alignat}
%
because the cross terms ($k,\, l$) in the sum (2nd row) are zeros: 
%
\begin{alignat*}{1}
	\E\left[\left(\frac{\sum_{\alpha=1}^4\frac{\partial\Po(\lambda_k^\alpha)}{\partial c_p}}{\sum_{\alpha=1}^4\Po(\lambda_k^\alpha)}\right)\left(\frac{\sum_{\alpha=1}^4\frac{\partial\Po(\lambda_l^\alpha)}{\partial c_p}}{\sum_{\alpha=1}^4\Po(\lambda_l^\alpha)}\right)\right] 
	& =\E_k\left[\frac{\sum_{\alpha=1}^4\frac{\partial\Po(\lambda_k^\alpha)}{\partial c_p}}{\sum_{\alpha=1}^4\Po(\lambda_k^\alpha)}\right]\E_l\left[\frac{\sum_{\alpha=1}^4\frac{\partial\Po(\lambda_l^\alpha)}{\partial c_p}}{\sum_{\alpha=1}^4\Po(\lambda_l^\alpha)}\right]\\
 	& =\sum_{\alpha=1}^4\frac{\partial}{\partial c_p}\left(\sum_{n_k\geq0}\Po(\lambda_k^\alpha)\right)\sum_{\alpha=1}^4\frac{\partial}{\partial c_p}\left(\sum_{n_k\geq0}\Po(\lambda_l^\alpha)\right)\\
 	& =0
\end{alignat*}

Expressing the derivatives and the expectation from \autoref{eq:app-Fisher Info Integrated Out - diagonal entries} we can write for the diagonal entries of the Fisher information matrix:
%
\begin{alignat*}{1}
	I_{pp}(\theta) & =\sum_{k=1}^N\E_k\left[\left\{ \frac{\sum_{\alpha=1}^4\left(\Po(n_k;\lambda_k^\alpha)\frac{(n_k-\lambda_k^\alpha)}{\lambda_k^\alpha}\frac{\partial\lambda_k^\alpha}{\partial c_p}\right)}{\sum_{\alpha=1}^4\Po(n_k;\lambda_k^\alpha)}\right\} ^2\right]\\
 	& =\frac{1}{4}\sum_{k=1}^N\sum_{n_k\geq0}\frac{\left\{ \sum_{\alpha=1}^4\left(\Po(n_k;\lambda_k^\alpha)\frac{(n_k-\lambda_k^\alpha)}{\lambda_k^\alpha}\frac{\partial\lambda_k^\alpha}{\partial c_p}\right)\right\} ^2}{\sum_{\alpha=1}^4\Po(n_k;\lambda_k^\alpha)}
\end{alignat*}

For the four states model we have $\lambda^{\alpha=3}(c_1,c_2)=\lambda^{\alpha=1}(c_1)+\lambda^{\alpha=2}(c_2)-b$ and so $\frac{\partial\lambda^{\alpha=3}}{\partial c_p}=\frac{\partial\lambda^{\alpha=p}}{\partial c_p}$ and $\frac{\partial\lambda^{\alpha=j}}{\partial c_p}=0,\, j\neq p$ for $p=\{1,2\},\: j=\{1,2,4\};$ so 
%
\begin{alignat*}{1}
	I_{pp}(\theta) & =\frac{1}{4}\sum_{k=1}^N\left(\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\sum_{n_k\geq0}\frac{\left\{\sum_{\alpha=\{p,3\}}\left(\Po(n_k;\lambda_k^\alpha)\frac{(n_k-\lambda_k^\alpha)}{\lambda_k^\alpha}\right)\right\}}{\sum_{\alpha=1}^4\Po(n_k;\lambda_k^\alpha)} ^2
\end{alignat*}


The Fisher information matrix off-diagonal entries:
%
\begin{alignat*}{2}
	I_{pq}(\theta) 
	& =\sum_{k=1}^N\E_k\left[\frac{\left(\sum_{\alpha=1}^4\frac{\partial\Po(\lambda_k^\alpha)}{\partial c_p}\right)\left(\sum_{\alpha=1}^4\frac{\partial\Po(\lambda_k^\alpha)}{\partial c_q}\right)}{\left(\sum_{\alpha=1}^4\Po(\lambda_k^\alpha)\right)^2}\right]\nonumber \\
 	& =\frac{1}{4}\sum_{k=1}^N\left(\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)\left(\frac{\partial\lambda_k^{\alpha=q}}{\partial c_q}\right)\times\\
	&\times\sum_{n_k\geq0}\frac{\left(\sum_{\alpha=\{p,3\}}\Po(n_k;\lambda_k^\alpha)\frac{(n_k-\lambda_k^\alpha)}{\lambda_k^\alpha}\right)\left(\sum_{\alpha=\{q,3\}}\Po(n_k;\lambda_k^\alpha)\frac{(n_k-\lambda_k^\alpha)}{\lambda_k^\alpha}\right)}{\sum_{\alpha=1}^4\Po(n_k;\lambda_k^\alpha)}
	\label{eq:app-Fisher Info Integrated Out-off diagonal entries}
\end{alignat*}

\subparagraph*{Limit $d\rightarrow0$:}

When $c^1=c^2$ then $\lambda^{\alpha=1}=\lambda^{\alpha=2}$ and $\frac{\partial\Po(\lambda^{\alpha=1})}{\partial c^1}=\frac{\partial\Po(\lambda^{\alpha=2})}{\partial c^2}$.
Then all entries in $I_{pq}$ are equal and the matrix is singular. For the limit $d\rightarrow0$ the determianat $\det(\bm{I})\rightarrow0$ and the variance $\var(d)\rightarrow\infty$.

\subparagraph*{Limit $d\rightarrow\infty$:}

Sources are far apart and $\lambda^{\alpha=1}$ and $\lambda^{\alpha=2}$ do not have a common overlap. For $k'$ where $\frac{\partial\lambda_{k'}^{\alpha=p}}{\partial c_p}\neq0,\,\frac{\partial\lambda_{k'}^{\alpha=q}}{\partial c_p}\equiv0$ and $\Po(n_{k'},\lambda_{k'}^{\alpha=3})=\Po(n_{k'},\lambda_{k'}^{\alpha=1})$. Also $\sum_\alpha\Po(\lambda_k^\alpha)=2\Po(\lambda_k^{\alpha=p})+2\Po(b)$ in the region where $\frac{\partial\lambda^{\alpha=p}}{\partial c_p}\neq 0$.

From \autoref{eq:app-Fisher Info Integrated Out - diagonal entries} the cross terms vanishes ($I_{pq}=0$ because $\frac{\partial\Po(\lambda^{\alpha=p})}{\partial c_p}\frac{\partial\Po(\lambda^{\alpha=q})}{\partial c_{q}}=0$). The diagonal elements 
%
\begin{alignat*}{1}
	I_{pp} & =\sum_{k=1}^N\E_k\left[\frac{\left(2\frac{\partial\Po(\lambda_k^{\alpha=p})}{\partial c_p}\right)^2}{\left(2\Po(\lambda_k^{\alpha=p})+2\Po(b)\right)^2}\right]\\
 	& =\sum_{k=1}^N\E_k\left[\frac{\left(\Po(\lambda_k^{\alpha=p})\frac{\left(n_k-\lambda_k^{\alpha=p}\right)}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2}{\left(\Po(\lambda_k^{\alpha=p})+\Po(b)\right)^2}\right]
\end{alignat*}
%
for $b=0$
%
\begin{alignat*}{1}
	I_{pp} 
	 &=\sum_{k=1}^N\left(\frac{1}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\E_k\left[\left(n_k-\lambda_k^{\alpha=p}\right)^2\right]\\
 	& =\sum_{k=1}^N\left(\frac{1}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\frac{1}{4}\sum_{n_k\geq0}\left(\sum_{i=1}^4\Po(\lambda_k^\alpha)\left(n_k-\lambda_k^{\alpha=p}\right)^2\right)\\
 	& =\sum_{k=1}^N\left(\frac{1}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\frac{1}{4}\sum_{n_k\geq0}\left(2\Po(\lambda_k^{\alpha=p})\left(n_k-\lambda_k^{\alpha=p}\right)^2\right)\\
 	& =\sum_{k=1}^N\left(\frac{1}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\frac{1}{2}\lambda_k^{\alpha=p}\\
 	& =\frac{1}{2}\sum_{k=1}^N\frac{1}{\lambda_k^{\alpha=p}}\left(\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2
\end{alignat*}
%
which is the \autoref{eq:app-Fisher Info for Poisson} (up to the factor $1/2$). The factor 1/2 comes from the fact that the source appears only in 50\% of the observations.

%When the background is present % <---- I think this is incorrect.... becaouose the background in in \lambda^p_k
%%
%\begin{alignat*}{1}
%	I_{pp} & =\sum_{k=1}^N\E_k\left[\frac{\left(2\frac{\partial\Po(\lambda_k^{\alpha=p})}{\partial c_p}\right)^2}{\left(2\Po(\lambda_k^{\alpha=p})+2\Po(b)\right)^2}\right]\\
% 	& =\sum_{k=1}^N\E_k\left[\frac{\left(\Po(\lambda_k^{\alpha=p})\frac{\left(n_k-\lambda_k^{\alpha=p}\right)}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2}{\left(\Po(\lambda_k^{\alpha=p})+\Po(b)\right)^2}\right]\\
% 	& =\sum_{k=1}^N\left(\frac{1}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\E_k\left[\left(\frac{\Po(\lambda_k^{\alpha=p})}{\Po(\lambda_k^{\alpha=p})+\Po(b)}\right)^2\left(n_k-\lambda_k^{\alpha=p}\right)^2\right]\\
% 	& =\sum_{k=1}^N\left(\frac{1}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\E_k\left[\left(1-\frac{\Po(b)}{\Po(\lambda_k^{\alpha=p})+\Po(b)}\right)^2\left(n_k-\lambda_k^{\alpha=p}\right)^2\right]\\
% 	& =\frac{1}{2}\sum_{k=1}^N\frac{1}{\lambda_k^{\alpha=p}}\left(\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2-S
%\end{alignat*}
%%
%where
%%
%\begin{equation}
%	S=\sum_{k=1}^N\left(\frac{1}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\E_k\left[\left\{ \frac{2\Po(b)}{\Po(\lambda_k^{\alpha=p})+\Po(b)}+\left(\frac{2\Po(b)}{\Po(\lambda_k^{\alpha=p})+\Po(b)}\right)^2\right\} \left(n_k-\lambda_k^{\alpha=p}\right)^2\right].
%\end{equation}
%%
%This term is positive and therefore reduces $I_{pp}$.