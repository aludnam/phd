%!TEX root = thesis.tex
\chapter{Resolution Limit for Blinking QDs\label{app:Appendix2}}

%==========================================
%==========================================

This is derivation of the fisher information for Poisson distributed
variable $X$ with mean $\lambda(\theta)$

\begin{equation}
X\sim\Po(n;\lambda)=\frac{\lambda^{n}e^{-\lambda}}{n!}.
\end{equation}



\section{Likelihood}

Likelihood of the Poisson distributed variable with detection $n_k$ in K pixels: 
%
\begin{equation}
	l(\theta)=\prod_{k=1}^Kl_k=\prod_{k=1}^K\frac{\lambda_k^{n_k}e^{-\lambda_k}}{n_k!},
	\label{eq:app-Likelihood of Poisson}
\end{equation}
%
where $l_k(\theta)=p(n_k|\theta)$ to emphasise the dependency on the parameter $\theta$.

Log-Likelihood:
\begin{equation}
	\LL=\sum_{k=1}^K\LL_k, 
\end{equation}
%
where
%
\begin{equation}
	\LL_k=n_k\log\lambda_k-\lambda_k-\log n_k!. 
	\label{eq:app-loglik}
\end{equation}


%==========================================
%==========================================

\section{Fisher Information}

Fisher information can be expressin in these eqvivalent forms:
\begin{equation}
	I(\theta)=-\E\left[\frac{\partial^2\LL}{\partial\theta^2}\right]=\E\left[\left(\frac{\partial\LL}{\partial\theta}\right)^2\right]=\E\left[\left(\sum_k\frac{\partial\log(l_k)}{\partial\theta}\right)^2\right]=\E\left[\left(\sum_k\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}\right)^2\right].
	\label{eq:app-Fisher Info Definition}
\end{equation}

Therefore
%\begin{alignat}{2}
%	I(\theta) 
%	& =\E\left[\left(\sum_k\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}\right)\left(\sum_m\frac{1}{l_m}\frac{\partial l_m}{\partial\theta}\right)\right]\\
%	& =\E\left[\sum_k\frac{1}{l_k^2}\left(\frac{\partial l_k}{\partial\theta}\right)^2\right]+\E\left[\sum_k\sum_{m\neq k}\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}\frac{1}{l_m}\frac{\partial l_m}{\partial\theta}\right]
%\end{alignat}
\begin{alignat}{2}
	I(\theta) 
	& =\E\left[\left(\sum_k\frac{\partial \LL_k}{\partial\theta}\right)\left(\sum_m\frac{\partial \LL_m}{\partial\theta}\right)\right]\\
	& =\E\left[\sum_k\left(\frac{\partial \LL_k}{\partial\theta}\right)^2\right]+\E\left[\sum_k\sum_{m\neq k}\frac{\partial \LL_k}{\partial\theta}\frac{\partial \LL_m}{\partial\theta}\right].
\end{alignat}
%
Because $n_k$ are iid, the second term can be expressed as 
%
\begin{equation}
	\E\left[\sum_k\sum_{m\neq k}\frac{\partial \LL_k}{\partial\theta}\frac{\partial \LL_m}{\partial\theta}\right] 
	=\sum_k\sum_{m\neq k}\E_k\left[\frac{\partial \LL_k}{\partial\theta}\right]\E_m\left[\frac{\partial \LL_m}{\partial\theta}\right],
\end{equation}
%
where
%
\begin{equation}
	\E_k\left[f(n_k)\right]=\sum_{n_k\geq0}p(n_k|\theta)f(n_k).
\end{equation}
%
But 
\begin{equation}
	\E_k\left[\frac{\partial \LL_k}{\partial\theta}\right]=\E_k\left[\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}\right]=\sum_{n_k}l_k\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}=\sum_{n_k}\frac{\partial l_k}{\partial\theta}=\frac{\partial}{\partial\theta}\sum_{n_k}p(n_k|\theta)=0,
\end{equation}
%
as $\sum_{n_k}p(n_k|\theta)=1$. 

The Fisher Information can then be written as 
%
\begin{equation}
	I(\theta) =\E\left[\sum_k\left(\frac{\partial \LL_k}{\partial\theta}\right)^2\right]
\end{equation}
%
and expressing the derivatives from \autoref{eq:app-loglik}
%
\begin{equation}
	\frac{\partial \LL_k}{\partial\theta}=\left(\frac{n_k-\lambda_k}{\lambda_k} \right)\frac{\partial\lambda_k}{\partial\theta},
\end{equation}
%
we get for Fisher information
%
\begin{alignat}{2}
	I(\theta)
	&=\E\left[\sum_k\frac{(n_k-\lambda_k)^2}{\lambda_k^2}\left(\frac{\partial\lambda_k}{\partial\theta}\right)^2\right]\\
	&=\sum_k\frac{1}{\lambda_k^2}\left(\frac{\partial\lambda_k}{\partial\theta}\right)^2\E\left[(n_k-\lambda_k)^2\right].
\end{alignat}
%
We recognize the variance $\var(n_k)=\E\left[(n_k-\lambda_k)^2\right]$. For Poisson variable 
%
\begin{equation}
	\var(n_k)=\mathrm{mean}(n_k)=\lambda_k,
\end{equation}
%
and therefore Fisher Information becomes
%
\begin{equation}
	I(\theta)=\sum_{k=1}^K\frac{1}{\lambda_k}\left(\frac{\partial \lambda_k}{\partial \theta}\right)^2.
	\label{eq:app-Fisher Info for Poisson}
\end{equation}

%==========================================
%==========================================
\clearpage
\section{Two sources separated by a distance $d$\label{sub:Two-sources-separated}}
This section comments on Fisher Information estimation as described by Ram et al. in \cite{Ram2006}.

For two sources separated by a distance $d$ we have a mean value of the intensity:
%
\begin{equation}
	\lambda(x)=\Lambda_1f_1(x)+\Lambda_2f_2(x),
\end{equation}
%
where $f_i$ and $\Lambda_i$ is the response function and intensity, respectively, of the source $i$. For translationally invariant PSF and in-focus sources: $f_1=q(x-\frac{d}{2})$ and $f_2=q(x+\frac{d}{2})$
%
\begin{equation}
	\lambda(d)=\Lambda_1q(x-\frac{d}{2})+\Lambda_2q(x+\frac{d}{2}),
	\label{eq:app-expected intensity d/2}
\end{equation}
%
where $q$ is the PSF of the sources. For pixelised version (integral over pixel area $\Gamma_k$) with homogeneous background $b$ in each pixel the intensity can be expressed as:
%
\begin{equation}
	\lambda_k(d)=\Lambda_1\int_{\Gamma_k}q(x-\frac{d}{2})dx+\Lambda_2\int_{\Gamma_k}q(x+\frac{d}{2})dx+b,
\end{equation}
%
so we get by plugging into \autoref{eq:app-Fisher Info for Poisson} expression for the Fisher Information:
%
\begin{alignat}{2}
	I(d)
	&=\frac{1}{4}\sum_{k=1}^K\frac{\left(\Lambda_1\int_{\Gamma_k}\partial_{x}q(x-\frac{d}{2})dx-\Lambda_2\int_{\Gamma_k}\partial_{x}q(x+\frac{d}{2})dx\right)^2}{\Lambda_1\int_{\Gamma_k}q(x-\frac{d}{2})dx+\Lambda_2\int_{\Gamma_k}q(x+\frac{d}{2})dx+b}\\
	&=\frac{1}{4}\sum_{k=1}^N\frac{\left[\Lambda_1q_k'(-\frac{d}{2})-\Lambda_2q_k'(\frac{d}{2})\right]^2}{\Lambda_1q_k(-\frac{d}{2})+\Lambda_2q_k(\frac{d}{2})+b},
	\label{eq:app-Fisher Info Pixelised - Ram}	
\end{alignat}
%
where we have set $q_k(z)=\int_{\Gamma_k}q(x-z)dx$ as the pixelised version of a point spread function translated by $z$ with $\Gamma_k$ being an area of the $k$th pixel, and $q'_k(z)=\int_{\Gamma_k}\frac{\partial q(x-z)}{\partial x}dx$ as the corresponding pixelised derivative.

FREM is defined as a lower bound standard deviation ($\sqrt{\var(d)}$) of the source separation $d$ estimation
\begin{equation}
 	\sqrt{\var(d)}\geq\textrm{FREM}=\sqrt{I^{-1}(\theta)}.
\end{equation}¥

\subparagraph*{Limit $d=0$:}\ \\
If $\Lambda_1=\Lambda_2$ then $I(d=0)=0$ which means $\var(d=0)\rightarrow\infty$. However, for unequal sources $\Lambda_1\neq\Lambda_2$ this does not hold and the variance remains finite.

\subparagraph*{Limit $d\rightarrow\infty$:}\ \\
When the sources are far apart the mixing term in nominator in \autoref{eq:app-Fisher Info Pixelised - Ram} $\Lambda_1\Lambda_2\partial_{x}q(x-\frac{d}{2})\partial q(d+\frac{d}{2})=0$ as the $\partial_{x}q(x-\frac{d}{2})$ and $\partial_{x}q(x+\frac{d}{2})$ do not have common overlap. \Autoref{eq:app-Fisher Info Pixelised - Ram} then decomposes into two individual terms
%
\begin{equation}
	I(d) 
	=\frac{1}{4}\sum_{k=1}^K\left[\frac{\left(\Lambda_1q'_k(x-\frac{d}{2})\right)^2}{\Lambda_1q_k(x-\frac{d}{2})+b}+\frac{\left(\Lambda_2 q'_k(x+\frac{d}{2})\right)^2}{\Lambda_2q_k(x+\frac{d}{2})+b}\right]
\end{equation}
%
This corresponds to a sum of Fisher Information for localisation of individual sources. 

\subparagraph*{Situation with missing source $\Lambda_i=0,\ \Lambda_j\neq0$:}\ \\
Even if one of the source is missing the Fisher information is strangely non-zero $I(d)\neq0$. The variance remains finite even if one of the sources is not present! This is a consequence of the assumption that the sources are located symmetrically at $\pm d/2$ with respect to the origin. Therefore only one source is needed to determine the distance $d/2$.

%==========================================
%==========================================

\section{An alternative way to derive Fisher information for two sources separated by $d$:}
\label{sec:Appendix FI alternative}
Below we show how to fix the problems with limits for Fisher Information derived above. This gives infinite variance when one of the sources is no present. Also fix strange behaviour of unequal sources for the limit $d\rightarrow0$. 

We consider two sources located at $c_1$ and $c_2$, respectively. The expectation of the intensity is therefore expressed as (cf. \autoref{eq:app-expected intensity d/2}):
%
\begin{equation}
	\lambda_k(\bm{c})=\Lambda_1q_k(x-c_1)+\Lambda_2q_k(x-c_2)+b.
	\label{eq:app-Expected intensity c_1 c_2}
\end{equation}
%
The distance between the sources is $d=c_1-c_2$. This is a linear combination $\bm{a}^T\cdot\bm{c}$ of the variable $\bm{c}=(c_1,c_2)^T$ where $\bm{a}=(1,-1)^T$. The variance of $d$ is given by 
%
\begin{equation}
	\var(d)=\var(\bm{a}^T\cdot\bm{c})=\bm{a}^T\cdot\bm{Q}\cdot\bm{a}=Q_{11}+Q_{22}-2Q_{12},
\end{equation}
%
where $\bm{Q}$ is a covariance matrix with lower bound expressed as an inverse of the the Fisher information matrix $\bm{Q}\geq\bm{I}^{-1}(\theta)$
%
\begin{equation}
	\bm{I}(\theta)=\left(
	\begin{array}{cc}
		I_{11} & I_{12}\\
		I_{21} & I_{22},
	\end{array}\right)
\end{equation}
%
given by generalisation of \autoref{eq:app-Fisher Info for Poisson}
%
\begin{equation}
	I_{ij}(\theta)=\sum_{k=1}^K\frac{1}{\lambda_k}\frac{\partial\lambda_k}{\partial c_i}\frac{\partial\lambda_k}{\partial c_j}.
	\label{eq:app-Fisher Info general lambda}
\end{equation}
%
Not that the Fisher information matrix is symmetrical $I_{12}=I_{21}$ due to exchangebilily of the derivatives.
 
The covariance matrix $\bm{Q}$ is therefore
%
\begin{equation}
	\bm{Q}\geq\bm{I}^{-1}(\theta)=\frac{1}{I_{11}I_{22}-I_{12}^2}\left(
	\begin{array}{cc}
		I_{22} & -I_{12}\\
		-I_{12} & I_{11}
	\end{array}\right)
\end{equation}
%
and the variance of $d=c_1-c_2$ estimator
%
\begin{alignat}{2}
	\var(d)
	&=(1,-1)^T\cdot\bm{Q}\cdot(1,-1)\\
	&\geq\frac{I_{11}+I_{22}+2I_{12}}{I_{11}I_{22}-I_{12}^2}=\frac{p}{r}.
	\label{eq:app-variance alternative}
\end{alignat}

The individual terms of the Fisher Information matrix by using \autoref{eq:app-Expected intensity c_1 c_2} in \autoref{eq:app-Fisher Info general lambda}
%
\begin{equation}
	I_{ij} =\Lambda_i\Lambda_j\sum_{k=1}^K\frac{q'_k(c_i)q'_k(c_j)}{f_k(c_1,c_2)},
	\label{eq:app-Fisher Information alternative - Individual}
\end{equation}
%
where $q_k(c_i)$ is the pixelised version (pixel area $\Gamma_k$) of the PSF
%
\begin{alignat}{2}
	q_k(c_i) & =\int_{\Gamma_k}q(x-c_i)dx\\
	q'_k(c_i) & =\int_{\Gamma_k}\frac{\partial q(x-c_i)}{\partial x}dx
\end{alignat}
%
and $f_k(c_1,c_2)=\Lambda_1q_k(c_1)+\Lambda_2q_k(c_2)+b$.
 
Then the numerator $p=I_{11}+I_{22}+2I_{12}$ in \autoref{eq:app-variance alternative} is given by
\begin{equation}
	p=\sum_{k=1}^K\frac{1}{f_k(c_1,c_2)}\left[\Lambda_1^2q'^2{}_k(c_1)+\Lambda_2^2q'{}_k^2(c_2)+2\Lambda_1\Lambda_2q'_k(c_1)q'_k(c_2)\right].
\end{equation}

The terms in the denominator $r=I_{11}I_{22}-I_{12}^2$ in \autoref{eq:app-variance alternative} are given by
%
\begin{alignat}{2}
	I_{11}I_{22} & =\Lambda_1^2\Lambda_2^2\sum_{k,l}^K\frac{\left(q'_k(c_1)q'_l(c_2)\right)^2}{f_k(c_1,c_2)f_l(c_1,c_2)}\\
	I_{12}^2 & =\Lambda_1^2\Lambda_2^2\sum_{k,l}^K\frac{q'_k(c_1)q'_k(c_2)q'_l(c_1)q'_l(c_2)}{f_k(c_1,c_2)f_l(c_1,c_2)}
\end{alignat}

We now consider the limits of the very close sources ($d\rightarrow 0$) and well separated emitters ($d\rightarrow\infty$) for FREM.

\subparagraph*{Close sources limit: $d\rightarrow0 \Rightarrow c_1\rightarrow c_2 \Rightarrow q_k(c_1)\rightarrow q_k(c_2)$:}\ \\
%
\begin{equation}
	p=(\Lambda_1^2+\Lambda_2^2+2\Lambda_1\Lambda_2)\sum_{k=1}^K\frac{q'{}_k^2(c)}{f_k(c,c)},
\end{equation}
%
which can be further simplified by explicitly substituing $f_k(c_1,c_2)$
%
\begin{equation}
	p=(\Lambda_1+\Lambda_2)\sum_{k=1}^K\frac{q'{}_k^2(c)}{q_k(c)+b/(\Lambda_1+\Lambda_2)}.
\end{equation}
%
$q_k$ and $(q'_k)^2$ are strictly positive functions, therefore the sum is not zero and $p$ is non-zero for any $\Lambda_1,\,\Lambda_2$. 

The two terms in the denominator in \autoref{eq:app-variance alternative} are identical for $c_1=c_2$
\begin{equation}
	I_{11}I_{22}=I_{12}^2
	%\frac{\Lambda_1^2\Lambda_2^2}{\left(\Lambda_1+\Lambda_2\right)^2}\sum_{k,l=1}^K\frac{\left(q'_k(c)\right)^2}{\left(q_k(c)+d/(\Lambda_1+\Lambda_2)\right)}\frac{\left(q'_l(c)\right)^2}{\left(q_l(c)+d/(\Lambda_1+\Lambda_2)\right)}
\end{equation}
%
and therefore 
%
\begin{equation}
	r=I_{11}I_{22}-I_{12}^2=\det\left[\bm{I}(\theta)\right] \equiv 0
\end{equation}
%
for any $\Lambda_i$. $\bm{I}(\theta)$ is therefore a singular matrix for $d=0$ and inverse $\bm{I}^{-1}(\theta)$ does not exist for $c_1=c_2$. However,  the limit $c_1\rightarrow c_2,\,(d\rightarrow0)$ gives $p\neq0,\, r\rightarrow0$ and FREM$=\frac{p}{r}\rightarrow\infty$. FREM therefore diverges for sources with any combination of source intenisties $\Lambda_1$ and $\Lambda_2$. This is in contratst to the original FREM formula \autoref{eq:app-Fisher Info Pixelised - Ram} wich gives diveriging FREM only for equally strong sources $\Lambda_1=\Lambda_2$. 

\subparagraph*{Well separated sources limit $d\rightarrow\infty$:}\ \\
The cross term $I_{ij}$ in \autoref{eq:app-Fisher Information alternative - Individual} vanishes ($I_{ij}=0,\: i\neq j$) because of the multiplication $q'_k(c_1)q'_k(c_2)$, which is very close to zero for well separated PSFs. This assumes that the PSF (and its first derivative) decreases to negligible values for distance far from the center of the PSF. Then from \autoref{eq:app-variance alternative} 
%
\begin{equation}
	\var(d)\geq\frac{1}{I_{11}}+\frac{1}{I_{22}},
\end{equation}
%
which is the sum of bounds on variances for localisatoin of two individual sources:
%
\begin{alignat}{2}
	I_{ii}
	&=\Lambda_i^2\sum_{k=1}^K\frac{q_k'^2(c_i)}{\Lambda_1q_k(c_1)+\Lambda_2q_k(c_2)+b}\\
	&=\Lambda_i\sum_{k=1}^K\frac{q_k'^2(c_i)}{q_k(c_i)+b/\Lambda_i}.
	\label{eq:app-I d to infty}
\end{alignat}
%
We used the fact that the PSFs $q(c_1)$ and $q(c_2)$ are well separated and decrase (with its first derivatives) towards zero in the regions far from their center. Therefore the $q(c_j)$ is negligible in the region where $q'(c_i)$ ($j\neq i$) have any significant values. The term $\Lambda_jq(c_j)$ in the denominator can be therefore neglected. 


If we use Gaussian approximation of the PSF (see Zhang et al. \cite{Zhang2007})
%
\begin{equation}
	q(x-c_i)=\frac{1}{Z}\exp\left(-\frac{(x-c_i)^2}{2\sigma^2}\right)
\end{equation}
% 
with derivatives with respect to $c_i$
%
\begin{alignat}{2}
 	q'(x-c_i) 
	&=\frac{x-c_i}{\sigma^2}\frac{1}{Z}\exp\left(-\frac{(x-c_i)^2}{2\sigma^2}\right)\\
	&=\frac{x-c_i}{\sigma^2}q(x-c_i),
\end{alignat}
%
then from \autoref{eq:app-I d to infty} for the situation with negligible background $b/\Lambda\ll1$
%
\begin{equation}
	I_{ii}=\frac{\Lambda_i}{\sigma^4}\sum_k\int_{\Gamma_k}(x-c_i)^2q(x-c_i)dx.
\end{equation}
%
Now using 
%
\begin{alignat}{2}
	\sum_k\int_{\Gamma_k}(x-c_i)^2q(x-c_i)dx
	&=\int_{\mathbb{R}^2}(x-c_i)^2q(x-c_i)dx\\
	&=\sigma^2,
\end{alignat}¥
% 
we obtain the terms of the Fisher information matrix
%
\begin{equation}
	I_{ii}=\frac{\Lambda_i}{\sigma^2}.
\end{equation}
%
These terms corresponds to the localisation of the individual sources. The ``localisatoin precision'' for one souces $s_i$ as used in conventional localisatoin techniques is then bounded by $\sigma/\sqrt{\Lambda_i}$, which corresponds to ``squeezing'' of the intitial `` localisation uncertainity'' $\sigma$ by the square root of the souce's intensity. 

The lower bound on the source separation estimator variance is given by
%
\begin{equation}
	\var(d\rightarrow\infty) \geq\sigma^2\left(\frac{1}{\Lambda_1}+\frac{1}{\Lambda_2}\right).
\end{equation}
%

\subparagraph*{One sources missing: $\Lambda_i=0,\ \Lambda_j\neq0$:}\ \\
$I_{ii}\equiv0$ and $I_{ij}\equiv0$ and so $\det(\bm{I}(\theta))\equiv0$, and matrix is singular. In the limit $\Lambda_i\rightarrow0$ the
variance \autoref{eq:app-variance alternative} $\var(d)\rightarrow\infty$. This is agian in contrast to the original FREM computed from \autoref{eq:app-Fisher Info Pixelised - Ram}, which gives finite FREM even for one source missing (see discussion of limits in \autoref{sub:Two-sources-separated}). 

%==========================================

\section{Comparison of the original FREM with our version}
For equally strong sources ($\Lambda_1= \Lambda_2=\Lambda$) the original FREM formula gives identical results as ours. In this situation \autoref{eq:app-Fisher Information alternative - Individual} gives equality of the diagonal terms $I_{11}=I_{22}$ for any $c_1$ and $c_2$. From \autoref{eq:app-variance alternative}
\begin{alignat}{2}
	\var(d)
	&\geq\frac{2(I_{11}+I_{12})}{I_{11}^2-I_{12}^2}\\
	&\geq\frac{2}{I_{11}-I_{12}}
	\label{eq:app-FREM equal brightness}
\end{alignat}
%
Using $q_k(c_1)=q_k(-d/2)$ and $q_k(c_2)=q_k(+d/2)$ we can rewrite the original FREM expression \autoref{eq:app-Fisher Info Pixelised - Ram} using \autoref{eq:app-Fisher Information alternative - Individual}
%
\begin{equation}
	\var^{ORIG}(d)\geq\frac{4}{I_{11}-2I_{12}+I_{22}},	
\end{equation}
%
which for $\Lambda_1=\Lambda_2$ ($I_{11}=I_{22}$) reduces to
%
\begin{equation}
	\var^{ORIG}(d)\geq\frac{2}{I_{11}-I_{12}}. 		
\end{equation}

Comparison with \autoref{eq:app-FREM equal brightness} shows the equality of the both formulas for the situation of equally strong sources $\Lambda_1=\Lambda_2$.

For sources with different intensity the expressions gives different resutls. If $\Lambda_2=\alpha\Lambda_1$, then can be shown, that for negligible values of $b/\Lambda_2$ the ratio between the original FREM and our proposed formula for $d\rightarrow\infty$ is $2\sqrt{2}/3$.

%==========================================
%==========================================

\section{Time distribution of the intensities - averaging\label{sec:Appendix - blinking not integrated}}
We assume a likelihood dependent on parameter $\bm{\Lambda}$ (for example, $\bm{\Lambda}=(\Lambda_1,\Lambda_2)$ - intensity of two sources in the recorede frame). If we \emph{knew the configuration} of $\bm{\Lambda}$ we would write the log-likelihood
%
\begin{equation}
	\mathcal{L}(\theta,\Lambda)=\sum_{k=1}^K\log\left(l_k(\theta,\bm{\Lambda})\right).
\end{equation}
%
Derivatives with respect to the parameter $\theta$:
%
\begin{alignat}{2}
	\frac{\partial\mathcal{L}(\theta,\bm{\Lambda})}{\partial \theta}
	&=\sum_{k=1}^K\frac{\partial\log\left(l_k(\theta,\bm{\Lambda})\right)}{\partial \theta}\\
	&=\frac{\partial\mathcal{L}(\theta,\bm{\Lambda})}{\partial \theta}. 
\end{alignat}

If we assume a probability distribution $p(\bm{\Lambda})$  of the intensity states $\bm{\Lambda}$ we can express the Fisher information (see \autoref{eq:app-Fisher Info Definition}):
%
\begin{equation}
	I(\theta) = \int_{\bm{\Lambda}}p(\bm{\Lambda})I_{\bm{\Lambda}}(\theta)d\bm{\Lambda},
\end{equation}
%
where $I_{\bm{\Lambda}}(\theta)$ is the Fisher information computed for a specific value of $\bm{\Lambda}$ (see \autoref{eq:app-Fisher Info Definition}).

For discrete states of $\bm{\Lambda}$, for example  
%
\begin{alignat}{4}
	\bm{\Lambda}
	&=\left\{\bm{\Lambda^{\alpha=1}},\,\bm{\Lambda^{\alpha=2}},\,\bm{\Lambda^{\alpha=3}},\,\bm{\Lambda^{\alpha=4}}\right\}\\
	&=\left\{[\Lambda_1,0],\,[\Lambda_2,0],\,[\Lambda_1,\Lambda_2],\,[0,0]\right\}
\end{alignat}	
% 
we get
\begin{equation}
	I(\theta)=\sum_{\alpha}p(\bm{\Lambda^\alpha})I_{\bm{\Lambda^\alpha}}(\theta),
\end{equation}
%
where the Fisher Information for every configuration of $\bm{\Lambda^\alpha}$ is averaged with weights $p(\bm{\Lambda^\alpha})$. 

%==========================================
%==========================================

\section{Time distribution of the intensities - integrating out $\Lambda$}
\label{sub:Appendix Time-distribution-Integrating out}
If we do not know the configuration of the $\bm{\Lambda^\alpha}$ in each frame, then we have to rely only on the distribution $p(\Lambda)$ and integrate over it within the likelihood function:
%
\begin{equation}
	l_k(\theta)=\int_{\Lambda}l_k(\theta,\Lambda)d\Lambda=\int_{\Lambda}p(n_k|\theta,\Lambda)p(\Lambda)d\Lambda
	\label{eq:app-log lik - int out}
\end{equation}
%
We assume four state model of two sources: $\left\{ (\Lambda_1,0),(0,\Lambda_2),(\Lambda_1,\Lambda_2),(0,0)\right\}$: 
%
\begin{alignat}{4}
	\lambda_k^{\alpha=1}&=\Lambda_1q_k(x-c_1) & &+b,\\ 
	\lambda_k^{\alpha=2}&=&\Lambda_2q_k(x-c_2) &+b,\\ 
	\lambda_k^{\alpha=3}&=\Lambda_1q_k(x-c_1)&+\Lambda_2q_k(x-c_2)&+b,\\ 
	\lambda_k^{\alpha=4}&=& &+b,
\end{alignat}
%
with uniform distribution over these states. We used uniform background intensity $b$ in each pixel of each frame. 

Assuming $p(\bm{\Lambda^\alpha})=1/4$ for $\alpha=1,..4$, then from \autoref{eq:app-log lik - int out}:
%
\begin{equation}
	l_k(\theta)=\frac{1}{4}\sum_{\alpha=1}^4\Po(\lambda_k^\alpha),
\end{equation}
%
with derivatives 
%
\begin{equation}
	\frac{\partial l_k}{\partial c_p}=\frac{1}{4}\sum_\alpha\frac{\partial\Po(\lambda_k^\alpha)}{\partial c_p}=\frac{1}{4}\sum_\alpha\left(\Po(\lambda_k^\alpha)\frac{(n_k-\lambda_k^\alpha)}{\lambda_k^\alpha}\frac{\partial\lambda_k^\alpha}{\partial c_p}\right).
\end{equation}
%
The diagonal entries of the Fisher information matrix are give by:
%
\begin{alignat}{2}
	I_{pp}(\bm{c}) & =\E\left[\left(\sum_{k=1}^N\frac{1}{l_k}\frac{\partial l_k}{\partial c_p}\right)^2\right] \\
 	& =\E\left[\left\{ \sum_{k=1}^N\left(\frac{1}{\sum_{\alpha=1}^4\Po(\lambda_k^\alpha)}\frac{\partial\sum_{\alpha=1}^4\Po(\lambda_k^\alpha)}{\partial c_p}\right)\right\} \left\{ \sum_{l=1}^N\left(\frac{1}{\sum_{\alpha=1}^4\Po(\lambda_l^\alpha)}\frac{\partial\sum_{\alpha=1}^4\Po(\lambda_l^\alpha)}{\partial c_p}\right)\right\} \right] \\
	& =\sum_{k=1}^N\E_k\left[\frac{\left(\sum_{\alpha=1}^4\frac{\partial\Po(\lambda_k^\alpha)}{\partial c_p}\right)^2}{\left(\sum_{\alpha=1}^4\Po(\lambda_k^\alpha)\right)^2}\right],
	\label{eq:app-Fisher Info Integrated Out - diagonal entries}
\end{alignat}
%
because the cross terms ($k,\, l$) in the sum (2nd row) are zeros: 
%
\begin{alignat}{2}
	\E\left[\left(\frac{\sum_{\alpha=1}^4\frac{\partial\Po(\lambda_k^\alpha)}{\partial c_p}}{\sum_{\alpha=1}^4\Po(\lambda_k^\alpha)}\right)\left(\frac{\sum_{\alpha=1}^4\frac{\partial\Po(\lambda_l^\alpha)}{\partial c_p}}{\sum_{\alpha=1}^4\Po(\lambda_l^\alpha)}\right)\right] 
	& =\E_k\left[\frac{\sum_{\alpha=1}^4\frac{\partial\Po(\lambda_k^\alpha)}{\partial c_p}}{\sum_{\alpha=1}^4\Po(\lambda_k^\alpha)}\right]\E_l\left[\frac{\sum_{\alpha=1}^4\frac{\partial\Po(\lambda_l^\alpha)}{\partial c_p}}{\sum_{\alpha=1}^4\Po(\lambda_l^\alpha)}\right]\\
 	& =\sum_{\alpha=1}^4\frac{\partial}{\partial c_p}\left(\sum_{n_k\geq0}\Po(\lambda_k^\alpha)\right)\sum_{\alpha=1}^4\frac{\partial}{\partial c_p}\left(\sum_{n_k\geq0}\Po(\lambda_l^\alpha)\right)\\
 	& =0
\end{alignat}

Expressing the derivatives and the expectation from \autoref{eq:app-Fisher Info Integrated Out - diagonal entries} we can write for the diagonal entries of the Fisher information matrix:
%
\begin{alignat}{2}
	I_{pp}(\bm{c}) 
	& =\sum_{k=1}^N\E_k\left[\left\{ \frac{\sum_{\alpha=1}^4\left(\Po(n_k;\lambda_k^\alpha)\frac{(n_k-\lambda_k^\alpha)}{\lambda_k^\alpha}\frac{\partial\lambda_k^\alpha}{\partial c_p}\right)}{\sum_{\alpha=1}^4\Po(n_k;\lambda_k^\alpha)}\right\} ^2\right]\\
 	& =\frac{1}{4}\sum_{k=1}^N\sum_{n_k\geq0}\frac{\left\{ \sum_{\alpha=1}^4\left(\Po(n_k;\lambda_k^\alpha)\frac{(n_k-\lambda_k^\alpha)}{\lambda_k^\alpha}\frac{\partial\lambda_k^\alpha}{\partial c_p}\right)\right\} ^2}{\sum_{\alpha=1}^4\Po(n_k;\lambda_k^\alpha)}
\end{alignat}

For the four states model we have $\lambda^{\alpha=3}(c_1,c_2)=\lambda^{\alpha=1}(c_1)+\lambda^{\alpha=2}(c_2)-b$ and so $\frac{\partial\lambda^{\alpha=3}}{\partial c_p}=\frac{\partial\lambda^{\alpha=p}}{\partial c_p}$ and $\frac{\partial\lambda^{\alpha=j}}{\partial c_p}=0,\, j\neq p$ for $p=\{1,2\},\: j=\{1,2,4\};$ so 
%
\begin{equation}
	I_{pp}(\bm{c}) 
	=\frac{1}{4}\sum_{k=1}^N\left(\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\sum_{n_k\geq0}\frac{\left\{\sum_{\alpha=\{p,3\}}\left(\Po(n_k;\lambda_k^\alpha)\frac{(n_k-\lambda_k^\alpha)}{\lambda_k^\alpha}\right)\right\}}{\sum_{\alpha=1}^4\Po(n_k;\lambda_k^\alpha)} ^2
\end{equation}


The off-diagonal entries of the  Fisher information matrix are given by:
%
\begin{alignat}{2}
	I_{pq}(\bm{c}) 
	& =\sum_{k=1}^N\E_k\left[\frac{\left(\sum_{\alpha=1}^4\frac{\partial\Po(\lambda_k^\alpha)}{\partial c_p}\right)\left(\sum_{\alpha=1}^4\frac{\partial\Po(\lambda_k^\alpha)}{\partial c_q}\right)}{\left(\sum_{\alpha=1}^4\Po(\lambda_k^\alpha)\right)^2}\right] \\
 	& =\frac{1}{4}\sum_{k=1}^N\left(\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)\left(\frac{\partial\lambda_k^{\alpha=q}}{\partial c_q}\right)\\
	&\times\sum_{n_k\geq0}\frac{\left(\sum_{\alpha=\{p,3\}}\Po(n_k;\lambda_k^\alpha)\frac{(n_k-\lambda_k^\alpha)}{\lambda_k^\alpha}\right)\left(\sum_{\alpha=\{q,3\}}\Po(n_k;\lambda_k^\alpha)\frac{(n_k-\lambda_k^\alpha)}{\lambda_k^\alpha}\right)}{\sum_{\alpha=1}^4\Po(n_k;\lambda_k^\alpha)}
	\label{eq:app-Fisher Info Integrated Out-off diagonal entries}
\end{alignat}

\subparagraph*{Limit $d\rightarrow0$:}\ \\
%
When $c^1=c^2$ then $\lambda^{\alpha=1}=\lambda^{\alpha=2}$ and $\frac{\partial\Po(\lambda^{\alpha=1})}{\partial c^1}=\frac{\partial\Po(\lambda^{\alpha=2})}{\partial c^2}$.
Then all entries in $I_{pq}$ are equal and the matrix is singular. For the limit $d\rightarrow0$ the determinant $\det(\bm{I})\rightarrow0$ and the variance $\var(d)\rightarrow\infty$.

\subparagraph*{Limit $d\rightarrow\infty$:}\ \\
%
Sources are far apart and $\lambda^{\alpha=1}$ and $\lambda^{\alpha=2}$ do not have a common overlap. For $k'$ where $\frac{\partial\lambda_{k'}^{\alpha=p}}{\partial c_p}\neq0,\,\frac{\partial\lambda_{k'}^{\alpha=q}}{\partial c_p}\equiv0$ and $\Po(n_{k'},\lambda_{k'}^{\alpha=3})=\Po(n_{k'},\lambda_{k'}^{\alpha=1})$. Also $\sum_\alpha\Po(\lambda_k^\alpha)=2\Po(\lambda_k^{\alpha=p})+2\Po(b)$ in the region where $\frac{\partial\lambda^{\alpha=p}}{\partial c_p}\neq 0$.

From \autoref{eq:app-Fisher Info Integrated Out - diagonal entries} the cross terms vanish ($I_{pq}=0$ because $\frac{\partial\Po(\lambda^{\alpha=p})}{\partial c_p}\frac{\partial\Po(\lambda^{\alpha=q})}{\partial c_{q}}=0$). The diagonal elements 
%
\begin{alignat}{3}
	I_{pp} 
	& =\sum_{k=1}^N\E_k\left[\frac{\left(2\frac{\partial\Po(\lambda_k^{\alpha=p})}{\partial c_p}\right)^2}{\left(2\Po(\lambda_k^{\alpha=p})+2\Po(b)\right)^2}\right] \\
 	& =\sum_{k=1}^N\E_k\left[\frac{\left(\Po(\lambda_k^{\alpha=p})\frac{\left(n_k-\lambda_k^{\alpha=p}\right)}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2}{\left(\Po(\lambda_k^{\alpha=p})+\Po(b)\right)^2}\right]\label{eq:app-FREM blink limit infty bg}
\end{alignat}
%
for $b=0$ $\left(\Po(b)=0\right)$:
%
\begin{alignat}{2}
	I_{pp} 
	 &=\sum_{k=1}^N\left(\frac{1}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\E_k\left[\left(n_k-\lambda_k^{\alpha=p}\right)^2\right]\\
 	& =\sum_{k=1}^N\left(\frac{1}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\frac{1}{4}\sum_{n_k\geq0}\left(\sum_{i=1}^4\Po(\lambda_k^\alpha)\left(n_k-\lambda_k^{\alpha=p}\right)^2\right)\\
 	& =\sum_{k=1}^N\left(\frac{1}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\frac{1}{4}\sum_{n_k\geq0}\left(2\Po(\lambda_k^{\alpha=p})\left(n_k-\lambda_k^{\alpha=p}\right)^2\right)\\
 	& =\sum_{k=1}^N\left(\frac{1}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\frac{1}{2}\lambda_k^{\alpha=p}\\
 	& =\frac{1}{2}\sum_{k=1}^N\frac{1}{\lambda_k^{\alpha=p}}\left(\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2	\label{eq:app-FREM blink lim infty b=0}
\end{alignat}
%
which is the expression for the static sources \autoref{eq:app-Fisher Info for Poisson}, up to a factor $1/2$. The factor 1/2 comes from the fact that the source appears only in 50\% of the observations. If we keep the number of photons constant in both blinking and the static case (by reducing the intensity of the static sources by factor of two) we get identical value of $\var(d)$ for the $d\rightarrow\infty$. 

For non zero background $b>0$ we cannot simplify \autoref{eq:app-FREM blink limit infty bg} due to the background term $\Po(b)$ in the denominator. However, as the term is positive, the element $I_{pp}$ will be decreasing with increasing background. The background makes therefore the variance $\var(d)$ bigger as we would expect.

%==========================================
%==========================================

\section{Comments on the ``dip'' in FREM curves for static sources}
\label{sec:Appendix dip comments}
These are several comments about the strange ``dip'' in the FREM curves for static sources (see \autoref{fig:FREM dip}). The expression for computing FREM for two sources with equal intensity is from \autoref{eq:var symmetric}
\begin{equation}
	\var(d)=\left[\frac{I_{11}-I_{22}}{2}\right]^{-1},
	\label{eq:app-vard}
\end{equation}
%
and the individual entries of the Fisher information matrix is from \autoref{eq:app-Fisher Information alternative - Individual}:
\begin{equation}
	I_{ij} =\Lambda\sum_{k=1}^{K}\frac{q'_k(c_i)q'_k(c_j)}{q_k(c_1)+q_k(c_2)+b/\Lambda};\; \ i,j=\{1,2\},
\end{equation}
%
where $q_k$ and $q'_k$ are pixelised PSF and derivative, respectively. 

For large background values ($b/\Lambda \gg \max[q_k(c_i)]$) the bottom term is nearly constant and therefore
\begin{equation}
	I_{ij} \approx C \sum_{k=1}^{K}q'_k(c_i)q'_k(c_j),
\end{equation}
where $C$ is a constant.

From \autoref{eq:app-vard}
\begin{equation}
	\var(d)\approx \left[C/2 \sum_k \left(q'_k(c_1)^2-q'_k(c_1)q'_k(c_2)\right)\right]^{-1}
\end{equation}

For Gaussian approximation of the PSF, the expression can be integrated analytically. The resulting curve sows the identical ``dip'' in the curves.

%When the background is present % <---- I think this is incorrect.... because the background in in \lambda^p_k
%%
%\begin{alignat*}{1}
%	I_{pp} & =\sum_{k=1}^N\E_k\left[\frac{\left(2\frac{\partial\Po(\lambda_k^{\alpha=p})}{\partial c_p}\right)^2}{\left(2\Po(\lambda_k^{\alpha=p})+2\Po(b)\right)^2}\right]\\
% 	& =\sum_{k=1}^N\E_k\left[\frac{\left(\Po(\lambda_k^{\alpha=p})\frac{\left(n_k-\lambda_k^{\alpha=p}\right)}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2}{\left(\Po(\lambda_k^{\alpha=p})+\Po(b)\right)^2}\right]\\
% 	& =\sum_{k=1}^N\left(\frac{1}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\E_k\left[\left(\frac{\Po(\lambda_k^{\alpha=p})}{\Po(\lambda_k^{\alpha=p})+\Po(b)}\right)^2\left(n_k-\lambda_k^{\alpha=p}\right)^2\right]\\
% 	& =\sum_{k=1}^N\left(\frac{1}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\E_k\left[\left(1-\frac{\Po(b)}{\Po(\lambda_k^{\alpha=p})+\Po(b)}\right)^2\left(n_k-\lambda_k^{\alpha=p}\right)^2\right]\\
% 	& =\frac{1}{2}\sum_{k=1}^N\frac{1}{\lambda_k^{\alpha=p}}\left(\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2-S
%\end{alignat*}
%%
%where
%%
%\begin{equation}
%	S=\sum_{k=1}^N\left(\frac{1}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\E_k\left[\left\{ \frac{2\Po(b)}{\Po(\lambda_k^{\alpha=p})+\Po(b)}+\left(\frac{2\Po(b)}{\Po(\lambda_k^{\alpha=p})+\Po(b)}\right)^2\right\} \left(n_k-\lambda_k^{\alpha=p}\right)^2\right].
%\end{equation}
%%
%This term is positive and therefore reduces $I_{pp}$.