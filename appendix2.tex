%!TEX root = thesis.tex
\chapter{Resolution limit for blinking QDs\label{app:Appendix2}}

%==========================================
%==========================================

This is derivation of the fisher information for Poisson distributed
variable $X$ with mean $\lambda$.

\begin{equation}
X\sim\Po(n,\lambda)=p(n|\theta)=\frac{\lambda^{n}e^{-\lambda}}{n!}
\end{equation}



\section{Likelihood}

Likelihood of the Poisson distributed variable with detection $n_k$ in K pixels: 
%
\begin{equation}
	l(\theta)=\prod_{k=1}^Kl_k=\prod_{k=1}^K\frac{\lambda_k^{n_k}e^{-\lambda_k}}{n_k!}\label{eq:app-Likelihood of Poisson}
\end{equation}
%
where $l_k(\theta)=p(n_k|\theta)$ to emphasise the dependency on the parameter $\theta$.

Log-Likelihood:
\begin{equation}
	\mathcal{L}=\sum_k\left(n_k\log\lambda_k-\lambda_k-\log n_k!\right)
\end{equation}

%==========================================
%==========================================

\section{Fisher Information}

Fisher information:
\begin{equation}
	I(\theta)=-\E\left[\frac{\partial^2\mathcal{L}}{\partial\theta^2}\right]=\E\left[\left(\frac{\partial\mathcal{L}}{\partial\theta}\right)^2\right]=\E\left[\left(\sum_k\frac{\partial\log(l_k)}{\partial\theta}\right)^2\right]=\E\left[\left(\sum_k\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}\right)^2\right]\label{eq:app-Fisher Info Definition}
\end{equation}


\begin{alignat*}{2}
	I(\theta) 
	& =\E\left[\left(\sum_k\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}\right)\left(\sum_{m}\frac{1}{l_{m}}\frac{\partial l_{m}}{\partial\theta}\right)\right]\\
	& =\E\left[\sum_k\frac{1}{l_k^2}\left(\frac{\partial l_k}{\partial\theta}\right)^2\right]+\E\left[\sum_k\sum_{m\neq k}\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}\frac{1}{l_{m}}\frac{\partial l_{m}}{\partial\theta}\right]
\end{alignat*}
%
as $n_k$ are iid then the second term can be expressed as 
%
\begin{align*}
	\E\left[\sum_k\sum_{m\neq k}\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}\frac{1}{l_{m}}\frac{\partial l_{m}}{\partial\theta}\right] 
	& =\sum_k\sum_{m\neq k}\E_k\left[\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}\right]\E_{m}\left[\frac{1}{l_{m}}\frac{\partial l_{m}}{\partial\theta}\right]
\end{align*}
%
where
%
\begin{equation}
	\E_k\left[f(n_k)\right]=\sum_{n_k\geq0}p(n_k|\theta)f(n_k)
\end{equation}
%
But 
\begin{equation}
	\E_k\left[\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}\right]=\sum_{n_k}l_k\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}=\sum_{n_k}\frac{\partial l_k}{\partial\theta}=\frac{\partial\sum_{n_k}l_k}{\partial\theta}=0
\end{equation}

as $\sum_{n_k}l_k=\sum_{n_k}p(n_k|\theta)=1$. The Fisher Information can then be expressed 
%
\begin{align*}
	I(\theta) & =\E\left[\sum_k\frac{1}{l_k^2}\left(\frac{\partial l_k}{\partial\theta}\right)^2\right]\\
	& =\sum_{k=1}^k\sum_{n_k\geq0}l_k\frac{1}{l_k^2}\left(\frac{\partial l_k}{\partial\theta}\right)^2\\
	& =\sum_{k=1}^k\sum_{n_k\geq0}\frac{1}{l_k}\left(\frac{\partial l_k}{\partial\theta}\right)^2
\end{align*}

Inserting the derivatives of likelihood \autoref{eq:app-Likelihood of Poisson}: 
%
\begin{equation}
	\frac{\partial l_k}{\partial\theta}=\frac{l_k(n_k-\lambda_k)}{\lambda_k}\frac{\partial\lambda_k}{\partial\theta}
\end{equation}
%
we obtain
%
\begin{align*}
	I(\theta) 
	& =\sum_{k=1}^k\sum_{n_k\geq0}\frac{l_k(n_k-\lambda_k)^2}{\lambda_k^2}\left(\frac{\partial\lambda_k}{\partial\theta}\right)^2\\
	& =\sum_{k=1}^k\frac{1}{\lambda_k^2}\left(\frac{\partial\lambda_k}{\partial\theta}\right)^2\E_k\left[(n_k-\lambda_k)^2\right]
\end{align*}
%
for Poisson $\var(n)=\mathrm{mean}(n)=\lambda$ gives 
%
\begin{equation}
	\E_k\left[(n_k-\lambda_k)^2\right]=\var(n_k)=\lambda_k,
\end{equation}
%
and therefore
%
\begin{equation}
	I(\theta)=\sum_{k=1}^K\frac{1}{\lambda_k}\left(\frac{\partial\lambda_k}{\partial\theta}\right)^2.
	\label{eq:app-Fisher Info for Poisson}   	  
\end{equation}

This is the pixelised version (detection of the photons in K detectors - CCD camera and $\lambda_k=\int_{\Gamma_k}\lambda(x)dx$ where $\Gamma_k$ is an area of the pixels of the detector). 

%==========================================
%==========================================
\clearpage{}
\section{Two sources separated by a distance $d$\label{sub:Two-sources-separated}}
(Comments on Fisher Information estimation as described by Ram et al. \cite{Ram2006}.)


For two sources separated by a distance $d$ we have a mean value of the intensity:
%
\begin{equation}
	\lambda=\Lambda_1f_1+\Lambda_2f_2
\end{equation}
%
where $f_i$ and $\Lambda_i$ is the response function and intensity, respectively, of the source $i$. For translationally invariant PSF and in-focus sources: $f_1=q(x-\frac{d}{2})$ and $f_2=q(x+\frac{d}{2})$
%
\begin{equation}
	\lambda(d)=\Lambda_1q(x-\frac{d}{2})+\Lambda_2q(x+\frac{d}{2})
	\label{eq:app-expected intensity d/2}
\end{equation}
%
where $q$ is the PSF of the sources. For pixelised version (integral over pixel area $\Gamma_k$) with homogeneous background $b$ in each pixel the intensity can be expressed as:
%
\begin{equation}
	\lambda_k(d)=\Lambda_1\int_{\Gamma_k}q(x-\frac{d}{2})dx+\Lambda_2\int_{\Gamma_k}q(x+\frac{d}{2})dx+b
\end{equation}
%
so we get by plugging into \autoref{eq:app-Fisher Info for Poisson} expression for the Fisher Information:
%
\begin{alignat}{2}
	I(d)
	&=\frac{1}{4}\sum_{k=1}^K\frac{\left(\Lambda_1\int_{\Gamma_k}\partial_{x}q(x-\frac{d}{2})dx-\Lambda_2\int_{\Gamma_k}\partial_{x}q(x+\frac{d}{2})dx\right)^2}{\Lambda_1\int_{\Gamma_k}q(x-\frac{d}{2})dx+\Lambda_2\int_{\Gamma_k}q(x+\frac{d}{2})dx+b}\nonumber\\
	&=\frac{1}{4}\sum_{k=1}^N\frac{\left[\Lambda_1q_k'(-\frac{d}{2})-\Lambda_2q_k'(\frac{d}{2})\right]^2}{\Lambda_1q_k(-\frac{d}{2})+\Lambda_2q_k(\frac{d}{2})+b},
	\label{eq:app-Fisher Info Pixelised - Ram}	
\end{alignat}
%
where we set $q_k(z)=\int_{\Gamma_k}q(x-z)dx$ as the pixelised version of a point spread function translated by $z$ with $\Gamma_k$ being an area of the $k$th pixel, and $q'_k(z)=\int_{\Gamma_k}\frac{\partial q(x-z)}{\partial x}dx$ as the corresponding pixelised derivative.


\subparagraph*{Limit $d=0$:}\ \\
If $\Lambda_1=\Lambda_2$ then $I(d=0)=0$ which means $\var(d=0)\rightarrow\infty$. However, for unequal sources $\Lambda_1\neq\Lambda_2$ this does not hold and the variance remains finite.

\subparagraph*{Limit $d\rightarrow\infty$:}\ \\
When sources are far apart then the mixing term in nominator in \autoref{eq:app-Fisher Info Pixelised - Ram} $\Lambda_1\Lambda_2\partial_{x}q(x-\frac{d}{2})\partial q(d+\frac{d}{2})=0$ as the $\partial_{x}q(x-\frac{d}{2})$ and $\partial_{x}q(x+\frac{d}{2})$ do not have common overlap. \Autoref{eq:app-Fisher Info Pixelised - Ram} then decomposes into two individual terms
%
\begin{equation*}
	I(d) 
	=\frac{1}{4}\sum_{k=1}^K\left[\frac{\left(\Lambda_1q'_k(x-\frac{d}{2})\right)^2}{\Lambda_1q_k(x-\frac{d}{2})+b}+\frac{\left(\Lambda_2 q'_k(x+\frac{d}{2})\right)^2}{\Lambda_2q_k(x+\frac{d}{2})+b}\right]
\end{equation*}
%
This corresponds to a sum of Fisher Information for localisation of individual sources. 

\subparagraph*{Situation with missing source $\Lambda_i=0,\ \Lambda_j\neq0$:}\ \\
Even if one of the source is missing the Fisher information is strangely non-zero $I(d)\neq0$. The variance remains finite even if one of the sources is not present!

%==========================================
%==========================================

\section{An alternative way to derive Fisher information for two sources separated by $d$:}
\label{sec:Appendix FI alternative}
This is a suggestion how to fix the problems with limits for Fisher Information derived above. This gives infinite variance when one of the sources is no present. Also fix weird behaviour of unequal sources for the limit $d\rightarrow0$. 

We consider two sources located at $c_1$ and $c_2$, respectively. The expectation of the intensity is therefore expressed as (cf. \autoref{eq:app-expected intensity d/2}):
%
\begin{equation}
	\lambda_k(\bm{c})=\Lambda_1q_k(x-c_1)+\Lambda_2q_k(x-c_2)+b
	\label{eq:app-Expected intensity c_1 c_2}
\end{equation}
%
The distance between the sources is $d=c_1-c_2$. This is a linear combination $\bm{a}^T\cdot\bm{c}$ of the variable $\bm{c}=(c_1,c_2)^T$ where $\bm{a}=(1,-1)^T$. The variance of $d$ is given by 
%
\begin{equation}
	\var(d)=\var(\bm{a}^T\cdot\bm{c})=\bm{a}^T\cdot\bm{Q}\cdot\bm{a}=Q_{11}+Q_{22}-2Q_{12}
\end{equation}
%
where $\bm{Q}$ is a covariance matrix $\bm{Q}=\bm{I}^{-1}(\theta)$ and $\bm{I}(\theta)$ is the Fisher information matrix (symmetric
$I_{12}=I_{21}$)
%
\begin{equation}
	\bm{I}(\theta)=\left(
	\begin{array}{cc}
		I_{11} & I_{12}\\
		I_{12} & I_{22}
	\end{array}\right)
\end{equation}
%
given by generalisation of \autoref{eq:app-Fisher Info for Poisson}
%
\begin{equation}
	I_{ij}(\theta)=\sum_{k=1}^K\frac{1}{\lambda_k}\frac{\partial\lambda_k}{\partial c_i}\frac{\partial\lambda_k}{\partial c_j}
	\label{eq:app-Fisher Info general lambda}
\end{equation}
%
The covariance matrix $\bm{Q}$ is then 
%
\begin{equation}
	\bm{Q}=\bm{I}^{-1}(\theta)=\frac{1}{I_{11}I_{22}-I_{12}^2}\left(
	\begin{array}{cc}
		I_{22} & -I_{12}\\
		-I_{12} & I_{11}
	\end{array}\right)
\end{equation}
%
and the variance of $d=c_1-c_2$ 
%
\begin{equation}
	\var(d)=(1,-1)^T\cdot\bm{Q}\cdot(1,-1)=\frac{I_{11}+I_{22}+2I_{12}}{I_{11}I_{22}-I_{12}^2}=\frac{p}{r}
	\label{eq:app-variance alternative}
\end{equation}

The individual terms of the Fisher Information matrix by using \autoref{eq:app-Expected intensity c_1 c_2} in \autoref{eq:app-Fisher Info general lambda}
%
\begin{equation}
	I_{ij} =\Lambda_i\Lambda_j\sum_{k=1}^K\frac{q'_k(c_i)q'_k(c_j)}{f_k(c_1,c_2)},
	\label{eq:app-Fisher Information alternative - Individual}
\end{equation}
%
where $q_k(c_i)$ is the pixelised version (pixel area $\Gamma_k$) of the PSF
%
\begin{alignat*}{2}
	q_k(c_i) & =\int_{\Gamma_k}q(x-c_i)dx\\
	q'_k(c_i) & =\int_{\Gamma_k}\frac{\partial q(x-c_i)}{\partial x}dx
\end{alignat*}
%
and $f_k(c_1,c_2)=\Lambda_1q_k(c_1)+\Lambda_2q_k(c_2)+b$.
 
Numerator $p=I_{11}+I_{22}+2I_{12}$ in \autoref{eq:app-variance alternative}
\begin{equation}
	p=\sum_{k=1}^K\frac{1}{f_k(c_1,c_2)}\left[\Lambda_1^2q'^2{}_k(c_1)+\Lambda_2^2q'{}_k^2(c_2)+2\Lambda_1\Lambda_2q'_k(c_1)q'_k(c_2)\right]
\end{equation}

The terms in the denominator $r=I_{11}I_{22}-I_{12}^2$ in \autoref{eq:app-variance alternative}
%
\begin{alignat*}{2}
	I_{11}I_{22} & =\Lambda_1^2\Lambda_2^2\sum_{k,l}^K\frac{\left(q'_k(c_1)q'_l(c_2)\right)^2}{f_k(c_1,c_2)f_l(c_1,c_2)}\\
	I_{12}^2 & =\Lambda_1^2\Lambda_2^2\sum_{k,l}^K\frac{q'_k(c_1)q'_k(c_2)q'_l(c_1)q'_l(c_2)}{f_k(c_1,c_2)f_l(c_1,c_2)}
\end{alignat*}

\subparagraph*{Limit $c_1\rightarrow c_2,\,(d\rightarrow0)\Rightarrow q_k(c_1)\rightarrow q_k(c_2)$:}\ \\
%
\begin{equation}
	p=(\Lambda_1^2+\Lambda_2^2+2\Lambda_1\Lambda_2)\sum_{k=1}^K\frac{q'{}_k^2(c)}{f_k(c,c)},
\end{equation}
%
which can be further simplified by explicitly writing $f_k(c_1,c_2)$
%
\begin{equation}
	p=(\Lambda_1+\Lambda_2)\sum_{k=1}^K\frac{q'{}_k^2(c)}{q_k(c)+b/(\Lambda_1+\Lambda_2)}
\end{equation}
%
$q_k$ and $(q'_k)^2$ are strictly positive functions, therefore the sum is not zero and $p$ is non-zero for any $\Lambda_1,\,\Lambda_2$. 

The two terms in the denominator in \autoref{eq:app-variance alternative} are identical for $c_1=c_2$
\begin{equation}
	I_{11}I_{22}=I_{12}^2
	%\frac{\Lambda_1^2\Lambda_2^2}{\left(\Lambda_1+\Lambda_2\right)^2}\sum_{k,l=1}^K\frac{\left(q'_k(c)\right)^2}{\left(q_k(c)+d/(\Lambda_1+\Lambda_2)\right)}\frac{\left(q'_l(c)\right)^2}{\left(q_l(c)+d/(\Lambda_1+\Lambda_2)\right)}
\end{equation}
%
and therefore 
%
\begin{equation*}
	r=I_{11}I_{22}-I_{12}^2=\det\left[\bm{I}(\theta)\right] \equiv 0
\end{equation*}
%
for any $\Lambda_i$. $\bm{I}(\theta)$ is therefore a singular matrix for $d=0$ and inversion $\bm{I}^{-1}(\theta)$ does not exist for $c_1=c_2$. However,  the limit $c_1\rightarrow c_2,\,(d\rightarrow0)$ gives $p\neq0,\, r\rightarrow0$ and $\var(d\rightarrow0)=\frac{p}{r}\rightarrow\infty$. The variance therefore diverges. 

\subparagraph*{Limit $d\rightarrow\infty$:}\ \\
The cross term $I_{ij}$ in \autoref{eq:app-Fisher Information alternative - Individual} vanishes ($I_{ij}=0,\: i\neq j$) because of the multiplication $q'_k(c_1)q'_k(c_2)$ which is zero for largely separated PSF with finite support (if the support of $q_k(c_1)$ and $q_k(c_2)$ do not have mutual overlap). Then from \autoref{eq:app-variance alternative} 
%
\begin{equation}
	\var(d)=\frac{1}{I_{11}}+\frac{1}{I_{22}}
\end{equation}
%
which is the sum of variances for estimation two separated sources.

\begin{equation}
	I_{ii}=\Lambda_i^2\sum_{k=1}^K\frac{q_k'^2(c_i)}{\Lambda_1q_k(c_1)+\Lambda_2q_k(c_2)+b}=\Lambda_i\sum_{k=1}^K\frac{q_k'^2(c_i)}{q_k(c_i)+b/\Lambda_i}
\end{equation}
%
if $q_k(c_i)$ (and $q_k'(c_i)$) have a finite support.

For non-pixelised version, negligible background $b/\Lambda\ll1$ and for Gaussian approximation of the PSF ($q(x-a)\propto\exp\left(-\frac{(x-a)^2}{2\sigma^2}\right)$) with
$\sigma=\frac{\sqrt{2}}{2\pi}\frac{\lambda}{NA}$ (\cite{Zhang2007}) we have $q'(x)=\frac{1}{\sigma^2}xq(x)$ and $q'^2/q=\frac{1}{\sigma^4}x^2q$ which gives $\int q'^2/qdx=\frac{1}{\sigma^4}\int qx^2=\frac{1}{\sigma^4}\sigma^2=\frac{1}{\sigma^2}$ and therefore $I_{ii}=\frac{\Lambda_i}{\sigma^2}$ 

\begin{equation*}
	\var(d\rightarrow\infty) =\sigma^2\left(\frac{1}{\Lambda_1}+\frac{1}{\Lambda_2}\right)
\end{equation*}

\subparagraph*{Situation with one missing source $\Lambda_i=0,\ \Lambda_j\neq0$:}\ \\
$I_{ii}\equiv0$ and $I_{ij}\equiv0$ and so $\det(\bm{I}(\theta))\equiv0$, and matrix is singular. In the limit $\Lambda_i\rightarrow0$ the
variance \autoref{eq:app-variance alternative} $\var(d)\rightarrow\infty$. 

%==========================================

\section{Comparison of the original FREM with our proposed formula}
For equally strong sources ($\Lambda_1= \Lambda_2=\Lambda$) the original FREM formula gives identical results as our proposed one. Due to symmetry we get equality of the diagonal terms $I_{11}=I_{22}$. From \autoref{eq:app-variance alternative}
\begin{alignat}{2}
	\var^{-1}(d)
	&=\frac{I_{11}^2-I_{12}^2}{2(I_{11}+I_{12})}\nonumber\\
	&=\frac{1}{2}\left(I_{11}-I_{12}\right)
\end{alignat}
%
From  \autoref{eq:app-Fisher Information alternative - Individual} we get
\begin{alignat}{2}
	\var^{-1}(d)
	&=\frac{\Lambda}{2}\sum_k\left[\frac{\left(q'_k(c_1)\right)^2-q'_k(c_1)q'_k(c_2)}{q_k(c_1)+q_k(c_2)+b/\Lambda} \right]\nonumber\\
	&=\frac{\Lambda}{4}\sum_k\left[\frac{\left(q'_k(c_1)-q'_k(c_2)\right)^2}{q_k(c_1)+q_k(c_2)+b/\Lambda} \right],
\end{alignat}
% 
where we used $\sum_k\left(q'_k(c_1)\right)^2=\sum_k\left(q'_k(c_2)\right)^2$. Comparison with \autoref{eq:app-Fisher Info Pixelised - Ram} shows the equality of the both formulas for the situation of equally strong sources $\Lambda_1=\Lambda_2$.

For sources with different intensity $\Lambda_1\neq\Lambda_2$ can be shown that the ratio between the original FREM and our proposed formula for a given distance $d$ is a function of $\Lambda_1/\Lambda_2$. 

%==========================================
%==========================================

\section{Time distribution of the intensities - averaging\label{sec:Appendix - blinking not integrated}}
We assume a likelihood dependent on parameter $\bm{\Lambda}$ (for example, $\bm{\Lambda}=(\Lambda_1,\Lambda_2)$ - intensity of two sources in the recorede frame). If we knew the configuration of $\bm{\Lambda}$ we would write the log-likelihood
%
\begin{equation}
	\mathcal{L}(\theta,\Lambda)=\sum_{k=1}^K\log\left(l_k(\theta,\bm{\Lambda})\right).
\end{equation}
%
Derivatives with respect to the parameter $\theta$:
%
\begin{alignat*}{2}
	\frac{\partial\mathcal{L}(\theta,\bm{\Lambda})}{\partial \theta}
	&=\sum_{k=1}^K\frac{\partial\log\left(l_k(\theta,\bm{\Lambda})\right)}{\partial \theta}\\
	&=\frac{\partial\mathcal{L}(\theta,\bm{\Lambda})}{\partial \theta}. 
\end{alignat*}

If we assume a probability distribution $p(\bm{\Lambda})$  of the intensity states $\bm{\Lambda}$ we can express the Fisher information (see \autoref{eq:app-Fisher Info Definition}):
%
\begin{equation*}
	I(\theta) = \int_{\bm{\Lambda}}p(\bm{\Lambda})I_{\bm{\Lambda}}(\theta)d\bm{\Lambda},
\end{equation*}
%
where $I_{\bm{\Lambda}}(\theta)$ is the Fisher information computed for a specific value of $\bm{\Lambda}$ (see \autoref{eq:app-Fisher Info Definition}).

For discrete states of $\bm{\Lambda}$, for example  
%
\begin{alignat*}{4}
	\bm{\Lambda}
	&=\left\{\bm{\Lambda^{\alpha=1}},\,\bm{\Lambda^{\alpha=2}},\,\bm{\Lambda^{\alpha=3}},\,\bm{\Lambda^{\alpha=4}}\right\}\\
	&=\left\{[\Lambda_1,0],\,[\Lambda_2,0],\,[\Lambda_1,\Lambda_2],\,[0,0]\right\}
\end{alignat*}	
% 
we get
\begin{equation}
	I(\theta)=\sum_{\alpha}p(\bm{\Lambda^\alpha})I_{\bm{\Lambda^\alpha}}(\theta),
\end{equation}
%
where the Fisher Information for every configuration of $\bm{\Lambda^\alpha}$ is averaged with weights $p(\bm{\Lambda^\alpha})$. 

%==========================================
%==========================================

\section{Time distribution of the intensities - integrating out $\Lambda$}
\label{sub:Appendix Time-distribution-Integrating out}
If we do not know the configuration of the $\bm{\Lambda^\alpha}$ in each frame, then we have to rely only on the distribution $p(\Lambda)$ and integrate over it within the likelihood function:
%
\begin{equation}
	l_k(\theta)=\int_{\Lambda}l_k(\theta,\Lambda)d\Lambda=\int_{\Lambda}p(n_k|\theta,\Lambda)p(\Lambda)d\Lambda
	\label{eq:app-log lik - int out}
\end{equation}
%
We assume four state model of two sources: $\left\{ (\Lambda_1,0),(0,\Lambda_2),(\Lambda_1,\Lambda_2),(0,0)\right\}$: 
%
\begin{alignat}{4}
	\lambda_k^{\alpha=1}&=\Lambda_1q_k(x-c_1) & &+b,\nonumber\\ 
	\lambda_k^{\alpha=2}&=&\Lambda_2q_k(x-c_2) &+b,\nonumber\\ 
	\lambda_k^{\alpha=3}&=\Lambda_1q_k(x-c_1)&+\Lambda_2q_k(x-c_2)&+b,\nonumber\\ 
	\lambda_k^{\alpha=4}&=& &+b,
\end{alignat}
%
with uniform distribution over these states. We used uniform background intensity $b$ in each pixel of each frame. 

From \autoref{eq:app-log lik - int out}:
%
\begin{equation}
	l_k(\theta)=\frac{1}{4}\sum_{\alpha=1}^4\Po(\lambda_k^\alpha),
\end{equation}
%
with derivatives 
%
\begin{equation}
	\frac{\partial l_k}{\partial c_p}=\frac{1}{4}\sum_\alpha\frac{\partial\Po(\lambda_k^\alpha)}{\partial c_p}=\frac{1}{4}\sum_\alpha\left(\Po(\lambda_k^\alpha)\frac{(n_k-\lambda_k^\alpha)}{\lambda_k^\alpha}\frac{\partial\lambda_k^\alpha}{\partial c_p}\right).
\end{equation}
%
The Fisher information matrix diagonal entries:
%
\begin{alignat}{2}
	I_{pp}(\bm{c}) & =\E\left[\left(\sum_{k=1}^N\frac{1}{l_k}\frac{\partial l_k}{\partial c_p}\right)^2\right]\nonumber \\
 	& =\E\left[\left\{ \sum_{k=1}^N\left(\frac{1}{\sum_{\alpha=1}^4\Po(\lambda_k^\alpha)}\frac{\partial\sum_{\alpha=1}^4\Po(\lambda_k^\alpha)}{\partial c_p}\right)\right\} \left\{ \sum_{l=1}^N\left(\frac{1}{\sum_{\alpha=1}^4\Po(\lambda_l^\alpha)}\frac{\partial\sum_{\alpha=1}^4\Po(\lambda_l^\alpha)}{\partial c_p}\right)\right\} \right]\nonumber \\
	& =\sum_{k=1}^N\E_k\left[\frac{\left(\sum_{\alpha=1}^4\frac{\partial\Po(\lambda_k^\alpha)}{\partial c_p}\right)^2}{\left(\sum_{\alpha=1}^4\Po(\lambda_k^\alpha)\right)^2}\right],
	\label{eq:app-Fisher Info Integrated Out - diagonal entries}
\end{alignat}
%
because the cross terms ($k,\, l$) in the sum (2nd row) are zeros: 
%
\begin{alignat*}{2}
	\E\left[\left(\frac{\sum_{\alpha=1}^4\frac{\partial\Po(\lambda_k^\alpha)}{\partial c_p}}{\sum_{\alpha=1}^4\Po(\lambda_k^\alpha)}\right)\left(\frac{\sum_{\alpha=1}^4\frac{\partial\Po(\lambda_l^\alpha)}{\partial c_p}}{\sum_{\alpha=1}^4\Po(\lambda_l^\alpha)}\right)\right] 
	& =\E_k\left[\frac{\sum_{\alpha=1}^4\frac{\partial\Po(\lambda_k^\alpha)}{\partial c_p}}{\sum_{\alpha=1}^4\Po(\lambda_k^\alpha)}\right]\E_l\left[\frac{\sum_{\alpha=1}^4\frac{\partial\Po(\lambda_l^\alpha)}{\partial c_p}}{\sum_{\alpha=1}^4\Po(\lambda_l^\alpha)}\right]\\
 	& =\sum_{\alpha=1}^4\frac{\partial}{\partial c_p}\left(\sum_{n_k\geq0}\Po(\lambda_k^\alpha)\right)\sum_{\alpha=1}^4\frac{\partial}{\partial c_p}\left(\sum_{n_k\geq0}\Po(\lambda_l^\alpha)\right)\\
 	& =0
\end{alignat*}

Expressing the derivatives and the expectation from \autoref{eq:app-Fisher Info Integrated Out - diagonal entries} we can write for the diagonal entries of the Fisher information matrix:
%
\begin{alignat*}{2}
	I_{pp}(\bm{c}) 
	& =\sum_{k=1}^N\E_k\left[\left\{ \frac{\sum_{\alpha=1}^4\left(\Po(n_k;\lambda_k^\alpha)\frac{(n_k-\lambda_k^\alpha)}{\lambda_k^\alpha}\frac{\partial\lambda_k^\alpha}{\partial c_p}\right)}{\sum_{\alpha=1}^4\Po(n_k;\lambda_k^\alpha)}\right\} ^2\right]\\
 	& =\frac{1}{4}\sum_{k=1}^N\sum_{n_k\geq0}\frac{\left\{ \sum_{\alpha=1}^4\left(\Po(n_k;\lambda_k^\alpha)\frac{(n_k-\lambda_k^\alpha)}{\lambda_k^\alpha}\frac{\partial\lambda_k^\alpha}{\partial c_p}\right)\right\} ^2}{\sum_{\alpha=1}^4\Po(n_k;\lambda_k^\alpha)}
\end{alignat*}

For the four states model we have $\lambda^{\alpha=3}(c_1,c_2)=\lambda^{\alpha=1}(c_1)+\lambda^{\alpha=2}(c_2)-b$ and so $\frac{\partial\lambda^{\alpha=3}}{\partial c_p}=\frac{\partial\lambda^{\alpha=p}}{\partial c_p}$ and $\frac{\partial\lambda^{\alpha=j}}{\partial c_p}=0,\, j\neq p$ for $p=\{1,2\},\: j=\{1,2,4\};$ so 
%
\begin{equation*}
	I_{pp}(\bm{c}) 
	=\frac{1}{4}\sum_{k=1}^N\left(\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\sum_{n_k\geq0}\frac{\left\{\sum_{\alpha=\{p,3\}}\left(\Po(n_k;\lambda_k^\alpha)\frac{(n_k-\lambda_k^\alpha)}{\lambda_k^\alpha}\right)\right\}}{\sum_{\alpha=1}^4\Po(n_k;\lambda_k^\alpha)} ^2
\end{equation*}


The Fisher information matrix off-diagonal entries:
%
\begin{alignat*}{2}
	I_{pq}(\bm{c}) 
	& =\sum_{k=1}^N\E_k\left[\frac{\left(\sum_{\alpha=1}^4\frac{\partial\Po(\lambda_k^\alpha)}{\partial c_p}\right)\left(\sum_{\alpha=1}^4\frac{\partial\Po(\lambda_k^\alpha)}{\partial c_q}\right)}{\left(\sum_{\alpha=1}^4\Po(\lambda_k^\alpha)\right)^2}\right]\nonumber \\
 	& =\frac{1}{4}\sum_{k=1}^N\left(\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)\left(\frac{\partial\lambda_k^{\alpha=q}}{\partial c_q}\right)\times\\
	&\times\sum_{n_k\geq0}\frac{\left(\sum_{\alpha=\{p,3\}}\Po(n_k;\lambda_k^\alpha)\frac{(n_k-\lambda_k^\alpha)}{\lambda_k^\alpha}\right)\left(\sum_{\alpha=\{q,3\}}\Po(n_k;\lambda_k^\alpha)\frac{(n_k-\lambda_k^\alpha)}{\lambda_k^\alpha}\right)}{\sum_{\alpha=1}^4\Po(n_k;\lambda_k^\alpha)}
	\label{eq:app-Fisher Info Integrated Out-off diagonal entries}
\end{alignat*}

\subparagraph*{Limit $d\rightarrow0$:}\ \\
%
When $c^1=c^2$ then $\lambda^{\alpha=1}=\lambda^{\alpha=2}$ and $\frac{\partial\Po(\lambda^{\alpha=1})}{\partial c^1}=\frac{\partial\Po(\lambda^{\alpha=2})}{\partial c^2}$.
Then all entries in $I_{pq}$ are equal and the matrix is singular. For the limit $d\rightarrow0$ the determinant $\det(\bm{I})\rightarrow0$ and the variance $\var(d)\rightarrow\infty$.

\subparagraph*{Limit $d\rightarrow\infty$:}\ \\
%
Sources are far apart and $\lambda^{\alpha=1}$ and $\lambda^{\alpha=2}$ do not have a common overlap. For $k'$ where $\frac{\partial\lambda_{k'}^{\alpha=p}}{\partial c_p}\neq0,\,\frac{\partial\lambda_{k'}^{\alpha=q}}{\partial c_p}\equiv0$ and $\Po(n_{k'},\lambda_{k'}^{\alpha=3})=\Po(n_{k'},\lambda_{k'}^{\alpha=1})$. Also $\sum_\alpha\Po(\lambda_k^\alpha)=2\Po(\lambda_k^{\alpha=p})+2\Po(b)$ in the region where $\frac{\partial\lambda^{\alpha=p}}{\partial c_p}\neq 0$.

From \autoref{eq:app-Fisher Info Integrated Out - diagonal entries} the cross terms vanish ($I_{pq}=0$ because $\frac{\partial\Po(\lambda^{\alpha=p})}{\partial c_p}\frac{\partial\Po(\lambda^{\alpha=q})}{\partial c_{q}}=0$). The diagonal elements 
%
\begin{alignat}{3}
	I_{pp} 
	& =\sum_{k=1}^N\E_k\left[\frac{\left(2\frac{\partial\Po(\lambda_k^{\alpha=p})}{\partial c_p}\right)^2}{\left(2\Po(\lambda_k^{\alpha=p})+2\Po(b)\right)^2}\right] \\
 	& =\sum_{k=1}^N\E_k\left[\frac{\left(\Po(\lambda_k^{\alpha=p})\frac{\left(n_k-\lambda_k^{\alpha=p}\right)}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2}{\left(\Po(\lambda_k^{\alpha=p})+\Po(b)\right)^2}\right]\label{eq:app-FREM blink limit infty bg}
\end{alignat}
%
for $b=0$ $\left(\Po(b)=0\right)$:
%
\begin{alignat}{2}
	I_{pp} 
	 &=\sum_{k=1}^N\left(\frac{1}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\E_k\left[\left(n_k-\lambda_k^{\alpha=p}\right)^2\right]\nonumber\\
 	& =\sum_{k=1}^N\left(\frac{1}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\frac{1}{4}\sum_{n_k\geq0}\left(\sum_{i=1}^4\Po(\lambda_k^\alpha)\left(n_k-\lambda_k^{\alpha=p}\right)^2\right)\nonumber\\
 	& =\sum_{k=1}^N\left(\frac{1}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\frac{1}{4}\sum_{n_k\geq0}\left(2\Po(\lambda_k^{\alpha=p})\left(n_k-\lambda_k^{\alpha=p}\right)^2\right)\nonumber\\
 	& =\sum_{k=1}^N\left(\frac{1}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\frac{1}{2}\lambda_k^{\alpha=p}\nonumber\\
 	& =\frac{1}{2}\sum_{k=1}^N\frac{1}{\lambda_k^{\alpha=p}}\left(\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2	\label{eq:app-FREM blink lim infty b=0}
\end{alignat}
%
which is the expression for the static sources \autoref{eq:app-Fisher Info for Poisson}, up to a factor $1/2$. The factor 1/2 comes from the fact that the source appears only in 50\% of the observations. If we keep the number of photons constant in both blinking and the static case (by reducing the intensity of the static sources by factor of two) we get identical value of $\var(d)$ for the $d\rightarrow\infty$. 

For non zero background $b>0$ we cannot simplify \autoref{eq:app-FREM blink limit infty bg} due to the background term $\Po(b)$ in the denominator. However, as the term is positive, the element $I_{pp}$ will be decreasing with increasing background. The background makes therefore the variance $\var(d)$ bigger as we would expect.

%When the background is present % <---- I think this is incorrect.... because the background in in \lambda^p_k
%%
%\begin{alignat*}{1}
%	I_{pp} & =\sum_{k=1}^N\E_k\left[\frac{\left(2\frac{\partial\Po(\lambda_k^{\alpha=p})}{\partial c_p}\right)^2}{\left(2\Po(\lambda_k^{\alpha=p})+2\Po(b)\right)^2}\right]\\
% 	& =\sum_{k=1}^N\E_k\left[\frac{\left(\Po(\lambda_k^{\alpha=p})\frac{\left(n_k-\lambda_k^{\alpha=p}\right)}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2}{\left(\Po(\lambda_k^{\alpha=p})+\Po(b)\right)^2}\right]\\
% 	& =\sum_{k=1}^N\left(\frac{1}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\E_k\left[\left(\frac{\Po(\lambda_k^{\alpha=p})}{\Po(\lambda_k^{\alpha=p})+\Po(b)}\right)^2\left(n_k-\lambda_k^{\alpha=p}\right)^2\right]\\
% 	& =\sum_{k=1}^N\left(\frac{1}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\E_k\left[\left(1-\frac{\Po(b)}{\Po(\lambda_k^{\alpha=p})+\Po(b)}\right)^2\left(n_k-\lambda_k^{\alpha=p}\right)^2\right]\\
% 	& =\frac{1}{2}\sum_{k=1}^N\frac{1}{\lambda_k^{\alpha=p}}\left(\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2-S
%\end{alignat*}
%%
%where
%%
%\begin{equation}
%	S=\sum_{k=1}^N\left(\frac{1}{\lambda_k^{\alpha=p}}\frac{\partial\lambda_k^{\alpha=p}}{\partial c_p}\right)^2\E_k\left[\left\{ \frac{2\Po(b)}{\Po(\lambda_k^{\alpha=p})+\Po(b)}+\left(\frac{2\Po(b)}{\Po(\lambda_k^{\alpha=p})+\Po(b)}\right)^2\right\} \left(n_k-\lambda_k^{\alpha=p}\right)^2\right].
%\end{equation}
%%
%This term is positive and therefore reduces $I_{pp}$.