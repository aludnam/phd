%!TEX root = thesis.tex
\chapter{Resolution limit for the blinking QDs\label{app:Resolution limit for the blinking QDs}}

%==========================================
%==========================================

\section{Poisson random variable}

This is derivation of the fisher information for Poisson distributed variable $X$ with mean $\lambda$.

\begin{equation}
	X\sim\Po(n,\lambda)=p(n|\theta)=\frac{\lambda^n e^{-\lambda}}{n!}
\end{equation}

Likelihood of the Poisson distributed variable with detection $n_k$ in K pixels: 
%
\begin{equation}
	l(\theta)=\prod_{k=1}^Kl_k=\prod_{k=1}^K\frac{\lambda_k^{n_k}e^{-\lambda_k}}{n_k!}
	\label{eq:Likelihood of Poisson}
\end{equation}
%
where $l_k(\theta)=p(n_k|\theta)$ to emphasise the dependency on the parameter $\theta$.

Log-likelihood:
%
\begin{equation}
	\mathcal{L}=\sum_k\left(n_k\log\lambda_k-\lambda_k-\log n_k!\right)
\end{equation}

%==========================================
%==========================================

\section{Fisher Information for a Poisson variable\label{sub: Appendix Fisher-Information-Poisson}}

Fisher information:
%
\begin{equation}
	I(\theta)=-\E\left[\frac{\partial^2\mathcal{L}}{\partial\theta^2}\right]=\E\left[\left(\frac{\partial\mathcal{L}}{\partial\theta}\right)^2\right]=\E\left[\left(\sum_k\frac{\partial\log(l_k)}{\partial\theta}\right)^2\right]=\E\left[\left(\sum_k\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}\right)^2\right]
	\label{eq:Fisher Info Definition}
\end{equation}

\begin{alignat*}{1}
	I(\theta) & =\E\left[\left(\sum_k\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}\right)\left(\sum_m\frac{1}{l_m}\frac{\partial l_m}{\partial\theta}\right)\right]\\
	 & =\E\left[\sum_k\frac{1}{l_k^2}\left(\frac{\partial l_k}{\partial\theta}\right)^2\right]+\E\left[\sum_k\sum_{m\neq k}\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}\frac{1}{l_m}\frac{\partial l_m}{\partial\theta}\right]
\end{alignat*}
%
as $n_k$ are iid then the second term can be expressed as 
%
\begin{align*}
	\E\left[\sum_k\sum_{m\neq k}\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}\frac{1}{l_m}\frac{\partial l_m}{\partial\theta}\right] 
	& =\sum_k\sum_{m\neq k}\E_k\left[\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}\right]\E_m\left[\frac{1}{l_m}\frac{\partial l_m}{\partial\theta}\right]
\end{align*}
%
where
%
\begin{equation}
	\E_k\left[f(n_k)\right]=\sum_{n_k\geq0}p(n_k|\theta)f(n_k)
\end{equation}

But 
%
\begin{equation}
	\E_k\left[\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}\right]=\sum_{n_k}l_k\frac{1}{l_k}\frac{\partial l_k}{\partial\theta}=\sum_{n_k}\frac{\partial l_k}{\partial\theta}=\frac{\partial\sum_{n_k}l_k}{\partial\theta}=0
\end{equation}
%
as $\sum_{n_k}l_k=\sum_{n_k}p(n_k|\theta)=1$. The Fisher Information can then be expressed 
%
\begin{align*}
	I(\theta) & =\E\left[\sum_k\frac{1}{l_k^2}\left(\frac{\partial l_k}{\partial\theta}\right)^2\right]\\
	 & =\sum_{k=1}^k\sum_{n_k\geq0}l_k\frac{1}{l_k^2}\left(\frac{\partial l_k}{\partial\theta}\right)^2\\
	 & =\sum_{k=1}^k\sum_{n_k\geq0}\frac{1}{l_k}\left(\frac{\partial l_k}{\partial\theta}\right)^2
\end{align*}

Derivatives of likelihood \autoref{eq:Likelihood of Poisson}: 
%
\begin{equation}
	\frac{\partial l_k}{\partial\theta}=\frac{l_k(n_k-\lambda_k)}{\lambda_k}\frac{\partial\lambda_k}{\partial\theta}
\end{equation}
%
And we get:
%
\begin{align*}
	I(\theta) & =\sum_{k=1}^k\sum_{n_k\geq0}\frac{l_k(n_k-\lambda_k)^2}{\lambda_k^2}\left(\frac{\partial\lambda_k}{\partial\theta}\right)^2\\
	 & =\sum_{k=1}^k\frac{1}{\lambda_k^2}\left(\frac{\partial\lambda_k}{\partial\theta}\right)^2\E_k\left[(n_k-\lambda_k)^2\right]
\end{align*}
%
for Poisson $\var(n)=\mathrm{mean}(n)=\lambda$ gives 
%
\begin{equation}
	\E_k\left[(n_k-\lambda_k)^2\right]=\var(n_k)=\lambda_k
\end{equation}
%
and
% 
\begin{equation}
	I(\theta)=\sum_{k=1}^K\frac{1}{\lambda_k}\left(\frac{\partial\lambda_k}{\partial\theta}\right)^2\label{eq:Fisher Info for Poisson}
\end{equation}

This is the pixelised version (detection of the photons in K detectors - CCD camera and $\lambda_k=\int_{C_k}\lambda(x)dx$ where$C_k$ is an area of the pixels of the detector). Non pixelised version \citep{Ram2006}

\begin{equation}
	I(\theta)=\int\frac{1}{\lambda(x)}\left(\frac{\partial\lambda(x)}{\partial\theta}\right)^2dx
\end{equation}

%==========================================
%==========================================

\section{Two sources separated by a distance $d$\label{sub:Appendix Two-sources-separated}}

These are comment on Fisher Information estimation as described in \citep{Ram2006}.

For two sources separated by a distance $d$ we have a mean value of the intensity:
%
\begin{equation}
	\lambda=\Lambda_1f_1+\Lambda_2f_2
\end{equation}
%
where $f_i$ and $\Lambda_i$ is the response function and intensity, respectively, of the source $i$. For translationally invariant PSF
and in-focus sources: $f_1=q(x-\frac{d}{2})$ and $f_2=q(x+\frac{d}{2})$
%
\begin{equation}
	\lambda(d)=\Lambda_1q(x-\frac{d}{2})+\Lambda_2q(x+\frac{d}{2})
\end{equation}
%
where $q$ is the PSF of the sources. For pixelised version (integral over pixel area $C_k$)
%
\begin{equation}
	\lambda_k(d)=\Lambda_1\int_{C_k}q(x-\frac{d}{2})dx+\Lambda_2\int_{C_k}q(x+\frac{d}{2})dx
\end{equation}
%
so we get (as described in\citet{Ram2006})
%
\begin{equation}
	I(d)=\frac{1}{4}\sum_{k=1}^K\frac{\left(\Lambda_1\int_{C_k}\partial_{x}q(x-\frac{d}{2})dx-\Lambda_2\int_{C_k}\partial_{x}q(x+\frac{d}{2})dx\right)^2}{\Lambda_1\int_{C_k}q(x-\frac{d}{2})dx+\Lambda_2\int_{C_k}q(x+\frac{d}{2})dx}
	\label{eq:Fisher Info Pixelised - Ram}
\end{equation}

\subparagraph*{Limit $d=0$}

If $\Lambda_1=\Lambda_2$ then $I(d=0)=0$ which means $\var(d=0)\rightarrow\infty$. (This does not hold for $\Lambda_1\neq\Lambda_2$). 

\noindent
\subparagraph*{Limit $d\rightarrow\infty$}

When sources are far apart then the mixing term in nominator in \autoref{eq:Fisher Info Pixelised - Ram} $\Lambda_1\Lambda_2\partial_{x}q(x-\frac{d}{2})\partial q(d+\frac{d}{2})=0$ as the $\partial_{x}q(x-\frac{d}{2})$ ($q(x-\frac{d}{2})$) and $\partial_{x}q(x+\frac{d}{2})$ ($q(x+\frac{d}{2})$ ) do not have any overlap. \Autoref{eq:Fisher Info Pixelised - Ram} then decomposes into two individual terms (sum of Fisher Information for localisation of individual sources.)

\begin{alignat*}{1}
	I(d) 
	& =\frac{1}{4}\sum_{k=1}^K\left[\frac{\left(\Lambda_1\int_{C_k}\partial_{x}q(x-\frac{d}{2})dx\right)^2}{\Lambda_1\int_{C_k}q(x-\frac{d}{2})dx}+\frac{\left(\Lambda_2\int_{C_k}\partial_{x}q(x+\frac{d}{2})dx\right)^2}{\Lambda_2\int_{C_k}q(x+\frac{d}{2})dx}\right]\\
	& =\frac{1}{4}\sum_{k=1}^K\frac{\left(\int_{C_k}\partial_{x}q(x)dx\right)^2}{\int_{C_k}q(x)dx}\left[\Lambda_1+\Lambda_2\right]
\end{alignat*}

\noindent
\subparagraph*{Limit $\Lambda_i=0$}

If $\Lambda_1=0$ or $\Lambda_2=0$ $I(d)\neq0$. So the variance is finite even if one of the sources is not present.

%==========================================
%==========================================

\section{An alternative way to derive Fisher information for two sources separated by $d$\label{sub:Appendix An-alternative-way-Fisher-info}}

This is a suggestion how to fix the problems with limits for Fisher Information derived above. This gives infinite variance when one of the sources is no present. Also fix weird behaviour of the $I(d)$ for $d=0$. 

For two sources $f_1=q(x-c_1)$ and $f_2=q(x-c_2)$ we have $\lambda=\Lambda_1f_1+\Lambda_2f_2$. The distance between the two sources is $d=c_1-c_2$. This is a linear combination $\bm{a}^T\cdot\bm{c}$ of the variable $\bm{c}=(c_1,c_2)$ where $\bm{a}=(1,-1)$. The variance of $d$ is given by
%
\begin{equation}
	\var(d)=\var(\bm{a}^T\cdot\bm{c})=\bm{a}^T\cdot\bm{Q}\cdot\bm{a}=Q_{11}+Q_{22}-2Q_{12}
	\label{eq:variance of linear combination}
\end{equation}
%
where $\bm{Q}$ is a covariance matrix $\bm{Q}=\bm{I}^{-1}(\theta)$ and $\bm{I}(\theta)$ is the Fisher information matrix (symmetric
$I_{12}=I_{21}$) 
%
\begin{equation}
	\bm{I}(\theta)=\left(
	\begin{array}{cc}
		I_{11} & I_{12}\\
		I_{12} & I_{22}
	\end{array}\right)
\end{equation}
%
given by generalisation of \autoref{eq:Fisher Info for Poisson}
%
\begin{equation}
	I_{ij}(\theta)=\sum_{k=1}^K\frac{1}{\lambda_k}\frac{\partial\lambda_k}{\partial\theta_i}\frac{\partial\lambda_k}{\partial\theta_j}
\end{equation}

The covariance matrix $\bm{Q}$ is then 
%
\begin{equation}
	\bm{Q}=\bm{I}^{-1}(\theta)=\frac{1}{I_{11}I_{12}-I_{12}^2}\left(
	\begin{array}{cc}	
		I_{22} & -I_{12}\\
		-I_{12} & I_{11}
	\end{array}\right)
\end{equation}
%
and the variance of $d=c_1-c_2$ 
%
\begin{equation}
	\var(d)=(1,-1)^T\cdot\bm{Q}\cdot(1,-1)=\frac{I_{11}+I_{22}+2I_{12}}{I_{11}I_{12}-I_{12}^2}
	\label{eq:variance alternative}
\end{equation}

The individual terms of the Fisher Information matrix 
%
\begin{equation}
	I_{11}=\sum_{k=1}^K\frac{1}{\lambda_k}\left(\frac{\partial\lambda_k}{\partial c_1}\right)^2=\sum_{k=1}^K\frac{\left(\Lambda_1q'_k(c_1)\right)^2}{\Lambda_1q_k(c_1)+\Lambda_2q_k(c_2)}
\end{equation}
%
where
%
\begin{alignat*}{2}
	q_k(c) & =\int_{C_k}q(x-c)dx & ,\ (q_k(0)=q_k)\\
	q'_k(c) & =\int_{C_k}\frac{\partial q(x-c)}{\partial x}dx & ,\ (q'_k(0)=q'_k)
\end{alignat*}

If this keeps translational invariance (non-pixelised version does as $\int_{\mathbb{R}}g(x+c)dx=\int_{\mathbb{R}}g(x)dx$) then 
%
\begin{equation}
	I_{11}=\sum_{k=1}^K\frac{\left(\Lambda_1q'_k\right)^2}{\Lambda_1q_k+\Lambda_2q_k(-d)}
\end{equation}
%
where $d=c_1-c_2$ and 
%
\begin{equation}
	I_{22}=\sum_{k=1}^K\frac{\left(\Lambda_2q'_k\right)^2}{\Lambda_2q_k+\Lambda_1q_k(d)}
\end{equation}

For symmetrical PSF $q(x-d)=q(x+d)$ we have 
%
\begin{equation}
	I_{ii}=\sum_{k=1}^K\frac{\left(\Lambda_iq'_k\right)^2}{\Lambda_iq_k+\Lambda_jq_k(d)}
	\label{eq:Fisher Information alternative - Individual}
\end{equation}
%
And the cross term ($i\neq j$) 
%
\begin{equation}
	I_{ij}=\sum_{k=1}^K\frac{\Lambda_i\Lambda_jq'_kq'_k(d)}{\Lambda_iq_k+\Lambda_jq_k(d)}
\end{equation}

\subparagraph{Limit $d\rightarrow0$}

For $d=0$ we have

\begin{alignat*}{1}
	I_{ii} & =\frac{\Lambda_i^2}{\Lambda_i+\Lambda_j}S(0)\\
	I_{ij} & =\frac{\Lambda_i\Lambda_j}{\Lambda_i+\Lambda_j}S(0)
\end{alignat*}
%
where $S(d)=\sum_{k=1}^K\frac{\left(q'_k\right)^2}{q_k+q_k(d)}$. 

Numerator $p$ in \autoref{eq:variance alternative} 
%
\begin{equation}
	p=I_{11}+I_{22}+2I_{12}=\frac{S(0)}{\Lambda_1+\Lambda_2}(\Lambda_1^2+\Lambda_2^2+2\Lambda_1\Lambda_2)=\frac{S(0)}{\Lambda_1+\Lambda_2}(\Lambda_1+\Lambda_2)^2
\end{equation}
%
is non-zero for any $\Lambda_1,\,\Lambda_2$.

The denominator in \autoref{eq:variance alternative} 
%
\begin{eqnarray*}
	r=\det\left[\bm{I}(\theta)\right]=I_{11}I_{22}-I_{12}^2=\frac{S^2(0)}{\left(\Lambda_1+\Lambda_2\right)^2}\left(\Lambda_1^2\Lambda_2^2-(\Lambda_1\Lambda_2)^2\right) & \equiv & 0\ \text{for any }\Lambda_i
\end{eqnarray*}

$\bm{I}(\theta)$ is therefore a singular matrix for $d=0$ and inversion $\bm{I}^{-1}(\theta)$ does not exist. 

However, for the limit $d\rightarrow0$ and $p\neq0,\, r\rightarrow0$
and $\var(d\rightarrow0)=\frac{p}{r}\rightarrow\infty$. 

\subparagraph*{Limit $d\rightarrow\infty$}

The cross term $I_{ij}=0,\: i\neq j$ and we get
%
\begin{equation}
	\var(d)=\frac{1}{I_{11}}+\frac{1}{I_{22}}
\end{equation}
%
and 
%
\begin{equation}
	I_{ii}=\sum_{k=1}^K\frac{\left(\Lambda_iq_k'\right)^2}{\Lambda_iq_k+\Lambda_jq_k(d)}=\Lambda_i\sum_{k=1}^K\frac{\left(q_k'\right)^2}{q_k}=2\Lambda_iS(0)
\end{equation}
%
as the PSF $q(x)$ (and also $q'(x)$) have a finite support, if $d$ is big, $q(x-d)$ is outside the support of the $q'(x)$. They have no overlap so it doesn't have any effect in the denominator. 

For non-pixelised version, $\Lambda_1=\Lambda_2=\Lambda$ and for Gaussian approximation of the PSF ($q(x-a)\propto\exp\left(-\frac{(x-a)^2}{2\sigma^2}\right)$ (with $\sigma=\frac{\sqrt{2}}{2\pi}\frac{\lambda}{NA}$ \citep{Zhang2007}) we have $q'(x)=\frac{1}{\sigma^2}xq(x)$ and and for $\Lambda_1=\Lambda_2=\Lambda$ and for Gaussian approximation of the PSF $S(0)=\frac{1}{2\sigma^2}$:
%
\begin{alignat*}{1}
	I(d\rightarrow\infty) & =\frac{\Lambda}{\sigma^2}\\
	\var(d\rightarrow\infty) & =\frac{\sigma^2}{\Lambda}
\end{alignat*}

\subparagraph{Limit $\Lambda_i=0,\ \Lambda_j\neq0$}

then $I_{ii}\equiv0$ and $I_{ij}\equiv0$ and so $\det(\bm{I}(\theta))\equiv0$, and matrix is singular. In the limit $\Lambda_i\rightarrow0$ the variance \autoref{eq:variance alternative} $\var(d)\rightarrow\infty$. 

%==========================================
%==========================================

\section{Time distribution of the intensities (blinking)\label{sub:Appendix: Time-distribution - Cheating}}

For likelihood dependent on parameter $\Lambda_t$ (T different time slices)
%
\begin{equation}
	l_T(d,\Lambda)=\prod_{k=1}^K\prod_{t=1}^Tp(n_k|d,\Lambda_t)p(\Lambda_t)
\end{equation}

\begin{equation}
	\mathcal{L}_T(d,\Lambda)=\sum_{k=1}^K\sum_{t=1}^T\left[\log\left(l_k(d,\Lambda_t)\right)+\log\left(p(\Lambda_t)\right)\right]
\end{equation}
%
as $p(\Lambda)$ is not dependent on $d$ then
%
\begin{equation}
	\frac{\partial^2\mathcal{L}_T(d,\Lambda)}{\partial d^2}=\sum_{t=1}^T\frac{\partial^2\mathcal{L}(d,\Lambda_t)}{\partial d^2}
\end{equation}
%
but in the expectation equation \autoref{eq:Fisher Info Definition} the time dependence appears as
%
\begin{alignat*}{1}
	I_T(\theta) & =-\E_T\left[\sum_{t=1}^T\frac{\partial^2\mathcal{L}(d,\Lambda_t)}{\partial d^2}\right]=\sum_{t=1}^T-\E_T\left[\frac{\partial^2\mathcal{L}(d,\Lambda_t)}{\partial d^2}\right]=\sum_{t=1}^T\E_T\left[\left(\frac{\partial\mathcal{L}(d,\Lambda_t)}{\partial d}\right)^2\right]\\
	 & =\sum_{t=1}^T\int_{\Lambda_t}p(\Lambda_t)I(\theta)d\Lambda_t=\sum_{t,k}\int_{\Lambda_t}p(\Lambda_t)\frac{1}{\lambda_k(\Lambda_t)}\left(\frac{\partial\lambda_k(\Lambda_t)}{\partial d}\right)^2d\Lambda_t
\end{alignat*}

%==========================================
%==========================================

\section{Time distribution of the intensities - integrating out $\Lambda$\label{sub:Appendix Time-distribution-Integrating out}}

\begin{equation}
	l_k(d)=\int_{\Lambda}l_k(d,\Lambda)d\Lambda=\int_{\Lambda}p(n_k|d,\Lambda)p(\Lambda)d\Lambda
\end{equation}
%
for four state model of two sources: $\left\{ (\Lambda_1,0),(0,\Lambda_2),(\Lambda_1,\Lambda_2),(0,0)\right\} $: $\lambda^1=\Lambda_1q(x-c_1)$, $\lambda^2=\Lambda_2q(x-c_2)$, $\lambda^3=+\Lambda_1q(x-c_1)+\Lambda_2q(x-c_2)$, $\lambda^4=0$ with uniform distribution over these states
%
\begin{equation}
	l_k(\theta)=\frac{1}{4}\sum_{i=1}^4\Po(\lambda_k^i)
\end{equation}
%
derivatives 
%
\begin{equation}
	\frac{\partial l_k}{\partial c_p}=\frac{1}{4}\sum_i\frac{\partial\Po(\lambda_k^i)}{\partial c_p}=\frac{1}{4}\sum_i\left(\Po(\lambda_k^i)\frac{(n_k-\lambda_k^i)}{\lambda_k^i}\frac{\partial\lambda_k^i}{\partial c_p}\right)
\end{equation}

The Fisher information matrix diagonal entries:
%
\begin{alignat}{1}
	I_{pp}(\theta) 
	& =\E\left[\left(\sum_{k=1}^n\frac{1}{l_k}\frac{\partial l_k}{\partial c_p}\right)^2\right]\nonumber \\
	& =\E\left[\left\{ \sum_{k=1}^n\left(\frac{1}{\sum_{j=1}^4\Po(\lambda_k^j)}\frac{\partial\sum_{i=1}^4\Po(\lambda_k^i)}{\partial c_p}\right)\right\} \left\{ \sum_{l=1}^n\left(\frac{1}{\sum_{j=1}^4\Po(\lambda_l^j)}\frac{\partial\sum_{i=1}^4\Po(\lambda_l^i)}{\partial c_p}\right)\right\} \right]\nonumber \\
	& =\sum_{k=1}^n\E_k\left[\frac{\left(\sum_{i=1}^4\frac{\partial\Po(\lambda_k^i)}{\partial c_p}\right)^2}{\left(\sum_{j=1}^4\Po(\lambda_k^j)\right)^2}\right]	 
	 \label{eq:Fisher Info Integrated Out - diagonal entries}
\end{alignat}

as the cross terms ($k,\, l$) in the sum (2nd row) are zeros: 

\begin{alignat*}{1}
	\E\left[\left(\frac{\sum_{i=1}^4\frac{\partial\Po(\lambda_k^i)}{\partial c_p}}{\sum_{j=1}^4\Po(\lambda_k^j)}\right)\left(\frac{\sum_{i=1}^4\frac{\partial\Po(\lambda_l^i)}{\partial c_p}}{\sum_{j=1}^4\Po(\lambda_l^j)}\right)\right] 
	& =\E_k\left[\frac{\sum_{i=1}^4\frac{\partial\Po(\lambda_k^i)}{\partial c_p}}{\sum_{j=1}^4\Po(\lambda_k^j)}\right]\E_l\left[\frac{\sum_{i=1}^4\frac{\partial\Po(\lambda_l^i)}{\partial c_p}}{\sum_{j=1}^4\Po(\lambda_l^j)}\right]\\
	& =\sum_{i=1}^4\frac{\partial}{\partial c_p}\left(\sum_{n_k\geq0}\Po(\lambda_k^i)\right)\sum_{i=1}^4\frac{\partial}{\partial c_p}\left(\sum_{n_k\geq0}\Po(\lambda_l^i)\right)\\
	& =0	
\end{alignat*}

Expressing the derivatives and the expectation from \autoref{eq:Fisher Info Integrated Out - diagonal entries}:
%
\begin{alignat*}{1}
	I_{pp}(\theta) & =\sum_{k=1}^n\E_k\left[\left\{ \frac{\sum_{i=1}^4\left(\Po(n_k;\lambda_k^i)\frac{(n_k-\lambda_k^i)}{\lambda_k^i}\frac{\partial\lambda_k^i}{\partial c_p}\right)}{\sum_{j=1}^4\Po(n_k;\lambda_k^j)}\right\} ^2\right]\\
	 & =\frac{1}{4}\sum_{k=1}^n\sum_{n_k\geq0}\frac{\left\{ \sum_{i=1}^4\left(\Po(n_k;\lambda_k^i)\frac{(n_k-\lambda_k^i)}{\lambda_k^i}\frac{\partial\lambda_k^i}{\partial c_p}\right)\right\} ^2}{\sum_{j=1}^4\Po(n_k;\lambda_k^j)}
\end{alignat*}

For the four states model we have $\lambda^3(c_1,c_2)=\lambda^1(c_1)+\lambda^2(c_2)$ and so $\frac{\partial\lambda^3}{\partial c_p}=\frac{\partial\lambda^p}{\partial c_p}$ and $\frac{\partial\lambda^j}{\partial c_p}=0,\, i\neq j$ for $p=\{1,2\},\: j=\{1,2,4\};$ so 
%
\begin{equation}
	I_{pp}(\theta) =\sum_{k=1}^N\left(\frac{\partial\lambda_k^p}{\partial c_p}\right)^2\E_k\left[\left\{ \frac{\sum_{i=\{p,3\}}\left(\Po(n_k;\lambda_k^i)\frac{(n_k-\lambda_k^i)}{\lambda_k^i}\right)}{\sum_{j=1}^4\Po(n_k;\lambda_k^j)}\right\} ^2\right]
\end{equation}

The Fisher information matrix off-diagonal entries:

\begin{alignat}{1}
	I_{pq}(\theta) & =\sum_{k=1}^n\E_k\left[\frac{\left(\sum_{i=1}^4\frac{\partial\Po(\lambda_k^i)}{\partial c_p}\right)\left(\sum_{l=1}^4\frac{\partial\Po(\lambda_k^{l})}{\partial c_q}\right)}{\left(\sum_{j=1}^4\Po(\lambda_k^j)\right)^2}\right]\nonumber \\
	 & =\sum_{k=1}^n\left(\frac{\partial\lambda_k^p}{\partial c_p}\right)\left(\frac{\partial\lambda_k^q}{\partial c_q}\right)\E_k\left[\frac{\left(\sum_{i=\{p,3\}}\Po(n_k;\lambda_k^i)\frac{(n_k-\lambda_k^i)}{\lambda_k^i}\right)\left(\sum_{i=\{q,3\}}\Po(n_k;\lambda_k^i)\frac{(n_k-\lambda_k^i)}{\lambda_k^i}\right)}{\left(\sum_{j=1}^4\Po(n_k;\lambda_k^j)\right)^2}\right]
	 \label{eq:Fisher Info Integrated Out-off diagonal entries}
\end{alignat}

\paragraph{Limit $d\rightarrow0$}

When $c^1=c^2$ then $\lambda^1=\lambda^2$ and $\frac{\partial\Po(\lambda^1)}{\partial c^1}=\frac{\partial\Po(\lambda^2)}{\partial c^2}$. Then all entries in $I_{pq}$ are equal and the matrix is singular. For the limit $d\rightarrow0$ the determianat $\det(\bm{I})\rightarrow0$ and the variance $\var(d)\rightarrow\infty$.

\paragraph{Limit $d\rightarrow\infty$}

Sources are far apart and $\lambda^1$ and $\lambda^2$ do not have a common overlap. For $k'$ where $\lambda_{k'}^1>0,\,\lambda_{k'}^2\equiv0$ and $\Po(n_{k'},\lambda_{k'}^3)=\Po(n_{k'},\lambda_{k'}^1)+\Po(n_{k'},\lambda_{k'}^2=0)=\Po(n_{k'},\lambda_{k'}^1)+1$.

Also $\frac{\partial\lambda^p}{\partial c_q}=0,\: p\neq q$. From \autoref{eq:Fisher Info Integrated Out - diagonal entries} the diagonal
elements 

\begin{alignat*}{1}
	I_{pp} & =\sum_{k=1}^n\E_k\left[\frac{\left(2\frac{\partial\Po(\lambda_k^p)}{\partial c_p}\right)^2}{\left(2\Po(\lambda_k^p)+2\Po(\lambda_k^q)\right)^2}\right]\\
	 & =\sum_{k=1}^n\E_k\left[\frac{\left(\Po(\lambda_k^p)\frac{\left(n_k-\lambda_k^p\right)^2}{\lambda_k^p}\frac{\partial\lambda_k^p}{\partial c_p}\right)^2}{\left(\Po(\lambda_k^p)+1\right)^2}\right]\\
	 & =\sum_{k=1}^n\left(\frac{1}{\lambda_k^p}\frac{\partial\lambda_k^p}{\partial c_p}\right)^2\E_k\left[\left(n_k-\lambda_k^p\right)^2\left(\frac{\Po(\lambda_k^p)}{\Po(\lambda_k^p)+1}\right)^2\right]
\end{alignat*}

For large $\lambda_k^p$ the second term in the expectation is approximately one: $\frac{\Po(\lambda_k^p)}{\Po(\lambda_k^p)+1}=1-\frac{1}{1+\Po(\lambda_k^p)}\approx1$
%
\begin{equation}
	I_{pp}\approx\frac{1}{2}\sum_{k=1}^n\frac{1}{\lambda_k^p}\left(\frac{\partial\lambda_k^p}{\partial c_p}\right)^2\label{eq: int out - approximation d to infty}
\end{equation}
%
which is the \autoref{eq:Fisher Info for Poisson} (up to the factor 2). As the the term is upper bounded by one: $\frac{\Po(\lambda_k^p)}{\Po(\lambda_k^p)+1}=1-\frac{1}{1+\Po(\lambda_k^p)}<1$ the terms $ $$I_{pp}$ will be slightly smaller then the approximation \autoref{eq: int out - approximation d to infty}:
%
\begin{equation}
	I_{pp}=\frac{1}{2}\sum_{k=1}^n\frac{1}{\lambda_k^p}\left(\frac{\partial\lambda_k^p}{\partial c_p}\right)^2-\epsilon
	\label{eq:int out d to infty}
\end{equation}

The off-diagonal entries: 
%
\begin{equation}
	I_{pq}=0
\end{equation}
%
as
% 
\begin{equation}
	\frac{\partial\Po(\lambda^p)}{\partial c_p}\frac{\partial\Po(\lambda^q)}{\partial c_q}=0
\end{equation}
%
because $\lambda^p(x)$ and $\lambda^q(x)$ do not have a common support. 

Therefore
%
\begin{alignat}{1}
	\var(d) & =\frac{1}{I_{11}}+\frac{1}{I_{22}}\nonumber \\
	& =2\left(\frac{1}{I_{11}^{\text{static}}-\epsilon}+\frac{1}{I_{22}^{\text{static}}-\epsilon}\right)\nonumber \\
	& >2\var(d^{\text{static}})\label{eq:variance d static vs blinking}
\end{alignat}
%
where $I^{\text{static}}$ and $\var^{\text{static}}$ correspond to the Fisher information matrix \autoref{eq:Fisher Info for Poisson} and the variance \autoref{eq:variance alternative} of the static case. The factor of 2 stems from the fact that the total number of photons is double in the static case compared to the blinking model. 